# EC2

## EC2 (Elastic Compute Cloud)

EC2 is a VM in the cloud.

### Pricing models:

* **On Demand** (default model): low cost, paying by hour or second. You have full control over its lifecycle—you decide when to launch, stop, hibernate, start, reboot, or terminate it. Sample: when **task run uninterrupted** from start to finish
* **Reserved**: the most economical option for **long-term workloads** with predictable usage patterns. Contract terms are 1 to 3 years. It operates at **regional** level.It includes different discounts
	* **Standard** Reserved instances (72% off on demand instances). 
	
	!!!danger "Standard Reserved Instances cannot be moved between regions: You can choose if a Reserved Instance applies to either a specific AZ, or an entire Region, but you cannot change the region."

	* **Convertible** Reserved instances (54%). has the option to change for a greater instance type 
	* **Schedule** Reserved instances, based on times, for example, every end of the day, week or month
* **Spot**: up to a 90% discount compared to On-Demand prices, it takes advantage of **unused** EC2 capacity. It can accept interruptions. Used for various stateless, fault-tolerant, or flexible applications such as big data, containerized workloads, CI/CD, web servers, HPC (high-performance computing), and other test & development workloads. Extra charge when you terminate the instance
* **Dedicated**: physical EC2 server. the most expensive option.
	* It reduces cost using your SW licenses (importing your licensed VM). 
	* Also when multitenant not supported by law (compliance)

Saving plans save up to 72% for EC2, lambda and fargate, commit to 1 or 3 years

### Instance types

determinate hardware as compute, memory and storage capabilities:

* Micro instances: **T1** free tier
* General Purpose por basic apps: **T2** (burst performance), **M5**, **M4** and **M3**. They provide balance of memory and CPU
* Compute Optimized for CPU intensive apps: **C5**, **C5n** **C4** and **C3**. Linux no supported
* FPGA instances (Field Programmable Gate Array): **F1** pararel processes
* GPU intances: **G2**, **G3** and **G4**, **P2**, **P3**
* Machine learning ASIC instances: **INF1** (inference)
* Memory Optimized for eXtreme memory requirements: **X1e**, **X1**, **R4**, **R3**, and **Z1D**
* Storage Optimized for high Input/Output access: **H1**, **I2**, **I3**, **I3en** and **D2**

Change the instance type required stopping the instance previously.

### Launching EC2:

1. From AWS EC2, `Launch instance`
2. Select AMI, for example: Amazon Linux 2 AMI
3. Choose an Instance Type (t2 micro - free tier)
4. Configure Instance Details (number of instances) 
    * enable Auto-assign public ip
    * enable Termination protection
    * enable CloudWatch every 5 minutes. Detailed monitoring every minute is not free
5. Add Storage. Root and EBS volume types allow encryption and delete on termination 
    * root has delete on termination by default, but not encryption by default
    * ebs has not delete on termination by default 
    * EC2 instance and volume are in the same AZ
6. Add tags like Name, Department or Employee_Id
7. Configure **Security Groups**: virtual **firewalls** to enable traffic - disabled by default
    * Linux=SSH port 22. Microsoft Windows=RDP (Remote Desktop Protocol) port 3389. http/https ports 80/443
	* default SG for linux instances brings already inbound with SSH. Default SG for windows instances bring RDP
    * source anywhere 0.0.0.0/0 (IPv4) and ::/0 (IPv6)
8. User data with command script (base64 encoded) to run when you launch your instance
9. Launch creating a key pair (public and private key)

#### How to connect to EC2 

1. Using EC2 Instance Connect from AWS Console, select the EC2 instance and clic on the `Connect` button. 
    * This option requires to [Configure network access to an instance](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-set-up.html?icmpid=docs_ec2_console#ec2-instance-connect-setup-security-group)

2. Using the terminal:

    ```shell
    cd "C:\Users\i24764\OneDrive - Verisk Analytics\AWS"
    CHMOD 400 ec2.pem # change permissions to unlock my key down
    ssh ec2-user@3.80.39.184 -i ec2.pem
    sudo su
    uname -a  # software details
    cat ~/.ssh/authorized_keys	# the public key
    ```

3. Using [putty](https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html) 

    * Using puttyGen, convert the pem file to a ppk file clicking on the load button and the save private key button
    
	**Key pair** can created or imported in AWS. You can create you key pair with the following command, keep the PK and import the public key in AWS:

	```shell
	ssh-keygen -C rnietoe@gmail.com -f ~/.ssh/rnietoe
	```

    * Configure SSH Auth with PK created before
    * Copy IP address to the session host name field. SampleL ec2-user@25.51.150.70 and port 22
    4. Open connection and enter `sudo su` command

4. Using CLI

    * Download the windows installer from [AWS Command Line Interface](https://aws.amazon.com/cli/?nc1=h_ls) and installe it
    * Now we have the **aws** command in our prompt
    * Configure IAM User with Programmatic access 
    * Download the access key and the secret access key:

	```shell
	aws configure
	aws configure --profile profile_name # when we want to work with cli profiles
	# once typep the access key, the secret access key, the default region and the output format (json/text)
	aws ec2 describe-regions
	# use :q to exit from command output
	aws <command> --profile <profile_name> # when we want to execute commands with a specific profile
	set AWS_PROFILE=<profile_name> # set/unset default profile
	echo $AWS_PROFILE
	aws sts get-caller-identity # print out account and user info    
    cat ~/.aws/config	# file containing profile configuration
    cat ~/.aws/credentials	# file containing profile credentials	
    rm -rf ~/.aws
	```

    We must use **roles** for security reasons instead of saving credentials (anyone could access to the .aws directory). Roles are global, they are not specified any region. Create a role to allows EC2 to use S3 as an admin:

    1. Go to `IAM/Roles` and create a new role
    2. Select `EC2` as the type of trusted entity
    3. Attach `AmazonS3FullAccess` permissions policies
    4. Go to `EC2`, select the instance and actions/instance settings/attach/replace iam role
    5. Then we can delete .aws directory with credential and still running `aws s3 ls`

    AWS CLI Pagination commands (to avoid timeout errors):     
    aws s3api list-objects --bucket <YOUR_BUCKET_NAME> --page-size 5
    aws s3api list-objects --bucket <YOUR_BUCKET_NAME> --max-items 1

#### How to build a web server

```shell
sudo su
yum update -y # check for updates
yum install httpd -y # install apache 
systemctl httpd start # sudo systemctl start httpd # start apache service
systemctl enable httpd # start apache on restarts | chkconfig on
systemctl status
cd /var/www/html # create index.html in this path
nano index.html
<html><body><h1>This is server 1</h1></body></html>
```

## EBS

**`AWS EBS`** (Elastic Block Storage) is like a virtual hard disk in the cloud. This is a **block** service used for **durable** storage in EC2 instances and again has no connection to S3.

* EBS cannot be shared by two EC2 instances
* EBS volume can be attached to any EC2 instance in the same AZ
* EBS volume attached as an additional disk (not the root volume) can be detached without stopping the instance
* EBS snapshots use **incremental** backups and are stored in S3
* By default, the **DeleteOnTermination** attribute is set to True for the root volume, and is set to False for all other volume types.
    * To preserve the root volume when an instance terminates, change the DeleteOnTermination attribute for the root volume to False.
* EBS doesn’t grow automatically as your data requirements increase – you would need to increase the volume size and then extend your filesystem.

[Amazon EBS volume types](https://aws.amazon.com/ebs/volume-types/):

* **SSD** (Solid State Drive)
	* GP2 - General Purpose, 3 IOPS (Input Output per second) per gb, maximun of 16000 IOPS
    * GP3 - latest generation, from 3000 to 16000 IOPS 20% cheaper than GP2
	* IO1 - Provisioned IOPS - 50 IOPS per gb. from 16.000 to 64.000 IOPS - high performance, more expensive
    * IO2 - latest generation. 500 IOPS per gb. from 16.000 to 64.000 IOPS - 99.999% durability instead of 99.9%. same price as IO1
    * IO2 Block Express - SAN in the cloud - up to 64tb and 256.000 IOPS - the highest performance, most expensive
* **HDD** (Hard Disk Drive) They cannot be a root volume
	* ST1 - Throughtput optimised - Low cost for frequently access. 500 mb/s. 
	* SC1 - Cold HDD - Lowest cost for less frequently access. 250 mb/s
	* Magnetic - Previous generation

    HDD based volumes will always be less expensive than SSD types. 

    an **EBS-optimized** EC2 instance should be used for high performance. No availabled in EC2 free-tier
    
EBS Pricing depends on:

* Volumes
* Snapshots
* Data transfer
* volume recovery (attaching volumes from own intances to another)

Although there is no direct way to encrypt an existing unencrypted volume or snapshot, you can encrypt them by creating either a volume or a snapshot.

* encrypted volumes from encrypted snapshots
* unencrypted volumes from unencrypted snapshots

```shell
aws ec2 create-snapshot # create a snapshot of an EBS volume
```

EBS Provisioned IOPS io1 volumes can be mounted to 16 nitro instances using **multi-attach** which is a new feature and has several constraints.

Create a volume from EC2 Dashboard/Volumes

## ELB

[Elastic Load Balancer](https://aws.amazon.com/elasticloadbalancing/faqs/?nc1=h_ls) implements Dynamic Load Balancer

Load Balancing algorithms:

* Round Robin: first request to first server, second request to second server...
* Randomized
* Centrally managed, based on defined conditions
* threshold-based: all request to a server till a defined limit (threshold)

[ELB types](https://aws.amazon.com/elasticloadbalancing/features/?nc1=h_ls):

* ALB (**Application** Load Balancers) for intelligent routing - HTTP/HTTPS (application layer 7)
* NLB (**Network** Load Balancers) for extreme performance and static IPs - TCP/TLS (transport layer 4)
	* You can passthrough encrypted traffic with an NLB and terminate the SSL on the EC2 instances as all data in transit encryption requirement
* CLB (**Classic** Load Balancers) for classic/**old/legacy** EC2 instantes or test/dev environments - low cost - HTTP/HTTPS/TCP (layers 4 and 7)
* GLB (**Gateway** Load Balancers) to deploy, scale, and manage your third-party **virtual appliances**

supported services:

* EC2
* ECS
* Auto Scaling
* CloudWatch
* Route53

some features:

* **X-Forwarded-For** header is used to get the IPv4 address of end users when routing the request
* **sticky sessions** bind a **user's session** to a specific EC2 instance. All user requests during the session are sent to the same intance (CLB) or target group (ALB)
* **cross zone load balancing** allow ELB to send traffic to **another AZ**
    
    !!!danger "ELB can spread (propagar) load across AZs not regions."

* **path patterns**  are listener with rules to foward requests based on the **URL path**
* Error **504** means gateway timeout

How to use classic load balancer

1. `Create load balancer` from EC2 : Load Balancers of type CLB
2. [Register EC2 instances](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-backend-instances.html)
3. Select our Security Group (virtual firewall)
7. Browse to the **ELB** (Elastic Load Balance) DNS name and see the result, instead of browsing to the EC2 IP address

How to use application load balancer

1. `Create target group` to group EC2 instances, IP addresses or Lambda functions for load balancing
2. `Create load balancer` of type ALB
3. Configure Load Balancer with name and select every AZ

    !!!danger "At least two public subnets are required to enable the LB"

4. Configure Routing with a Target Group name and the following health check settings:
	* healthy threshold: 3 times
	* unhealthy threshold: 3 times
	* timeout: 3 seconds
	* interval: 5 seconds
	* success code: 200
5. Register target adding our EC2 instances (to registered)
6. Review and create

## Route53

**`AWS Route 53`** service name comes from port 53, where **DNS** (Domain Name System) work on to map domain name to EC2 instances, Load Balancers and S3 buckets

we can register a DNS using `Route53` - `Register domain`. You can purchase and manage domain names such as example.com, and Route 53 will automatically configure DNS settings for your domains

!!!tip "Ensure there is a free bucket with the same domain name"

**Route 53 Traffic Flow** makes it easy for you to manage traffic globally through a variety of routing types. Using Route 53 Traffic Flow’s simple visual editor, you can easily manage how your end-users are routed to your application’s endpoints—whether in a single AWS region or distributed around the globe. 

.com => **NS** (Name Server) Records => **SOA** (Start Of Authority)

DNS changes can take 48 hours to take effect due to the cache

* **CName** (Canonical Name) maps to the host name: https://mobile.acloud.guru = https://m.acloud.guru 
* **Alias Record** provide a Route 53–specific extension to DNS functionality. An alias could be created for the ELB. Alias Records can also point to AWS Resources that are hosted in other accounts by manually entering the ARN

!!!tips "ELB resolve DNS names instead of IPv4 addresses"

Routing Policies:

* **Simple** Routing: one dns record with multiple IP addresses
* **Weighted** Routing: traffic based on weighting (20%-30%-50%)
* **Latency-based** Routing: traffic based on the lowest latency (better performance)
* **Failover** Routing: route the traffic to the primary or secondary site defined based on health checks
* **Geolocation** Routing: traffic based on the user's location
* **Geoproximity** Routing: traffic based on the users' and resources' location. Available in traffic **flow-only** mode using **bias**
* **Multivalue Answer** Routing, similar to simple routing, but using health checks on each record sets to serve traffic to **random** web servers

It can be used to load balance however it does not have the ability to route based on information in the incoming request path.

Using Route53:

1. `Register domain` from Route53 takes between 2 hours and 3 days
2. `Create Record Set` of type IPv4 address with the three EC2 public IPs. Set TTL (Time to Live) to 1 min to clear from cache
3. `Create healthcheck` and associate it to each record set, so it will be removed from Route53 until it passes the health check
4. `Create traffic policy` to configure Geoproximity Routing

```shell
ipconfig /flushdns # to remove saved ip from cache from the client side
```

!!!danger "With Route 53, there is a default limit of **50** domain names. However, this limit can be increased by contacting AWS support"