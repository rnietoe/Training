{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Training index \u00b6 Security \u00b6 Web Security: OAuth And OpenID Connect AWS \u00b6 AWS Solutions Architect Associate Git \u00b6 Git Essential Training: The Basics Git: Branches, Merges, and Remotes Git Intermediate Techniques React \u00b6 Building React and ASP.NET Core Applications Become a React Developer","title":"Training index"},{"location":"#training-index","text":"","title":"Training index"},{"location":"#security","text":"Web Security: OAuth And OpenID Connect","title":"Security"},{"location":"#aws","text":"AWS Solutions Architect Associate","title":"AWS"},{"location":"#git","text":"Git Essential Training: The Basics Git: Branches, Merges, and Remotes Git Intermediate Techniques","title":"Git"},{"location":"#react","text":"Building React and ASP.NET Core Applications Become a React Developer","title":"React"},{"location":"AWS/0-AWS-Solutions-Architect-Associate/","text":"AWS Architect \u00b6 https://rnietoe.signin.aws.amazon.com/console rnietoe@gmail.com - AWS account (root account) rnietoe/training rnietoe/paying Abodroc@83 Google Authenticator for Android - MFA ( M ulti- F actor A uthentication) Book your exam here Introduction Storage Network Compute Security Management HA Architecture Databases ACloud.Guru Final Practice Exam \u00b6 https://aws.amazon.com/certification/certification-prep/ \u2022 Step 1: Take an AWS Training Class \u2022 Step 2: Review the Exam Guide and Sample Questions \u2022 Step 3: Practice with Self-Placed Labs and an Exam Prep Quest \u2022 Step 4: Study AWS Whitepapers \u2022 Step 5: Review AWS FAQs \u2022 Step 6: Take an Exam Prep Workshop \u2022 Step 7: Take a Practice Exam \u2022 Step 8: Schedule Your Exam and Get Certified https://www.qwiklabs.com/ AWS Documentation \u00b6 https://docs.aws.amazon.com/index.html?nc2=h_ql_doc_do https://aws.amazon.com/faqs/ https://aws.amazon.com/whitepapers/?whitepapers-main.sort-by=item.additionalFields.sortDate&whitepapers-main.sort-order=desc https://www.youtube.com/c/amazonwebservices/videos https://aws.amazon.com/architecture/ https://aws.amazon.com/new/ https://www.meetup.com/es/topics/amazon-web-services/ AWS Certified Solutions Architect Associate Practice Exams AWS Exam \u00b6 https://aws.amazon.com/certification/faqs/ https://aws.amazon.com/certification/certified-solutions-architect-associate/ AWS Cloud Practitioner Essentials (Second Edition) (Spanish) https://www.aws.training/Details/Curriculum?id=46152","title":"AWS Architect"},{"location":"AWS/0-AWS-Solutions-Architect-Associate/#aws-architect","text":"https://rnietoe.signin.aws.amazon.com/console rnietoe@gmail.com - AWS account (root account) rnietoe/training rnietoe/paying Abodroc@83 Google Authenticator for Android - MFA ( M ulti- F actor A uthentication) Book your exam here Introduction Storage Network Compute Security Management HA Architecture Databases","title":"AWS Architect"},{"location":"AWS/0-AWS-Solutions-Architect-Associate/#acloudguru-final-practice-exam","text":"https://aws.amazon.com/certification/certification-prep/ \u2022 Step 1: Take an AWS Training Class \u2022 Step 2: Review the Exam Guide and Sample Questions \u2022 Step 3: Practice with Self-Placed Labs and an Exam Prep Quest \u2022 Step 4: Study AWS Whitepapers \u2022 Step 5: Review AWS FAQs \u2022 Step 6: Take an Exam Prep Workshop \u2022 Step 7: Take a Practice Exam \u2022 Step 8: Schedule Your Exam and Get Certified https://www.qwiklabs.com/","title":"ACloud.Guru Final Practice Exam"},{"location":"AWS/0-AWS-Solutions-Architect-Associate/#aws-documentation","text":"https://docs.aws.amazon.com/index.html?nc2=h_ql_doc_do https://aws.amazon.com/faqs/ https://aws.amazon.com/whitepapers/?whitepapers-main.sort-by=item.additionalFields.sortDate&whitepapers-main.sort-order=desc https://www.youtube.com/c/amazonwebservices/videos https://aws.amazon.com/architecture/ https://aws.amazon.com/new/ https://www.meetup.com/es/topics/amazon-web-services/ AWS Certified Solutions Architect Associate Practice Exams","title":"AWS Documentation"},{"location":"AWS/0-AWS-Solutions-Architect-Associate/#aws-exam","text":"https://aws.amazon.com/certification/faqs/ https://aws.amazon.com/certification/certified-solutions-architect-associate/ AWS Cloud Practitioner Essentials (Second Edition) (Spanish) https://www.aws.training/Details/Curriculum?id=46152","title":"AWS Exam"},{"location":"AWS/1-Introduction/","text":"1. Introduction \u00b6 IaaS : Infrastructure as a service: you manage the entire infrastructure ( EC2 , VPC , RDS , S3 ) in the cloud, but platform and software run on others' infrastructure PaaS : Platform as a Service, you simply manage platform instead of the infrastructure, such as Elastic Beanstalk or Lightsail SaaS : Software as a Service, you use the software developed by others from the cloud, such as Gmail FaaS : Function as a Service DaaS : Desktop as a Service such as Windows or Linux, provide by WorkSpaces . Three different ways to access AWS: Programmatic, using the aws CLI ( C ommand L ine I nterface) Using the Console Using SDK The three types of cloud deployments are: Public Hybrid Private (also called 'on-prem') Traditional Computing VS Cloud Computing IT Assets as provisioned resources Bootstrapping scripts, Golden images, Containers, Hybrid CloudFormation Automation: serverless management and deployment AWS Elastic Beanstalk EC2 auto recovery and Auto Scaling Alarms and Events - CloudWatch - Lambda scheduled events, WAF security automations Global, available and scalable capacity Scale up / Vertical Scaling (increase RAM, CPU) Scale Out - Horizontal Scaling (add multiple virtual machines behind a ELB) Stateless applications (Lambda - Alexa) Stateless componets (login credentials in a cookie) Statefull components (store information in a db) Distribute load to multiple nodes Distributed Processing Implemente distributed processing Higher level managed services Built-in security Architecting for cost Operations on AWS Build your systems to be scalable, use disposable resources, reduce infrastructure to code, and assume EVERYTHING will fail sooner or later. Global services (for every region): IAM Route53 CloudFront SNS (Simple Notify Service) SES Regional Services with global view S3 Caching services: CloudFront API Gateway ElastiCache DynamoDB Accelerator (DAX) AWS Service used on premise: AWS Snow Family: Snowball - to upload 50TB (200$) or 80TB (250$) to AWS in a week instead of three months. This allow S3 imports/exports Snowball Edge - Local compute and storage only till 100TB Snowmobile - a track with 100PB First 10 days are free. 15$ per day later Data transfer to S3 is free. Data transfer out is charged Storage gateway : connect on-premise software with cloud-based storage (S3) File Gateway. objects are flat files Volume Gateway: objects are hard disk drives. It looks like EBS snapshots. there are storage volumes (entire dataset) and cached volumes Tape gateway CodeDeploy - include applications Opsworks - include applications IoT greengrass Overview: Availability Zone (AZ) are data center. AZs are randomized by AWS (us-east-1 can be different AZ for different accounts). Region is a distinct location within a geographical area with 2 or more AZ, designed to provide high availability to a specific geography. Choosen by law, latency and AWS Services US East (N. Virginia) us-east-1 was the first region and all new services are deployed here first Edge Locations are Amazon's CDN ( C ontent D elivery N etwork) end points. Objects are cached for 48 hours by default (TTL - T ime T o L ive). This not just read only. Types: Web Distribution (websites) RTMD (media streaming) - Discontinued support by CloudFront on December 31, 2020\" Number of Edge Locations > Number of Availability Zones > Number of Regions CloudFront \u00b6 AWS CloudFront distribution is a collection of a CDN's Edge Locations. CloudFront content is cached in Edge Locations. This allows you to distribute content using a worldwide network of edge locations that provide low latency and high data transfer speeds. From AwS CloudFront select Create Distribution The CloudFront origin can be an S3 bucket, an EC2 instance, an Elastic Load Balancer or Route53. We can restrict viewer access using signed URLs for individual files or signed cookies for multiple files. Netflix or CloudGuru samples. OAI - O rigin A ccess I dentity If the origin is EC2 then use CloudFront Signed url If the origin is S3 then use S3 signed url We can create invalidations to remove origin objets on the edge location To delete a CloudFront distribution, you have to disable it first. This process takes 15 minutes Pricing depends on: traffic distribution requests data transfer out We will be charged when deleting cached data from an edge location Resource Groups and Tag Editor \u00b6 Tags allow to find AWS resources in a selected region, but it can not directly managing those resources. This can make it easier to search for and filter resources by purpose, owner, environment, or other criteria. Resources groups allow to execute operations from AWS Systems Manager to different resources (such as a EC2 fleet) based on resources groups","title":"1. Introduction"},{"location":"AWS/1-Introduction/#1-introduction","text":"IaaS : Infrastructure as a service: you manage the entire infrastructure ( EC2 , VPC , RDS , S3 ) in the cloud, but platform and software run on others' infrastructure PaaS : Platform as a Service, you simply manage platform instead of the infrastructure, such as Elastic Beanstalk or Lightsail SaaS : Software as a Service, you use the software developed by others from the cloud, such as Gmail FaaS : Function as a Service DaaS : Desktop as a Service such as Windows or Linux, provide by WorkSpaces . Three different ways to access AWS: Programmatic, using the aws CLI ( C ommand L ine I nterface) Using the Console Using SDK The three types of cloud deployments are: Public Hybrid Private (also called 'on-prem') Traditional Computing VS Cloud Computing IT Assets as provisioned resources Bootstrapping scripts, Golden images, Containers, Hybrid CloudFormation Automation: serverless management and deployment AWS Elastic Beanstalk EC2 auto recovery and Auto Scaling Alarms and Events - CloudWatch - Lambda scheduled events, WAF security automations Global, available and scalable capacity Scale up / Vertical Scaling (increase RAM, CPU) Scale Out - Horizontal Scaling (add multiple virtual machines behind a ELB) Stateless applications (Lambda - Alexa) Stateless componets (login credentials in a cookie) Statefull components (store information in a db) Distribute load to multiple nodes Distributed Processing Implemente distributed processing Higher level managed services Built-in security Architecting for cost Operations on AWS Build your systems to be scalable, use disposable resources, reduce infrastructure to code, and assume EVERYTHING will fail sooner or later. Global services (for every region): IAM Route53 CloudFront SNS (Simple Notify Service) SES Regional Services with global view S3 Caching services: CloudFront API Gateway ElastiCache DynamoDB Accelerator (DAX) AWS Service used on premise: AWS Snow Family: Snowball - to upload 50TB (200$) or 80TB (250$) to AWS in a week instead of three months. This allow S3 imports/exports Snowball Edge - Local compute and storage only till 100TB Snowmobile - a track with 100PB First 10 days are free. 15$ per day later Data transfer to S3 is free. Data transfer out is charged Storage gateway : connect on-premise software with cloud-based storage (S3) File Gateway. objects are flat files Volume Gateway: objects are hard disk drives. It looks like EBS snapshots. there are storage volumes (entire dataset) and cached volumes Tape gateway CodeDeploy - include applications Opsworks - include applications IoT greengrass Overview: Availability Zone (AZ) are data center. AZs are randomized by AWS (us-east-1 can be different AZ for different accounts). Region is a distinct location within a geographical area with 2 or more AZ, designed to provide high availability to a specific geography. Choosen by law, latency and AWS Services US East (N. Virginia) us-east-1 was the first region and all new services are deployed here first Edge Locations are Amazon's CDN ( C ontent D elivery N etwork) end points. Objects are cached for 48 hours by default (TTL - T ime T o L ive). This not just read only. Types: Web Distribution (websites) RTMD (media streaming) - Discontinued support by CloudFront on December 31, 2020\" Number of Edge Locations > Number of Availability Zones > Number of Regions","title":"1. Introduction"},{"location":"AWS/1-Introduction/#cloudfront","text":"AWS CloudFront distribution is a collection of a CDN's Edge Locations. CloudFront content is cached in Edge Locations. This allows you to distribute content using a worldwide network of edge locations that provide low latency and high data transfer speeds. From AwS CloudFront select Create Distribution The CloudFront origin can be an S3 bucket, an EC2 instance, an Elastic Load Balancer or Route53. We can restrict viewer access using signed URLs for individual files or signed cookies for multiple files. Netflix or CloudGuru samples. OAI - O rigin A ccess I dentity If the origin is EC2 then use CloudFront Signed url If the origin is S3 then use S3 signed url We can create invalidations to remove origin objets on the edge location To delete a CloudFront distribution, you have to disable it first. This process takes 15 minutes Pricing depends on: traffic distribution requests data transfer out We will be charged when deleting cached data from an edge location","title":"CloudFront"},{"location":"AWS/1-Introduction/#resource-groups-and-tag-editor","text":"Tags allow to find AWS resources in a selected region, but it can not directly managing those resources. This can make it easier to search for and filter resources by purpose, owner, environment, or other criteria. Resources groups allow to execute operations from AWS Systems Manager to different resources (such as a EC2 fleet) based on resources groups","title":"Resource Groups and Tag Editor"},{"location":"AWS/2-Storage/","text":"2. Storage \u00b6 S3 \u00b6 AWS S3 is Object-based for the safe storage of flat files such as text files, videos, pictures and any other flat file from 0 to 5 tb. The object has a key (filename), value (data), versionID, metadata, encryption, and security by ACL ( A ccess C ontrol L ists), torrent and Bucket Policies Objects stored in S3 are stored in multiple servers in multiple facilities across AWS Buckets are folders/containers for everything that you store in S3. S3 bucket names are global , and must be unique. Universal namespaces are like https://s3-region_name.amazonaws.com/bucket_name. The response code is http 200 when file upload succeeds. Create new files are read inmediately. Updates and deletes takes a little bit of time to propagate. S3 classes: S3 Standard : Frequently accessed data S3 - IA ( I nfrequently A ccessed) S3 One Zone - IA (multiple AZ not required) aka RRS S3 - Intelligent Tiering , using machine learning to move files to the most cost-effective access tier S3 Glacier (low cost storage. retrieval times from 2 minutes to hours) S3 Glacier Deep Archive (lowest-cost. retrieval time of 12 hours is acceptable) It does not make sense to use S3 standard. Instead, use S3 - Intelligent Tiering to save money More features: Unlimited storage We can use the bucket to host a static website using S3 with option static website hosting eanbled, with index.html and error.html. Dynamic website can not be hosted on S3. Encryption: Encryption in Transit, using https (SSL/TLS) Encryption at Rest: In the server side ( S erver S ide E ncryption): SSE-S3 : An encryption key that Amazon S3 creates, manages, and uses for you. SSE-KMS : An encryption key protected by AWS K ey M anagament S ervice SSE-C with customer provided keys In the client side - encrypted before uploading Versioning (disabled by default): usefull as backup tool MFA Delete setting: an additional layer of security that requires MFA for changing Bucket Versioning settings and permanently deleting object versions. To modify MFA delete settings, use the AWS CLI, AWS SDK, or the Amazon S3 REST API. Cannot be disabled once enabled Uploaded new verions are private by default Deleting a file create a new version ( deleted marker ), then restoring the file recovers all previous versions. Integrated with lifecycle rules S3 Transfer Acceleration takes advantage of Amazon CloudFront (edge location - cache) using Amazon internal network (no internet). It enables fast, easy and secure transfers of files to and from your bucket. Speed Comparison Tool Cross Region Replication : replicate a bucket from a region to another: From S3 Management tab, select Create replication rule Create a new role and select the source bucket and destination bucket. Replicated buckets can be in different AWS accounts Replication requires versioning to be enable on the source and destionation buckets. we can change the storage class for the replicated objetcs replication starts when files are added and updated later. Deletes are not replicated. Permissions and previous versions are not replicated Use S3 Lifecycle rules to define actions you want AWS S3 to take during an object's lifetime such as transitioning objects to another storage class, archiving them, or deleting them after a specified period of time. Multipart uploads use multithreading to upload large files to S3 buckets in parallel (the parts of the file are uploaded in parallel). recommended for > 100mb and required for > 5Gb AWS DataSync is used to move large amounts of data from on-premise to AWS S3, EFS, FSx, etc. AWS DMS ( D atabase M igrations S ervice) is the best choice for conventional database migrations to AWS. homogenous: sample of Oracle to Oracle heterogenous with SCT ( S chema C onversion T ool): sample of SQL Server to Aurora General S3 FAQs S3 Pricing depends on: Storage class Storage Requests Data transfer Transfer acceleration Cross region replication To upload a file larger than 160 GB, use the AWS CLI, AWS SDK, or Amazon S3 REST API. S3 Security: Uploaded files are private by default Bucket policies applies to the entire bucket (like one hosting a static S3 website - public) Bucket ACLs. Object policies applies to an individual file. Public bucket policy sample: { \"Version\" : \"2020-09-08\" , // identify the document structure \"Statement\" : [ { \"Sid\" : \"PublicReadGetObject\" , // statement id \"Effect\" : \"Allow\" , // allow | deny \"Principal\" : \"*\" , \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" ] } ] } S3 Objetc lock are objects W ritten O nce and R ead M any. WORM model. Objects (the whole bucket or individual files) became unmodificable and undeletable: Governance mode: some users are grant with permissions to alter settings or delete the object version Compliance mode: objects version cannot be modified or deleted, even by the root user S3 Performance: Use prefixes to improve performance: path between the bucket and file Use SSE-KMS have limitations Use Multipart uploads Use downloads with S3 Byte-Range Fetches Use S3 Select / Glacier Select using SQL to download only the subset of data we need from ZIP or CSV files Glacier \u00b6 Glacier pricing: storage Data Retrieval times S3 Glacier Vault Lock Policy : compliance controls for S3 Glacier with a Vault Lock policy. The policy can no longer be changed EFS \u00b6 Based on NFSv4 , Amazon EFS ( E lastic F ile S ystem) is a mountable file storage service for EC2 (linux), but has no connection to S3 which is an object storage service. Data is stored across multiple AZ's within a region How to create an EFS shared by two EC2 instances: \u00b6 From AWS EFS, Create file system Create EC2 instances with following user data: #!/bin/bash yum update -y yum install httpd -y service httpd start chkconfig httpd on yum install amazon-efs-utils -y default SG require inbound rule of type NFSv4 ( N etwork F ile S ystem, port 2049) with source SG WebDMZ connect to the EC2 instances, and then mount EFS: cd /var/www/html cd .. mount -t efs -o tls fs-9816b269:/ /var/www/html # this command comes from `EFS - Amazon EC2 mount instructions from local VPC` cd html echo \"<html><body><h1>using EFS</h1></body></html>\" > index.html EBS \u00b6 AWS EBS ( E lastic B lock S torage) is like a virtual hard disk in the cloud. This is a block level storage service for use with AWS EC2 and again has no connection to S3. EBS cannot be shared by two EC2 instances an Amazon EBS volume attached as an additional disk (not the root volume) can be detached without stopping the instance EBS Types: SSD ( S olid S tate D rive) GP2 - General Purpose IO1 - Provisioned IOPS - Input Output per second - high perfrmance HDD ( H ard D isk D rive) ST1 - Throughtput optimised - Low cost for frequently access SC1 - Cold HDD - Lowest cost for less frequently access Magnetic - Previous generation HDD based volumes will always be less expensive than SSD types. EBS Pricing depends on: Volumes Snapshots Data transfer FSx \u00b6 AWS FSx for windows file server - MS Windows file system Based on SMB (Windows S erver M essage B lock) AWS FSx for Lustre Optimised file system can store data on S3","title":"2. Storage"},{"location":"AWS/2-Storage/#2-storage","text":"","title":"2. Storage"},{"location":"AWS/2-Storage/#s3","text":"AWS S3 is Object-based for the safe storage of flat files such as text files, videos, pictures and any other flat file from 0 to 5 tb. The object has a key (filename), value (data), versionID, metadata, encryption, and security by ACL ( A ccess C ontrol L ists), torrent and Bucket Policies Objects stored in S3 are stored in multiple servers in multiple facilities across AWS Buckets are folders/containers for everything that you store in S3. S3 bucket names are global , and must be unique. Universal namespaces are like https://s3-region_name.amazonaws.com/bucket_name. The response code is http 200 when file upload succeeds. Create new files are read inmediately. Updates and deletes takes a little bit of time to propagate. S3 classes: S3 Standard : Frequently accessed data S3 - IA ( I nfrequently A ccessed) S3 One Zone - IA (multiple AZ not required) aka RRS S3 - Intelligent Tiering , using machine learning to move files to the most cost-effective access tier S3 Glacier (low cost storage. retrieval times from 2 minutes to hours) S3 Glacier Deep Archive (lowest-cost. retrieval time of 12 hours is acceptable) It does not make sense to use S3 standard. Instead, use S3 - Intelligent Tiering to save money More features: Unlimited storage We can use the bucket to host a static website using S3 with option static website hosting eanbled, with index.html and error.html. Dynamic website can not be hosted on S3. Encryption: Encryption in Transit, using https (SSL/TLS) Encryption at Rest: In the server side ( S erver S ide E ncryption): SSE-S3 : An encryption key that Amazon S3 creates, manages, and uses for you. SSE-KMS : An encryption key protected by AWS K ey M anagament S ervice SSE-C with customer provided keys In the client side - encrypted before uploading Versioning (disabled by default): usefull as backup tool MFA Delete setting: an additional layer of security that requires MFA for changing Bucket Versioning settings and permanently deleting object versions. To modify MFA delete settings, use the AWS CLI, AWS SDK, or the Amazon S3 REST API. Cannot be disabled once enabled Uploaded new verions are private by default Deleting a file create a new version ( deleted marker ), then restoring the file recovers all previous versions. Integrated with lifecycle rules S3 Transfer Acceleration takes advantage of Amazon CloudFront (edge location - cache) using Amazon internal network (no internet). It enables fast, easy and secure transfers of files to and from your bucket. Speed Comparison Tool Cross Region Replication : replicate a bucket from a region to another: From S3 Management tab, select Create replication rule Create a new role and select the source bucket and destination bucket. Replicated buckets can be in different AWS accounts Replication requires versioning to be enable on the source and destionation buckets. we can change the storage class for the replicated objetcs replication starts when files are added and updated later. Deletes are not replicated. Permissions and previous versions are not replicated Use S3 Lifecycle rules to define actions you want AWS S3 to take during an object's lifetime such as transitioning objects to another storage class, archiving them, or deleting them after a specified period of time. Multipart uploads use multithreading to upload large files to S3 buckets in parallel (the parts of the file are uploaded in parallel). recommended for > 100mb and required for > 5Gb AWS DataSync is used to move large amounts of data from on-premise to AWS S3, EFS, FSx, etc. AWS DMS ( D atabase M igrations S ervice) is the best choice for conventional database migrations to AWS. homogenous: sample of Oracle to Oracle heterogenous with SCT ( S chema C onversion T ool): sample of SQL Server to Aurora General S3 FAQs S3 Pricing depends on: Storage class Storage Requests Data transfer Transfer acceleration Cross region replication To upload a file larger than 160 GB, use the AWS CLI, AWS SDK, or Amazon S3 REST API. S3 Security: Uploaded files are private by default Bucket policies applies to the entire bucket (like one hosting a static S3 website - public) Bucket ACLs. Object policies applies to an individual file. Public bucket policy sample: { \"Version\" : \"2020-09-08\" , // identify the document structure \"Statement\" : [ { \"Sid\" : \"PublicReadGetObject\" , // statement id \"Effect\" : \"Allow\" , // allow | deny \"Principal\" : \"*\" , \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" ] } ] } S3 Objetc lock are objects W ritten O nce and R ead M any. WORM model. Objects (the whole bucket or individual files) became unmodificable and undeletable: Governance mode: some users are grant with permissions to alter settings or delete the object version Compliance mode: objects version cannot be modified or deleted, even by the root user S3 Performance: Use prefixes to improve performance: path between the bucket and file Use SSE-KMS have limitations Use Multipart uploads Use downloads with S3 Byte-Range Fetches Use S3 Select / Glacier Select using SQL to download only the subset of data we need from ZIP or CSV files","title":"S3"},{"location":"AWS/2-Storage/#glacier","text":"Glacier pricing: storage Data Retrieval times S3 Glacier Vault Lock Policy : compliance controls for S3 Glacier with a Vault Lock policy. The policy can no longer be changed","title":"Glacier"},{"location":"AWS/2-Storage/#efs","text":"Based on NFSv4 , Amazon EFS ( E lastic F ile S ystem) is a mountable file storage service for EC2 (linux), but has no connection to S3 which is an object storage service. Data is stored across multiple AZ's within a region","title":"EFS"},{"location":"AWS/2-Storage/#how-to-create-an-efs-shared-by-two-ec2-instances","text":"From AWS EFS, Create file system Create EC2 instances with following user data: #!/bin/bash yum update -y yum install httpd -y service httpd start chkconfig httpd on yum install amazon-efs-utils -y default SG require inbound rule of type NFSv4 ( N etwork F ile S ystem, port 2049) with source SG WebDMZ connect to the EC2 instances, and then mount EFS: cd /var/www/html cd .. mount -t efs -o tls fs-9816b269:/ /var/www/html # this command comes from `EFS - Amazon EC2 mount instructions from local VPC` cd html echo \"<html><body><h1>using EFS</h1></body></html>\" > index.html","title":"How to create an EFS shared by two EC2 instances:"},{"location":"AWS/2-Storage/#ebs","text":"AWS EBS ( E lastic B lock S torage) is like a virtual hard disk in the cloud. This is a block level storage service for use with AWS EC2 and again has no connection to S3. EBS cannot be shared by two EC2 instances an Amazon EBS volume attached as an additional disk (not the root volume) can be detached without stopping the instance EBS Types: SSD ( S olid S tate D rive) GP2 - General Purpose IO1 - Provisioned IOPS - Input Output per second - high perfrmance HDD ( H ard D isk D rive) ST1 - Throughtput optimised - Low cost for frequently access SC1 - Cold HDD - Lowest cost for less frequently access Magnetic - Previous generation HDD based volumes will always be less expensive than SSD types. EBS Pricing depends on: Volumes Snapshots Data transfer","title":"EBS"},{"location":"AWS/2-Storage/#fsx","text":"AWS FSx for windows file server - MS Windows file system Based on SMB (Windows S erver M essage B lock) AWS FSx for Lustre Optimised file system can store data on S3","title":"FSx"},{"location":"AWS/3-Network/","text":"3. Network \u00b6 ENI - E lastic N etwork I nterface - virtual network card for basic networking. It can include multiple attributes, such as security groups, IPv6 and IPv4 addresses, MAC addresses, and more. ENA - E nhanced N etworking A dapter - use SR-IOV ( S ingle R oot I/O V irtualization) to allow speeds between 10 and 100 Gbps requirement EFA - E lastic F abric A dapter - machine learning or HPC ( H igh P erformance C omputing) requirement VPC \u00b6 AN INTERACTIVE IP ADDRESS AND CIDR RANGE VISUALIZER AWS VPC ( V irtual P rivate C loud) is like a logical datacenter in AWS. A VPC is an isolated portion of the AWS cloud dedicated to a single AWS account where you can launch AWS resources. You define a VPC\u2019s IP address space from ranges you select (10.0.0.0/16). Subnets are segments of a VPC\u2019s IP address range where you can place groups of isolated resources (10.0.1.0/24). 1 subnet = 1 AZ Internet Gateway allow communication between your VPC and the internet. An IG serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses 1 VPC = 1 IG egress-only internet gateway allows IPv6 based traffic within a VPC to access the internet, whilst denying any internet based resources to connection back into the VPC. VPG ( V irtual P rivate G ateway) is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection. customer gateway is a resource that is installed on the customer side and provides a customer gateway inside a VPC. VPC peering creates a connection between two VPCs using same or different accounts and regions. no transitive peering VPC-A <=> VPC-B <=> VPC-C ... VPC-A <> VPC-C VPC Endpoints : Enables private connectivity to services hosted in AWS, from within your VPC without using an Internet Gateway, VPN, Network Address Translation (NAT) devices, or firewall proxies. VPC pricing: traffic in the VPC is free using same AZ with private IP is free using different AZs and public IP has a cost Some scans can be performed without alerting AWS, some require you to alert, such as Penetration Testing You can have up to 5 non-default VPCs per account and region, but you can place a support request to increase the number. How to create a VPC \u00b6 Creating a Basic VPC and Associated Components in AWS Create VPC with IPv4 CIDR block equals 10.0.0.0/16 and IPv6 CIDR block provided by Amazon. Note that only a RT, a network ACL and a SG are created Create subnet with IPv4 CIDR block equals 10.0.1.0/24 (public) and 10.0.2.0/24 (private) with different AZs. AWS reserve 5 addresses so there are 251 available IPv4 addresses instead of 256 Select the public subnet and from Actions, clic on Modify auto-assign IP settings to automatically request a public IP for a new network interface in this subnet. Create internet gateway as the virtual router that connects the VPC to the internet. Select the IG and from Actions, clic on attach to VPC aws ec2 attach-internet-gateway --vpc-id \"vpc-05fa9d6e7085db9bf\" --internet-gateway-id \"igw-0e2c1cfa68b4dfc76\" --region us-east-2 Create route table to specify how packets are forwarded between the subnets within your VPC, the internet, and your VPN connection. New subnets will be associated to the main route table instead of this public RT. Allow the internet access selecting the public RT and Edit routes button and add route with destination 0.0.0.0/0 (IPv4) and ::/0 (IPv6) and our IG as the target Edit subnet associations and select 10.0.1.0/24 to associate the public subnet with the public RT Create first EC2 instances as WebServer with network equals our VPC select public subnet and auto-assign public ip equals use subnet settings (enabled) for the first instance Create new SG as WebDMZ with rules SSH and HTTP Create a new key-pair as rnietoe-ohio chmod 400 rnietoe-ohio.pem ssh ec2-user@3.16.203.67 -i rnietoe-ohio.pem Create second EC2 instances as DBServer with network equals our VPC select private subnet and auto-assign public ip equals use subnet settings (disabled) for the second instance Select default SG instead of WebDMZ there is no public IP so we can't connect to this instance Create new SG as DBSG with our VPC Add inbound rules of type All ICMP - IPv2 with source 10.0.1.0/24 to allow WebDMZ to ping EC2 instance inside this SG, HTTP, HTTPS, SSH and MySQL/Aurora Set DBSG to DBServer from actions : security : change security group . Remove default SG too Ping the DBServer private IP from WebServer ping 10 .0.2.200 Copy private key (rnietoe-ohio.pem) to WebServer and check the connection from WebServer to DBServer nano rnietoe-ohio.pem chmod 400 rnietoe-ohio.pem ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # this fail because private instance does not have access to internet By default, instances in new subnets in a custom VPC can communicate with each other across AZs. How to create a NAT Gateway \u00b6 NAT instances ( N etwork A ddress T ransaction) are single EC2 instances. NAT Gateway is a highly available gateway that allows you to get your private subnets communicate out to the internet without becaming public NAT gateway enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. NAT gateway are redundant inside the AZ 1 NAT gateway per AZ not need to patch not associated with SGs no need to disable source/destination check Launch EC2 instance choosing Amazon Linux 2 AMI 2.0.20201126.0 x86_64 HVM gp2 from Community AMIs Select rnietoeVPC and public subnet Select WebDMZ as SG and the same key pair Select EC2 instance and clic on actions : networking : change source/destination check and disable all the traffic it sends and receives, as NAT gateway requirement aws ec2 modify-instance-attribute --instance-id = i-0cdece2dd619e009b --no-source-dest-check Edit private route table and create a route (from edit routes ) to allow connections (0.0.0.0/0) to the NAT instance ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # should work now, but it does not we have created a small VM that will not work for thouthands of EC2 instances. To use a NAT gateway, create one in a public subnet and assign it an Elastic IP address. Then, update the route tables for your private subnets to point internet traffic to the NAT gateway. Terminate EC2 NAT instance From VPC, create NAT gateway Select public subnet and allocate a new Elastic IP Edit private route table and create a route (from edit routes ) to allow connections (0.0.0.0/0) to the NAT gateway ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # this work successfully yum install mysql -y Elastic IP / NAT Gateway are not free Network ACLs \u00b6 SG are statefull while network ACLs are stateless (inbounds settings are not applied to outbound settings implicitly) Block IP addresses using NACL instead of SG 1 NACL - 1 subnet, NACLs act on the subnet level, while security groups act on the instance level. Default NACL allow all traffic Create Network ACL as WebNACL using our VPC. All inbound rules are denied by default Create a Web page in the EC2 WebServer and check the valid connection: service httpd status # Unit httpd.service could not be found sudo su yum install httpd -y chkconfig httpd on service httpd start cd /var/www/html nano index.html <html><body><h1>This is server 1 </h1></body></html> Edit subnet associations for our NACL named WebNACL and select the public subnet public subnet is disassociate from the default NACL becuase a subnet can be associated to a NACL only 1 NACL - many subnets web page is not accessible now Edit inbound rules adding new rules (100, 200, 300) allowing ports 80, 443 and 22 Rule Number increase in 100 units, like 100, 200, 300... inbound rules work order by rule number, so firt allow some ports and then deny everything else Edit outbound rules adding new rules allowing ports 80, 443 and 1024-65535 NACL are stateless: outbound rules have to be defined explicitly Ephemeral ports Requests originating from Elastic Load Balancing use ports 1024-65535. Edit inbound rules adding new rule 400 denying port 80 to my public Ip 92.189.102.194/32 Web page still accessible since rule 100 allow the traffic Edit inbound rules editing rule 400 as 99 Web page no accessible since rule 99 deny the traffic before rule 100 allow all traffic This is not working in my demo Edit inbound rules removing rule 99 and add new rule 400 to allow traffic on ports 1024-65535 yum update -y # it should works again``` VPC FlowLogs \u00b6 VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. VPC Flow Logs can be created at the VPC, subnet, and network interface levels. you can enable flow logs if VPC is in the same aws account you can not edit flow logs not all ip traffic is monitored such as: Amazon DNS Server Windows license activation 169.254.169.254 DHCP traffic VPC reserved ports How to configure VPC FlowLogs: Create VPC with ipv4 CIDR block as 10.0.0.0/16 Create subnet with ipv4 CIDR block as 10.0.1.0/24 and enable auto-assign public IPv4 address Launch EC2 instance with VPC, subnet and a new SG with HTTP and SSH rules Create internet gateway and associate it to the default VPC RT Create flow log in the Network interface (eni) specify the filter: accepted traffic only, rejected traffic only or capture all traffic set Maximum aggregation interval to 1 min flow log data destination can be CloudWatch Logs or S3 bucket. Select S3 bucket Create bucket as arn:aws:s3:::rnietoeflowlogs Specify AWS default format Create log group from CloudWatch as VPCFlowLogs Create flow log again specify the filter with all tthe traffic set Maximum aggregation interval to 1 min set flow log data destination as CloudWatch Logs select the destination log group as VPCFlowLogs set up permissions to define the IAM role that has permission to publish to the Amazon CloudWatch log group. Create new role as flowlogsRole with the following policy: { \"Statement\" : [ { \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:CreateLogStream\" , \"logs:DescribeLogGroups\" , \"logs:DescribeLogStreams\" , \"logs:PutLogEvents\" ], \"Effect\" : \"Allow\" , \"Resource\" : \"*\" } ] } Go to CloudWatch : Log groups : VPCFlowLogs : Log stream and check results Create filter pattern as (protocol 6 is TCP): [version, account, eni, source, destination, srcport, destport=\"22\", protocol=\"6\", packets, bytes, windowstart, windowend, action=\"ACCEPT\", flowlogstatus] test pattern 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 22 6 1 40 1563108188 1563108227 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 182.68.238.8 172.31.83.158 42227 22 6 1 44 1563109030 1563109067 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 42.171.23.181 172.31.83.158 52417 22 6 24 4065 1563191069 1563191121 ACCEPT OK 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 80 6 1 40 1563108188 1563108227 REJECT OK set metric details Create alarm for above metric filter set period to 1 min Whenever SSHAccept is greater/equal than 1 Run query from CloudWatch Logs Insights using VPC Flow Logs sample queries (right panel) Go back to S3 and check a new folder named AWSLogs has been created in our rnietoeflowlogs bucket Go to AWS Athena and set up a query result location in AWS S3 with the arn: s3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/2020/12/12/ Run query to create Athena Table CREATE EXTERNAL TABLE IF NOT EXISTS default . vpc_flow_logs ( version int , account string , interfaceid string , sourceaddress string , destinationaddress string , sourceport int , destinationport int , protocol int , numpackets int , numbytes bigint , starttime int , endtime int , action string , logstatus string ) PARTITIONED BY ( dt string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' LOCATION 's3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/' TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); Create Partitions ALTER TABLE default . vpc_flow_logs ADD PARTITION ( dt = '2020-12-12' ) location 's3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/2020/12/12' ; Analyze Data SELECT day_of_week ( from_iso8601_timestamp ( dt )) AS day , dt , interfaceid , sourceaddress , destinationport , action , protocol FROM vpc_flow_logs WHERE action = 'ACCEPT' AND protocol = 6 order by sourceaddress LIMIT 100 ; Bastion host \u00b6 a bastion (jump box) is used to securely administer/manage EC2 instances using SSH or RDP NAT gateway cannot be used as a bastion host Direct Connect \u00b6 Dedicated line from on premise to AWS to improve the network connection (security and performance) Create virtual interface from AWS Direct Connect : Virtual interfaces as Public Create Customer Gateway from VPC : Customer Gateways Create Virtual Private Gateway from VPC : Virtual Private Gateways attach the VPG to the VPC Create VPN Connection from VPC : Site-to-Site VPN Connections using the VPG and Customer Gateway Set up the VPN on the customer gateway How do I configure a VPN over AWS Direct Connect? Global Accelerator \u00b6 improves the availability and performance of your applications for local or global users 1234567890abcdef.awsglobalaccelerator.com Create accelerator from AWS Global Accelerator Add a listener to checks for connection requests that arrive to an assigned set of static IP addresses on a port or port range that you specify. (80, 443) Leave client affinity setting as none Add endpoint groups where the accelerator direct traffic to from one or more listeners. An endpoint group includes endpoints, such as load balancers. Add endpoints to each endpoint group Endpoints can be Network Load Balancers, Application Load Balancers, EC2 instances, or Elastic IP addresses. two static IP addresses are assign Disable Global Accelerator before removing is required Network zones are simliar to AZs. They are isolated units with their own set of physical infrastructure and service IP addresses from a unique IP subnet. If one IP address from a network zone becomes unavailable, due to network disruptions or IP address blocking by certain client networks, your client applications can retry using the healthy static IP address from the other isolated network zone. VPC endpoints \u00b6 VPC endpoint is a service that replace NAT gateway and allow connections from the private subnet to other AWS services, such as S3 Interface endpoints: ENI ( E lastic N etwork I nterface) with private IP as entry point Gateway endpoints support S3 and DynamoDB Create Endpoint from VPC : Endpoints select service com.amazonaws.us-east-2.s3 , our VPC, our main route table and the full access policy the update in the route table could take some time AWS Private Link \u00b6 To open up our apps to other VPCs, we can try: Open up the VPC to the internet. everything will be public use VPC peering . However, many relationships will be required AWS Private Link peers many VPCs. They only require a NLB on the AWS VPC and a ENI on the customer VPC Transit Gateway \u00b6 TGW ( T ransit G ate W ay) is a network transit hub that interconnects attachments (VPCs and VPNs) within the same account or across accounts. Cross region is allowed support IP multicast VPN CloudHub \u00b6 AWS VPN CloudHub manage multiple sites with own VPN connections Route 53 \u00b6 AWS Route 53 service name comes from port 53, where DNS work on we can register a DNS using Route53 - Register domain . You can purchase and manage domain names such as example.com, and Route 53 will automatically configure DNS settings for your domains Ensure there is a free bucket with the same domain name Failover Routing policy routes data to a second resource if the first is unhealthy. Route 53 can be used for Disaster Recovery by simply shifting traffic to the new region. Latency-based Routing policy routes data to resources that have better performance Route 53 Traffic Flow makes it easy for you to manage traffic globally through a variety of routing types, including Latency Based Routing, Geo DNS, Geoproximity, and Weighted Round Robin\u2014all of which can be combined with DNS Failover to enable a variety of low-latency, fault-tolerant architectures. Using Route 53 Traffic Flow\u2019s simple visual editor, you can easily manage how your end-users are routed to your application\u2019s endpoints\u2014whether in a single AWS region or distributed around the globe. .com => NS (Name Server) Records => SOA (Start Of Authority) DNS changes can take 48 hours to take effect due to the cache CName (Canonical Name) maps to the host name: https://mobile.acloud.guru = https://m.acloud.guru Alias Record provide a Route 53\u2013specific extension to DNS functionality. An alias could be created for the ELB. Alias Records can also point to AWS Resources that are hosted in other accounts by manually entering the ARN ELB resolve DNS names isnteaf of IPv4 addresses Routing Policies: Simple Routing: one dns record with multiple IP addresses Weighted Routing: traffic based on weighting (ponderaciones 20%-30%-50%) Latency-based Routing: traffic based on the lowest latency Failover Routing: route the traffic to the primary or secondary site defined based on health checks Geolocation Routing: traffic based on the user's location Geoproximity Routing: traffic based on the users' and resources' location. Available in traffic flow-only mode using bias Multivalue Answer Routing, similar to simple routing, but using health checks on each record sets to serve traffic to random web servers Register domain from Route53 takes between 2 hours and 3 days Create Record Set of type IPv4 address with the three EC2 public IPs. Set TTL (Time to Live) to 1 min to clear from cache Create healthcheck and associate it to each record set, so it will be removed from Route53 until it passes the health check Create traffic policy to configure Geoproximity Routing ipconfig /flushdns # to remove saved ip from cache With Route 53, there is a default limit of 50 domain names. However, this limit can be increased by contacting AWS suppor","title":"3. Network"},{"location":"AWS/3-Network/#3-network","text":"ENI - E lastic N etwork I nterface - virtual network card for basic networking. It can include multiple attributes, such as security groups, IPv6 and IPv4 addresses, MAC addresses, and more. ENA - E nhanced N etworking A dapter - use SR-IOV ( S ingle R oot I/O V irtualization) to allow speeds between 10 and 100 Gbps requirement EFA - E lastic F abric A dapter - machine learning or HPC ( H igh P erformance C omputing) requirement","title":"3. Network"},{"location":"AWS/3-Network/#vpc","text":"AN INTERACTIVE IP ADDRESS AND CIDR RANGE VISUALIZER AWS VPC ( V irtual P rivate C loud) is like a logical datacenter in AWS. A VPC is an isolated portion of the AWS cloud dedicated to a single AWS account where you can launch AWS resources. You define a VPC\u2019s IP address space from ranges you select (10.0.0.0/16). Subnets are segments of a VPC\u2019s IP address range where you can place groups of isolated resources (10.0.1.0/24). 1 subnet = 1 AZ Internet Gateway allow communication between your VPC and the internet. An IG serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses 1 VPC = 1 IG egress-only internet gateway allows IPv6 based traffic within a VPC to access the internet, whilst denying any internet based resources to connection back into the VPC. VPG ( V irtual P rivate G ateway) is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection. customer gateway is a resource that is installed on the customer side and provides a customer gateway inside a VPC. VPC peering creates a connection between two VPCs using same or different accounts and regions. no transitive peering VPC-A <=> VPC-B <=> VPC-C ... VPC-A <> VPC-C VPC Endpoints : Enables private connectivity to services hosted in AWS, from within your VPC without using an Internet Gateway, VPN, Network Address Translation (NAT) devices, or firewall proxies. VPC pricing: traffic in the VPC is free using same AZ with private IP is free using different AZs and public IP has a cost Some scans can be performed without alerting AWS, some require you to alert, such as Penetration Testing You can have up to 5 non-default VPCs per account and region, but you can place a support request to increase the number.","title":"VPC"},{"location":"AWS/3-Network/#how-to-create-a-vpc","text":"Creating a Basic VPC and Associated Components in AWS Create VPC with IPv4 CIDR block equals 10.0.0.0/16 and IPv6 CIDR block provided by Amazon. Note that only a RT, a network ACL and a SG are created Create subnet with IPv4 CIDR block equals 10.0.1.0/24 (public) and 10.0.2.0/24 (private) with different AZs. AWS reserve 5 addresses so there are 251 available IPv4 addresses instead of 256 Select the public subnet and from Actions, clic on Modify auto-assign IP settings to automatically request a public IP for a new network interface in this subnet. Create internet gateway as the virtual router that connects the VPC to the internet. Select the IG and from Actions, clic on attach to VPC aws ec2 attach-internet-gateway --vpc-id \"vpc-05fa9d6e7085db9bf\" --internet-gateway-id \"igw-0e2c1cfa68b4dfc76\" --region us-east-2 Create route table to specify how packets are forwarded between the subnets within your VPC, the internet, and your VPN connection. New subnets will be associated to the main route table instead of this public RT. Allow the internet access selecting the public RT and Edit routes button and add route with destination 0.0.0.0/0 (IPv4) and ::/0 (IPv6) and our IG as the target Edit subnet associations and select 10.0.1.0/24 to associate the public subnet with the public RT Create first EC2 instances as WebServer with network equals our VPC select public subnet and auto-assign public ip equals use subnet settings (enabled) for the first instance Create new SG as WebDMZ with rules SSH and HTTP Create a new key-pair as rnietoe-ohio chmod 400 rnietoe-ohio.pem ssh ec2-user@3.16.203.67 -i rnietoe-ohio.pem Create second EC2 instances as DBServer with network equals our VPC select private subnet and auto-assign public ip equals use subnet settings (disabled) for the second instance Select default SG instead of WebDMZ there is no public IP so we can't connect to this instance Create new SG as DBSG with our VPC Add inbound rules of type All ICMP - IPv2 with source 10.0.1.0/24 to allow WebDMZ to ping EC2 instance inside this SG, HTTP, HTTPS, SSH and MySQL/Aurora Set DBSG to DBServer from actions : security : change security group . Remove default SG too Ping the DBServer private IP from WebServer ping 10 .0.2.200 Copy private key (rnietoe-ohio.pem) to WebServer and check the connection from WebServer to DBServer nano rnietoe-ohio.pem chmod 400 rnietoe-ohio.pem ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # this fail because private instance does not have access to internet By default, instances in new subnets in a custom VPC can communicate with each other across AZs.","title":"How to create a VPC"},{"location":"AWS/3-Network/#how-to-create-a-nat-gateway","text":"NAT instances ( N etwork A ddress T ransaction) are single EC2 instances. NAT Gateway is a highly available gateway that allows you to get your private subnets communicate out to the internet without becaming public NAT gateway enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. NAT gateway are redundant inside the AZ 1 NAT gateway per AZ not need to patch not associated with SGs no need to disable source/destination check Launch EC2 instance choosing Amazon Linux 2 AMI 2.0.20201126.0 x86_64 HVM gp2 from Community AMIs Select rnietoeVPC and public subnet Select WebDMZ as SG and the same key pair Select EC2 instance and clic on actions : networking : change source/destination check and disable all the traffic it sends and receives, as NAT gateway requirement aws ec2 modify-instance-attribute --instance-id = i-0cdece2dd619e009b --no-source-dest-check Edit private route table and create a route (from edit routes ) to allow connections (0.0.0.0/0) to the NAT instance ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # should work now, but it does not we have created a small VM that will not work for thouthands of EC2 instances. To use a NAT gateway, create one in a public subnet and assign it an Elastic IP address. Then, update the route tables for your private subnets to point internet traffic to the NAT gateway. Terminate EC2 NAT instance From VPC, create NAT gateway Select public subnet and allocate a new Elastic IP Edit private route table and create a route (from edit routes ) to allow connections (0.0.0.0/0) to the NAT gateway ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # this work successfully yum install mysql -y Elastic IP / NAT Gateway are not free","title":"How to create a NAT Gateway"},{"location":"AWS/3-Network/#network-acls","text":"SG are statefull while network ACLs are stateless (inbounds settings are not applied to outbound settings implicitly) Block IP addresses using NACL instead of SG 1 NACL - 1 subnet, NACLs act on the subnet level, while security groups act on the instance level. Default NACL allow all traffic Create Network ACL as WebNACL using our VPC. All inbound rules are denied by default Create a Web page in the EC2 WebServer and check the valid connection: service httpd status # Unit httpd.service could not be found sudo su yum install httpd -y chkconfig httpd on service httpd start cd /var/www/html nano index.html <html><body><h1>This is server 1 </h1></body></html> Edit subnet associations for our NACL named WebNACL and select the public subnet public subnet is disassociate from the default NACL becuase a subnet can be associated to a NACL only 1 NACL - many subnets web page is not accessible now Edit inbound rules adding new rules (100, 200, 300) allowing ports 80, 443 and 22 Rule Number increase in 100 units, like 100, 200, 300... inbound rules work order by rule number, so firt allow some ports and then deny everything else Edit outbound rules adding new rules allowing ports 80, 443 and 1024-65535 NACL are stateless: outbound rules have to be defined explicitly Ephemeral ports Requests originating from Elastic Load Balancing use ports 1024-65535. Edit inbound rules adding new rule 400 denying port 80 to my public Ip 92.189.102.194/32 Web page still accessible since rule 100 allow the traffic Edit inbound rules editing rule 400 as 99 Web page no accessible since rule 99 deny the traffic before rule 100 allow all traffic This is not working in my demo Edit inbound rules removing rule 99 and add new rule 400 to allow traffic on ports 1024-65535 yum update -y # it should works again```","title":"Network ACLs"},{"location":"AWS/3-Network/#vpc-flowlogs","text":"VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. VPC Flow Logs can be created at the VPC, subnet, and network interface levels. you can enable flow logs if VPC is in the same aws account you can not edit flow logs not all ip traffic is monitored such as: Amazon DNS Server Windows license activation 169.254.169.254 DHCP traffic VPC reserved ports How to configure VPC FlowLogs: Create VPC with ipv4 CIDR block as 10.0.0.0/16 Create subnet with ipv4 CIDR block as 10.0.1.0/24 and enable auto-assign public IPv4 address Launch EC2 instance with VPC, subnet and a new SG with HTTP and SSH rules Create internet gateway and associate it to the default VPC RT Create flow log in the Network interface (eni) specify the filter: accepted traffic only, rejected traffic only or capture all traffic set Maximum aggregation interval to 1 min flow log data destination can be CloudWatch Logs or S3 bucket. Select S3 bucket Create bucket as arn:aws:s3:::rnietoeflowlogs Specify AWS default format Create log group from CloudWatch as VPCFlowLogs Create flow log again specify the filter with all tthe traffic set Maximum aggregation interval to 1 min set flow log data destination as CloudWatch Logs select the destination log group as VPCFlowLogs set up permissions to define the IAM role that has permission to publish to the Amazon CloudWatch log group. Create new role as flowlogsRole with the following policy: { \"Statement\" : [ { \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:CreateLogStream\" , \"logs:DescribeLogGroups\" , \"logs:DescribeLogStreams\" , \"logs:PutLogEvents\" ], \"Effect\" : \"Allow\" , \"Resource\" : \"*\" } ] } Go to CloudWatch : Log groups : VPCFlowLogs : Log stream and check results Create filter pattern as (protocol 6 is TCP): [version, account, eni, source, destination, srcport, destport=\"22\", protocol=\"6\", packets, bytes, windowstart, windowend, action=\"ACCEPT\", flowlogstatus] test pattern 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 22 6 1 40 1563108188 1563108227 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 182.68.238.8 172.31.83.158 42227 22 6 1 44 1563109030 1563109067 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 42.171.23.181 172.31.83.158 52417 22 6 24 4065 1563191069 1563191121 ACCEPT OK 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 80 6 1 40 1563108188 1563108227 REJECT OK set metric details Create alarm for above metric filter set period to 1 min Whenever SSHAccept is greater/equal than 1 Run query from CloudWatch Logs Insights using VPC Flow Logs sample queries (right panel) Go back to S3 and check a new folder named AWSLogs has been created in our rnietoeflowlogs bucket Go to AWS Athena and set up a query result location in AWS S3 with the arn: s3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/2020/12/12/ Run query to create Athena Table CREATE EXTERNAL TABLE IF NOT EXISTS default . vpc_flow_logs ( version int , account string , interfaceid string , sourceaddress string , destinationaddress string , sourceport int , destinationport int , protocol int , numpackets int , numbytes bigint , starttime int , endtime int , action string , logstatus string ) PARTITIONED BY ( dt string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' LOCATION 's3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/' TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); Create Partitions ALTER TABLE default . vpc_flow_logs ADD PARTITION ( dt = '2020-12-12' ) location 's3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/2020/12/12' ; Analyze Data SELECT day_of_week ( from_iso8601_timestamp ( dt )) AS day , dt , interfaceid , sourceaddress , destinationport , action , protocol FROM vpc_flow_logs WHERE action = 'ACCEPT' AND protocol = 6 order by sourceaddress LIMIT 100 ;","title":"VPC FlowLogs"},{"location":"AWS/3-Network/#bastion-host","text":"a bastion (jump box) is used to securely administer/manage EC2 instances using SSH or RDP NAT gateway cannot be used as a bastion host","title":"Bastion host"},{"location":"AWS/3-Network/#direct-connect","text":"Dedicated line from on premise to AWS to improve the network connection (security and performance) Create virtual interface from AWS Direct Connect : Virtual interfaces as Public Create Customer Gateway from VPC : Customer Gateways Create Virtual Private Gateway from VPC : Virtual Private Gateways attach the VPG to the VPC Create VPN Connection from VPC : Site-to-Site VPN Connections using the VPG and Customer Gateway Set up the VPN on the customer gateway How do I configure a VPN over AWS Direct Connect?","title":"Direct Connect"},{"location":"AWS/3-Network/#global-accelerator","text":"improves the availability and performance of your applications for local or global users 1234567890abcdef.awsglobalaccelerator.com Create accelerator from AWS Global Accelerator Add a listener to checks for connection requests that arrive to an assigned set of static IP addresses on a port or port range that you specify. (80, 443) Leave client affinity setting as none Add endpoint groups where the accelerator direct traffic to from one or more listeners. An endpoint group includes endpoints, such as load balancers. Add endpoints to each endpoint group Endpoints can be Network Load Balancers, Application Load Balancers, EC2 instances, or Elastic IP addresses. two static IP addresses are assign Disable Global Accelerator before removing is required Network zones are simliar to AZs. They are isolated units with their own set of physical infrastructure and service IP addresses from a unique IP subnet. If one IP address from a network zone becomes unavailable, due to network disruptions or IP address blocking by certain client networks, your client applications can retry using the healthy static IP address from the other isolated network zone.","title":"Global Accelerator"},{"location":"AWS/3-Network/#vpc-endpoints","text":"VPC endpoint is a service that replace NAT gateway and allow connections from the private subnet to other AWS services, such as S3 Interface endpoints: ENI ( E lastic N etwork I nterface) with private IP as entry point Gateway endpoints support S3 and DynamoDB Create Endpoint from VPC : Endpoints select service com.amazonaws.us-east-2.s3 , our VPC, our main route table and the full access policy the update in the route table could take some time","title":"VPC endpoints"},{"location":"AWS/3-Network/#aws-private-link","text":"To open up our apps to other VPCs, we can try: Open up the VPC to the internet. everything will be public use VPC peering . However, many relationships will be required AWS Private Link peers many VPCs. They only require a NLB on the AWS VPC and a ENI on the customer VPC","title":"AWS Private Link"},{"location":"AWS/3-Network/#transit-gateway","text":"TGW ( T ransit G ate W ay) is a network transit hub that interconnects attachments (VPCs and VPNs) within the same account or across accounts. Cross region is allowed support IP multicast","title":"Transit Gateway"},{"location":"AWS/3-Network/#vpn-cloudhub","text":"AWS VPN CloudHub manage multiple sites with own VPN connections","title":"VPN CloudHub"},{"location":"AWS/3-Network/#route-53","text":"AWS Route 53 service name comes from port 53, where DNS work on we can register a DNS using Route53 - Register domain . You can purchase and manage domain names such as example.com, and Route 53 will automatically configure DNS settings for your domains Ensure there is a free bucket with the same domain name Failover Routing policy routes data to a second resource if the first is unhealthy. Route 53 can be used for Disaster Recovery by simply shifting traffic to the new region. Latency-based Routing policy routes data to resources that have better performance Route 53 Traffic Flow makes it easy for you to manage traffic globally through a variety of routing types, including Latency Based Routing, Geo DNS, Geoproximity, and Weighted Round Robin\u2014all of which can be combined with DNS Failover to enable a variety of low-latency, fault-tolerant architectures. Using Route 53 Traffic Flow\u2019s simple visual editor, you can easily manage how your end-users are routed to your application\u2019s endpoints\u2014whether in a single AWS region or distributed around the globe. .com => NS (Name Server) Records => SOA (Start Of Authority) DNS changes can take 48 hours to take effect due to the cache CName (Canonical Name) maps to the host name: https://mobile.acloud.guru = https://m.acloud.guru Alias Record provide a Route 53\u2013specific extension to DNS functionality. An alias could be created for the ELB. Alias Records can also point to AWS Resources that are hosted in other accounts by manually entering the ARN ELB resolve DNS names isnteaf of IPv4 addresses Routing Policies: Simple Routing: one dns record with multiple IP addresses Weighted Routing: traffic based on weighting (ponderaciones 20%-30%-50%) Latency-based Routing: traffic based on the lowest latency Failover Routing: route the traffic to the primary or secondary site defined based on health checks Geolocation Routing: traffic based on the user's location Geoproximity Routing: traffic based on the users' and resources' location. Available in traffic flow-only mode using bias Multivalue Answer Routing, similar to simple routing, but using health checks on each record sets to serve traffic to random web servers Register domain from Route53 takes between 2 hours and 3 days Create Record Set of type IPv4 address with the three EC2 public IPs. Set TTL (Time to Live) to 1 min to clear from cache Create healthcheck and associate it to each record set, so it will be removed from Route53 until it passes the health check Create traffic policy to configure Geoproximity Routing ipconfig /flushdns # to remove saved ip from cache With Route 53, there is a default limit of 50 domain names. However, this limit can be increased by contacting AWS suppor","title":"Route 53"},{"location":"AWS/4-Compute/","text":"4. Compute \u00b6 EC2 \u00b6 AWS EC2 ( E lastic C ompute C loud) is a virtual server in the cloud. It is a web service that provides resizeable compute capacity in the cloud EC2 fleet - multiple EC2 instances. They are manage by AWS System Manager Spot fleet - multiple Spot (and on demand) instances AWS originally used a modified version of the Xen Hypervisor to host EC2. In 2017, AWS began rolling out their own Hypervisor called Nitro Placement groups \u00b6 Clustered : Group homogenous EC2 instances within a single AZ for network performance Spread : Individial EC2 instances are placed on distinct rack within one region for hardware errors Partitiononed : Multiple EC2 instances in the same rack Pricing models: \u00b6 On Demand : low cost, paying by hour or second. You have full control over its lifecycle\u2014you decide when to launch, stop, hibernate, start, reboot, or terminate it. Sample: when task run uninterrupted from start to finish Reserved : the most economical option for long-term workloads with predictable usage patterns. Contract terms are 1 to 3 years. It includes different discounts Standard Reserved instances (75% off on demand instances) Convertible Reserved instances (54%) Schedule Reserved instances, based on times Spot : taket advantage of unused EC2 capacity. It can accept interruptions. Used for various stateless, fault-tolerant, or flexible applications such as big data, containerized workloads, CI/CD, web servers, HPC (high-performance computing), and other test & development workloads. Extra charge when you terminate the instance Spot Instances are available at up to a 90% discount compared to On-Demand prices. Dedicated : physical EC2 server. It reduces cost using your SW licenses. Also when multitenant not supported by law Standard Reserved Instances cannot be moved between regions. You can choose if a Reserved Instance applies to either a specific AZ, or an Entire Region, but you cannot change the region EC2 Pricing depends on: Clock hours of server time instance type pricing model (on demand, reserved, spot, dedicated host) number of instances load balancing detailed monitoring auto scaling Elastic IP Addresses Operative Systems and sw packages AMI \u00b6 ( A mazon M achine I mange) are instance image snapshots of different Operative System AMI are based on region, OS, architecture (32 or 64 bits), launch permissions and storage for the root volume (EBS or Instance store - ephemeral storage) EC2 instance with Instance Store can't be stopped Instance Store does not appear in the AWS EC2 Volume list To use hibernation, the root volume must be an encrypted EBS volume. RAM be less than 150gb Use snapshots and AMI to change EC2 volumes (AZ and encryption). Snapshots are incremental You must first deregister the AMI before you can delete the snapshot You can use AWS APIs, CLI or the AWS Console to copy snapshots, share snapshots, and create volumes from snapshots. EBS snapshots use incremental backups and are stored in S3. Volumes exist on EBS. Snapshots and instance store exist on S3. aws ec2 create-snapshot How to create EC2: \u00b6 From AWS EC2, Launch instance Select AMI, for example: Amazon Linux 2 AMI Choose an Instance Type (t2 micro - free tier) Configure Instance Details (number of instances) Enable CloudWatch detailed monitoring is not free Add Storage. Root and EBS volume types allow encryption and delete on termination (turn off by default on EBS) EC2 instance and volume are in the same AZ Add tags like Name, Department or Employee_Id Configure Security Groups - virtual firewalls to enable traffic (types ssh & http - ports 22 & 80) Security groups support \"allow\" rules only. All inbound traffic is blocked by default and all outbound traffic is allowed (SG are STATEFUL) Linux=SSH port 22. Microsoft Windows= RDP (Remote Desktop Protocol) port 3389. http/https ports 80/443 SG changes are take effect immediately one or more SG can be assigned to EC2 instance. EC2 and SG relationshipt is many to many Security groups operate at the instance level. use Network ACL to block specific IP address instead of SG Launch using a key pair (public and private key) Always design for failure. Have one EC2 instance in each AZ EC2 instance are provisioned in AZ Creating an Elastic IP address and associate it with your instance would be the simplest way to make your instance reachable from the outside world. How to connect to EC2 \u00b6 Using AWS Console, select the EC2 instance and clic on the Connect button Using putty: Download putty and load private key file created at EC2/Key Pairs (rnietoe.ppk) using putty gen. Key pair can created or imported in AWS. You can create you key pair with the following command, keep the private key and import the public key in AWS: ssh-keygen -C rnietoe@gmail.com -f ~/.ssh/rnietoe Configure SSH Auth with private key Copy IP address to the session host name field Open connection, login as ec2-user and type sudo su command Using gitbash and install a web server: cd \"C:\\Users\\rniet\\OneDrive\\AWS\" CHMOD 400 rnietoe.pem # change permissions to lock my key down ssh ec2-user@3.80.39.184 -i rnietoe.pem sudo su uname -a # software details cat ~/.ssh/authorized_keys # the public key while true ; do echo ; done # to monitor the CPU usage Using CLI Download the windows installer from AWS Command Line Interface and installe it Now we have the aws command in our prompt Configure IAM User with Programmatic access Download the access key and the secret access key: aws configure aws configure --profile profile_name # when we want to work with cli profiles # once type the access key, the secret access key, the default region and the output format (json/text) aws ec2 describe-regions # use :q to exit from command output aws <command> --profile <profile_name> # when we want to execute commands with a specific profile set AWS_PROFILE = <profile_name> # set/unset default profile echo $AWS_PROFILE aws sts get-caller-identity # print out account and user info cat ~/.aws/config # file containing profile configuration cat ~/.aws/credentials # file containing profile credentials rm -rf ~/.aws We must use roles for security reasons instead of saving credentials (anyone could access to the .aws directory). Roles are global. They are not specified any region. Create a role to allows EC2 to use S3 as an admin: Go to IAM/Roles and crete a new role Select EC2 as the type of trusted entity Attach AmazonS3FullAccess permissions policies Go to EC2 , select the instance and actions/instance settings/attach/replace iam role Then we can delete .aws directory with credential and still running aws s3 ls aws s3 ls How to create a static website on S3 \u00b6 aws configure # enter user\u00b4s accesskeys and us-east-1 as default region aws s3 mb s3://rnietoe2 # make bucket command aws s3 ls # list all s3 instances echo \"hello world\" > hello_world.txt aws s3 cp hello_world.txt s3://rnietoe2 #upload: ./hello_world.txt to s3://rnietoe2/hello_world.txt cd ~ # go to home directory cd .aws # go the hidden directory nano credentials # display access keys More details to create Bootstrap actions to install additional software are here How to build a web server \u00b6 sudo su yum update -y # check for updates yum install httpd -y # install apache cd /var/www/html # create index.html in this path nano index.html <html><body><h1>This is server 1 </h1></body></html> service httpd start # sudo systemctl start httpd # start apache service chkconfig on # start apache on restarts Elastic Beanstalk \u00b6 AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. Create Application from Elastic Beanstalk Select PHP as platform and the sample application code Crete environment includes: S3 bucket LB / Target group Security group (virtual firewall) Launch Configuration / Auto Scaling Ec2 instance CloudWatch alarm ... Retrieving instance metadata \u00b6 Connect to the EC2 instance and get instance metadata to get information about an instance: curl http://169.254.169.254/latest/user-data curl http://169.254.169.254/latest/user-data > bootstrap.txt cat bootstrap.txt curl http://169.254.169.254/latest/meta-data curl http://169.254.169.254/latest/meta-data/local-ipv4 curl http://169.254.169.254/latest/meta-data/public-ipv4 Lambda \u00b6 AWS Lambda is the Function-as-a-Service (FaaS) to run your code without provisioning or managing servers. Lambda can be used for Infrastructure as Code You can use JSON or YAML for Lambda templates. The resources section is the only required field in Lambda templates. Pricing: request pricing 1 million request per month free 0,20$ next million requests duration pricing (how long lambda functions are executing for) 4000.000 gb-seconds per month free, up to 3,2 million seconds of compute time 0,00001667 for every GB second used thereafter additional charges when using other AWS services Batch \u00b6 AWS Batch enables you to easily and efficiently run batch computing jobs of any scale on AWS using EC2 and EC2 Spot.","title":"4. Compute"},{"location":"AWS/4-Compute/#4-compute","text":"","title":"4. Compute"},{"location":"AWS/4-Compute/#ec2","text":"AWS EC2 ( E lastic C ompute C loud) is a virtual server in the cloud. It is a web service that provides resizeable compute capacity in the cloud EC2 fleet - multiple EC2 instances. They are manage by AWS System Manager Spot fleet - multiple Spot (and on demand) instances AWS originally used a modified version of the Xen Hypervisor to host EC2. In 2017, AWS began rolling out their own Hypervisor called Nitro","title":"EC2"},{"location":"AWS/4-Compute/#placement-groups","text":"Clustered : Group homogenous EC2 instances within a single AZ for network performance Spread : Individial EC2 instances are placed on distinct rack within one region for hardware errors Partitiononed : Multiple EC2 instances in the same rack","title":"Placement groups"},{"location":"AWS/4-Compute/#pricing-models","text":"On Demand : low cost, paying by hour or second. You have full control over its lifecycle\u2014you decide when to launch, stop, hibernate, start, reboot, or terminate it. Sample: when task run uninterrupted from start to finish Reserved : the most economical option for long-term workloads with predictable usage patterns. Contract terms are 1 to 3 years. It includes different discounts Standard Reserved instances (75% off on demand instances) Convertible Reserved instances (54%) Schedule Reserved instances, based on times Spot : taket advantage of unused EC2 capacity. It can accept interruptions. Used for various stateless, fault-tolerant, or flexible applications such as big data, containerized workloads, CI/CD, web servers, HPC (high-performance computing), and other test & development workloads. Extra charge when you terminate the instance Spot Instances are available at up to a 90% discount compared to On-Demand prices. Dedicated : physical EC2 server. It reduces cost using your SW licenses. Also when multitenant not supported by law Standard Reserved Instances cannot be moved between regions. You can choose if a Reserved Instance applies to either a specific AZ, or an Entire Region, but you cannot change the region EC2 Pricing depends on: Clock hours of server time instance type pricing model (on demand, reserved, spot, dedicated host) number of instances load balancing detailed monitoring auto scaling Elastic IP Addresses Operative Systems and sw packages","title":"Pricing models:"},{"location":"AWS/4-Compute/#ami","text":"( A mazon M achine I mange) are instance image snapshots of different Operative System AMI are based on region, OS, architecture (32 or 64 bits), launch permissions and storage for the root volume (EBS or Instance store - ephemeral storage) EC2 instance with Instance Store can't be stopped Instance Store does not appear in the AWS EC2 Volume list To use hibernation, the root volume must be an encrypted EBS volume. RAM be less than 150gb Use snapshots and AMI to change EC2 volumes (AZ and encryption). Snapshots are incremental You must first deregister the AMI before you can delete the snapshot You can use AWS APIs, CLI or the AWS Console to copy snapshots, share snapshots, and create volumes from snapshots. EBS snapshots use incremental backups and are stored in S3. Volumes exist on EBS. Snapshots and instance store exist on S3. aws ec2 create-snapshot","title":"AMI"},{"location":"AWS/4-Compute/#how-to-create-ec2","text":"From AWS EC2, Launch instance Select AMI, for example: Amazon Linux 2 AMI Choose an Instance Type (t2 micro - free tier) Configure Instance Details (number of instances) Enable CloudWatch detailed monitoring is not free Add Storage. Root and EBS volume types allow encryption and delete on termination (turn off by default on EBS) EC2 instance and volume are in the same AZ Add tags like Name, Department or Employee_Id Configure Security Groups - virtual firewalls to enable traffic (types ssh & http - ports 22 & 80) Security groups support \"allow\" rules only. All inbound traffic is blocked by default and all outbound traffic is allowed (SG are STATEFUL) Linux=SSH port 22. Microsoft Windows= RDP (Remote Desktop Protocol) port 3389. http/https ports 80/443 SG changes are take effect immediately one or more SG can be assigned to EC2 instance. EC2 and SG relationshipt is many to many Security groups operate at the instance level. use Network ACL to block specific IP address instead of SG Launch using a key pair (public and private key) Always design for failure. Have one EC2 instance in each AZ EC2 instance are provisioned in AZ Creating an Elastic IP address and associate it with your instance would be the simplest way to make your instance reachable from the outside world.","title":"How to create EC2:"},{"location":"AWS/4-Compute/#how-to-connect-to-ec2","text":"Using AWS Console, select the EC2 instance and clic on the Connect button Using putty: Download putty and load private key file created at EC2/Key Pairs (rnietoe.ppk) using putty gen. Key pair can created or imported in AWS. You can create you key pair with the following command, keep the private key and import the public key in AWS: ssh-keygen -C rnietoe@gmail.com -f ~/.ssh/rnietoe Configure SSH Auth with private key Copy IP address to the session host name field Open connection, login as ec2-user and type sudo su command Using gitbash and install a web server: cd \"C:\\Users\\rniet\\OneDrive\\AWS\" CHMOD 400 rnietoe.pem # change permissions to lock my key down ssh ec2-user@3.80.39.184 -i rnietoe.pem sudo su uname -a # software details cat ~/.ssh/authorized_keys # the public key while true ; do echo ; done # to monitor the CPU usage Using CLI Download the windows installer from AWS Command Line Interface and installe it Now we have the aws command in our prompt Configure IAM User with Programmatic access Download the access key and the secret access key: aws configure aws configure --profile profile_name # when we want to work with cli profiles # once type the access key, the secret access key, the default region and the output format (json/text) aws ec2 describe-regions # use :q to exit from command output aws <command> --profile <profile_name> # when we want to execute commands with a specific profile set AWS_PROFILE = <profile_name> # set/unset default profile echo $AWS_PROFILE aws sts get-caller-identity # print out account and user info cat ~/.aws/config # file containing profile configuration cat ~/.aws/credentials # file containing profile credentials rm -rf ~/.aws We must use roles for security reasons instead of saving credentials (anyone could access to the .aws directory). Roles are global. They are not specified any region. Create a role to allows EC2 to use S3 as an admin: Go to IAM/Roles and crete a new role Select EC2 as the type of trusted entity Attach AmazonS3FullAccess permissions policies Go to EC2 , select the instance and actions/instance settings/attach/replace iam role Then we can delete .aws directory with credential and still running aws s3 ls aws s3 ls","title":"How to connect to EC2"},{"location":"AWS/4-Compute/#how-to-create-a-static-website-on-s3","text":"aws configure # enter user\u00b4s accesskeys and us-east-1 as default region aws s3 mb s3://rnietoe2 # make bucket command aws s3 ls # list all s3 instances echo \"hello world\" > hello_world.txt aws s3 cp hello_world.txt s3://rnietoe2 #upload: ./hello_world.txt to s3://rnietoe2/hello_world.txt cd ~ # go to home directory cd .aws # go the hidden directory nano credentials # display access keys More details to create Bootstrap actions to install additional software are here","title":"How to create a static website on S3"},{"location":"AWS/4-Compute/#how-to-build-a-web-server","text":"sudo su yum update -y # check for updates yum install httpd -y # install apache cd /var/www/html # create index.html in this path nano index.html <html><body><h1>This is server 1 </h1></body></html> service httpd start # sudo systemctl start httpd # start apache service chkconfig on # start apache on restarts","title":"How to build a web server"},{"location":"AWS/4-Compute/#elastic-beanstalk","text":"AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS. Create Application from Elastic Beanstalk Select PHP as platform and the sample application code Crete environment includes: S3 bucket LB / Target group Security group (virtual firewall) Launch Configuration / Auto Scaling Ec2 instance CloudWatch alarm ...","title":"Elastic Beanstalk"},{"location":"AWS/4-Compute/#retrieving-instance-metadata","text":"Connect to the EC2 instance and get instance metadata to get information about an instance: curl http://169.254.169.254/latest/user-data curl http://169.254.169.254/latest/user-data > bootstrap.txt cat bootstrap.txt curl http://169.254.169.254/latest/meta-data curl http://169.254.169.254/latest/meta-data/local-ipv4 curl http://169.254.169.254/latest/meta-data/public-ipv4","title":"Retrieving instance metadata"},{"location":"AWS/4-Compute/#lambda","text":"AWS Lambda is the Function-as-a-Service (FaaS) to run your code without provisioning or managing servers. Lambda can be used for Infrastructure as Code You can use JSON or YAML for Lambda templates. The resources section is the only required field in Lambda templates. Pricing: request pricing 1 million request per month free 0,20$ next million requests duration pricing (how long lambda functions are executing for) 4000.000 gb-seconds per month free, up to 3,2 million seconds of compute time 0,00001667 for every GB second used thereafter additional charges when using other AWS services","title":"Lambda"},{"location":"AWS/4-Compute/#batch","text":"AWS Batch enables you to easily and efficiently run batch computing jobs of any scale on AWS using EC2 and EC2 Spot.","title":"Batch"},{"location":"AWS/5-Security/","text":"5. Security \u00b6 IAM \u00b6 AWS IAM ( I dentity and A ccess M anagement) enables you to manage access to AWS services and resources securely. Users, groups, roles and policies are managed globally and not for a specific a region. You may have a 3rd party device that uses BioMetrics to initiate and exchange of the password or secret key with AWS, but that is not an AWS IAM service You can use permissions to allow and deny users and groups access to AWS resources. Managed policies. Attach readonly policies already defined in AWS Inline policies: Select a policy template, generate a policy, or create a custom policy. Not explicitly allowed = implicitly denied explicit deny > everything else Groups are a collection of users with specific permissions/policies Roles are a secure way to grant permissions to entities that you trust. Create role from IAM. Select EC2 as trusted entity to call AWS services on your behalf. Attach permission policy AmazonS3FullAccess Named as S3_Admin_Access aws iam create-role --role-name DEV_ROLE --assume-role-policy-document file://mypolicy.json To attach an IAM role to an instance that has no role, the instance can be in the stopped or running state. To replace the IAM role on an instance that already has an attached IAM role, the instance must be in the running state. Identities include users, groups, and roles. These are the IAM resource objects that are used to identify and group. You can attach a policy to an IAM identity. A Principal is a person or application that uses the AWS account root user, an IAM user, or an IAM role to sign in and make requests to AWS. New users are assigned Access Key Id and secret when first created to access AWS via the APIs and CLI. Power User Access allows access to all AWS services except the management of groups and users within IAM. New users have NO permissions when first created. When you manage IAM policies, follow the standard security advice of granting the least privilege , or granting only the permissions required to perform a task. Determine what users (and roles) need to do, and then craft policies that allow them to perform only those tasks. A Policy is the document used to grant permissions to users, groups, and roles, but it can not be attached directly to an application. Policies are written using JSON. AdministratorAccess Policy json: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } IAM policies applies to users and groups. IAM roles by another AWS account provide the aws link to other account. Switch role for cross account with Console access Changes to IAM Policies take effect almost immediately (with maybe a few seconds delay). With the IAM Policy Simulator , you can test and troubleshoot identity-based policies, IAM permissions boundaries (delegate permissions to other user), Organizations service control policies, and resource-based policies. Permissions boundary define the maximum permissions an identity can have ARN ( A mazon R esource N ame) arn : partition : service : region : account_id : resouce_type : qualifier : resource : qualifier partition: aws|aws-cn service: s3|ec2|rds region: us-east-1|eu-central-1 (omitted when service is global such as iam) account_id: twelve digits (ommitted with service is globally unique, such as s3) resource_type: resource: qualifier: Security in the cloud \u00b6 AWS Artifact : Compliance and security reports in the AWS Cloud A PCI DSS Level 1 certification attests to the security of the AWS platform regarding credit card transactions . A HIPAA certification attests to the fact that the AWS Platform has met the standard required for the secure storage of medical records in the US \"AWS is responsible for the security OF the cloud. The customer is responsible for security IN the cloud \". Customer is responsible of patching EC2 instances. If you can do in the aws console, you are the responsible Encryption is a shared responsability Shared responsability model talks about who is responsible for waht in cloud The customer would be responsible for patching the Operating System for IaaS solutions Shield protect a lot of traffic (DDOS attacks). Only AWS Shield Advanced offers automated application layer monitoring. This costs $3000/month Inspector to anayze and report security issues on EC2, but it can not examine individual policies Trusted Advisor for recomendations and advices (not only EC2 instances). It helps you optimize cost, fault-tolerance, and more. CloudTrail track user activity and API usage CloudWatch monitoring performance Config monitor configuration settings Athena serverless service for querying data in S3 using SQL. Commonly used to analyse logs. It will work with a number of data formats including JSON, Apache Parquet, Apache ORC amongst others, but XML is not a format that is supported. Macie uses Machine learning to protect sensitive data ( P ersonally I dentifiable I nformation) stored in S3 Kinesis work with Real-Time Streaming Data Personal Health Dashboard helps you to inspect account alerts and find remediation guidance for your account Service Health Dashboard displays the general status of AWS services It's safer to use IAM roles than it is to use Access Keys. WAF \u00b6 AWS WAF ( W eb A pplication F irewall) block request from specific IP address to stop hackers requests. It operates down to Layer 7. Directory Service \u00b6 There are three options that help you migrate Active Directory -dependent applications to the AWS Cloud: Managed Microsoft AD - Directory Service for Microsoft Active Diretory AD Connector Simple AD These solutions also enable users to sign into AWS applications such as Amazon WorkSpaces and Amazon QuickSight with their AD credentials. Developers who don\u2019t need AD can use Amazon Cloud Directory to create cloud-scale directories that organize and manage hierarchical information such as organizational charts, course catalogs, and device registries. Amazon Cognito user pools offer mobile and web application developers Internet-scale user directories with integrated sign-up and sign-in. RAM \u00b6 R esource A ccess M anagement shares AWS resources with other AWS accounts. Create a resource share Choose the resources type to add to the resource share Select the resource id Add principals to the resource share. Principals can be AWS accounts, organizational units, or your organization. Add tags to the resource share. from the other account, browse to Resource Access Manager : Shared with me : Resource shares and accept the resource share SSO \u00b6 AWS S ingle S ign- O n is a cloud service that makes it easy to manage SSO access to multiple AWS accounts and business applications. this require Active Directory and SAML ( S ecurity A ssertion M arkup L anguage) integration","title":"5. Security"},{"location":"AWS/5-Security/#5-security","text":"","title":"5. Security"},{"location":"AWS/5-Security/#iam","text":"AWS IAM ( I dentity and A ccess M anagement) enables you to manage access to AWS services and resources securely. Users, groups, roles and policies are managed globally and not for a specific a region. You may have a 3rd party device that uses BioMetrics to initiate and exchange of the password or secret key with AWS, but that is not an AWS IAM service You can use permissions to allow and deny users and groups access to AWS resources. Managed policies. Attach readonly policies already defined in AWS Inline policies: Select a policy template, generate a policy, or create a custom policy. Not explicitly allowed = implicitly denied explicit deny > everything else Groups are a collection of users with specific permissions/policies Roles are a secure way to grant permissions to entities that you trust. Create role from IAM. Select EC2 as trusted entity to call AWS services on your behalf. Attach permission policy AmazonS3FullAccess Named as S3_Admin_Access aws iam create-role --role-name DEV_ROLE --assume-role-policy-document file://mypolicy.json To attach an IAM role to an instance that has no role, the instance can be in the stopped or running state. To replace the IAM role on an instance that already has an attached IAM role, the instance must be in the running state. Identities include users, groups, and roles. These are the IAM resource objects that are used to identify and group. You can attach a policy to an IAM identity. A Principal is a person or application that uses the AWS account root user, an IAM user, or an IAM role to sign in and make requests to AWS. New users are assigned Access Key Id and secret when first created to access AWS via the APIs and CLI. Power User Access allows access to all AWS services except the management of groups and users within IAM. New users have NO permissions when first created. When you manage IAM policies, follow the standard security advice of granting the least privilege , or granting only the permissions required to perform a task. Determine what users (and roles) need to do, and then craft policies that allow them to perform only those tasks. A Policy is the document used to grant permissions to users, groups, and roles, but it can not be attached directly to an application. Policies are written using JSON. AdministratorAccess Policy json: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } IAM policies applies to users and groups. IAM roles by another AWS account provide the aws link to other account. Switch role for cross account with Console access Changes to IAM Policies take effect almost immediately (with maybe a few seconds delay). With the IAM Policy Simulator , you can test and troubleshoot identity-based policies, IAM permissions boundaries (delegate permissions to other user), Organizations service control policies, and resource-based policies. Permissions boundary define the maximum permissions an identity can have ARN ( A mazon R esource N ame) arn : partition : service : region : account_id : resouce_type : qualifier : resource : qualifier partition: aws|aws-cn service: s3|ec2|rds region: us-east-1|eu-central-1 (omitted when service is global such as iam) account_id: twelve digits (ommitted with service is globally unique, such as s3) resource_type: resource: qualifier:","title":"IAM"},{"location":"AWS/5-Security/#security-in-the-cloud","text":"AWS Artifact : Compliance and security reports in the AWS Cloud A PCI DSS Level 1 certification attests to the security of the AWS platform regarding credit card transactions . A HIPAA certification attests to the fact that the AWS Platform has met the standard required for the secure storage of medical records in the US \"AWS is responsible for the security OF the cloud. The customer is responsible for security IN the cloud \". Customer is responsible of patching EC2 instances. If you can do in the aws console, you are the responsible Encryption is a shared responsability Shared responsability model talks about who is responsible for waht in cloud The customer would be responsible for patching the Operating System for IaaS solutions Shield protect a lot of traffic (DDOS attacks). Only AWS Shield Advanced offers automated application layer monitoring. This costs $3000/month Inspector to anayze and report security issues on EC2, but it can not examine individual policies Trusted Advisor for recomendations and advices (not only EC2 instances). It helps you optimize cost, fault-tolerance, and more. CloudTrail track user activity and API usage CloudWatch monitoring performance Config monitor configuration settings Athena serverless service for querying data in S3 using SQL. Commonly used to analyse logs. It will work with a number of data formats including JSON, Apache Parquet, Apache ORC amongst others, but XML is not a format that is supported. Macie uses Machine learning to protect sensitive data ( P ersonally I dentifiable I nformation) stored in S3 Kinesis work with Real-Time Streaming Data Personal Health Dashboard helps you to inspect account alerts and find remediation guidance for your account Service Health Dashboard displays the general status of AWS services It's safer to use IAM roles than it is to use Access Keys.","title":"Security in the cloud"},{"location":"AWS/5-Security/#waf","text":"AWS WAF ( W eb A pplication F irewall) block request from specific IP address to stop hackers requests. It operates down to Layer 7.","title":"WAF"},{"location":"AWS/5-Security/#directory-service","text":"There are three options that help you migrate Active Directory -dependent applications to the AWS Cloud: Managed Microsoft AD - Directory Service for Microsoft Active Diretory AD Connector Simple AD These solutions also enable users to sign into AWS applications such as Amazon WorkSpaces and Amazon QuickSight with their AD credentials. Developers who don\u2019t need AD can use Amazon Cloud Directory to create cloud-scale directories that organize and manage hierarchical information such as organizational charts, course catalogs, and device registries. Amazon Cognito user pools offer mobile and web application developers Internet-scale user directories with integrated sign-up and sign-in.","title":"Directory Service"},{"location":"AWS/5-Security/#ram","text":"R esource A ccess M anagement shares AWS resources with other AWS accounts. Create a resource share Choose the resources type to add to the resource share Select the resource id Add principals to the resource share. Principals can be AWS accounts, organizational units, or your organization. Add tags to the resource share. from the other account, browse to Resource Access Manager : Shared with me : Resource shares and accept the resource share","title":"RAM"},{"location":"AWS/5-Security/#sso","text":"AWS S ingle S ign- O n is a cloud service that makes it easy to manage SSO access to multiple AWS accounts and business applications. this require Active Directory and SAML ( S ecurity A ssertion M arkup L anguage) integration","title":"SSO"},{"location":"AWS/6-Management/","text":"6. Management and Governance \u00b6 CloudFormation \u00b6 AWS CloudFormation is IaaS tool to create , update and delete resources with templates (json or yaml). While Elastic Beanstalk we can create and manage resources based on code... Create stack using the sample template Wordpress blog, where stack is a set of related resources. enter rnietoe for every name and Abodroc83 for every password. Select instance type t2.micro and our key value pair rnietoe Create stack . This will create and configure an EC2 instance based on the wordpress template from Output tab, click on the value link. from Template tab, clik on View in Designer CloudFormation service is free, but the resources that it provisions have a cost CloudFormation has a wide set of supported resources, but does not support the creation of all AWS resources. AWS Quick Start is a way of deploying environments quickly using CloudFormation templates built by experts AWS Solucions Architects. Template anatomy : AWSTemplateFormatVersion : \"optional version date. sample: 2010-09-09\" Description : optional string Metadata : # optional template metadata Parameters : # optional set of parameters Mappings : # optional set of mappings Conditions : # set of conditions Transform : # set of transforms Resources : # required set of resources Outputs : # optional set of outputs When updating a stack, a change set (such as tags) is created with the summary of proposed changes. This change set is executed to update the stack. Only the change set is executed instead of all resources defined in the template. At the other hand, when adding a new resource (such as SecurityGroup) for our previous deployed resource (EC2), a new EC2 instance in created with the SecurityGroup, and the old EC2 instance is deleted automatically Resources \u00b6 Resources UserData property: perform actions on system startup only runs on the first boot cycle execution time impact on the startup time Base64 encoded on Windows: Run as local admin Batch commands and/or powersheel support Executed by EC2Config or EC2Launch #script echo Current date and time >> %SystemRoot% \\T em \\t est.log echo %DATE% %TIME% >> %SystemRoot% \\T em \\t est.log #powershell $file = $env :SystemRoot + \"\\Temp\\\" + (Get=Date).ToString(\" MM-dd-yy-hh-mm \") New-Item $file -ItemType file on Linux: Run as root (no need for sudo) Not run interactively (no user feedback) Logs output to /var/log/cloud-init-output.log #!/bin/bash # interpreter yum update -y yum install -y httpd service httpd start Sample template: Resources : EC2Instance : Type : AWS :: EC2 :: Instance Properties : UserData : ! Base64 : | # !/ bin / bash - xe yum update - y yum install httpd - y service httpd start Mappings \u00b6 Mappings Mappings : RegionMap : # MapName us-east-1 : # TopLevelKey 1 AMI : ami-1853ac65 # SecondLevelKey 1 us-west-1 : # TopLevelKey 2 AMI : ami-bf5540df # SecondLevelKey 2 Resources : MyEc2Instance : Type : AWS::EC2::Instance Properties : InstanceType : t2.micro ImageId : !FindInMap : - RegionMap # MapName - !Ref AWS::Region # TopLevelKey - AMI # SecondLevelKey Input Parameters \u00b6 Parameters enable you to input custom values to your template each time you create or update a stack. Supported parameter types: String Number List CommaDelimitedList AWS-specific types(AWS::ec2::Image::Id) Parameters : InstanceTypeParameter : # Parameter ID Type : String # Parameter Type Default : t2.micro # Parameter Property 1 AllowedValues : # Parameter Property 2 - t2.micro - m1.small - m1.large Description : # ParameterProperty 3 EC2 Instance Type Resources : Ec2Instance : Type : AWS::EC2::Instance Properties : InstanceType : !Ref : InstanceTypeParameter # Parameter Metadata \u00b6 Metadata Conditions \u00b6 Conditions Transform \u00b6 Transform Outputs \u00b6 Outputs information about resources, within a cloudformation stack. For example: the public IP or DNS of a EC2 instance the S3 bucket name for a stack Outputs : ServerDns : # OutputID Value : ! GetAtt # Value to return - Ec2Instance - PublicDnsName Export : Name : # Value to export Intrinsic functions \u00b6 Intrinsic functions Join : appends a set of values into a single value { \"Fn::Join\" : [ \":\" , [ \"a\" , \"b\" , \"c\" ] ] } !Join [ \":\", [ a, b, c ] ] Ref : : returns the value of the specified resource. { \"Ref\" : \"resourceId\" } !Ref resourceId FindInMap : returns the value corresponding to keys in a two-level map that is declared in the Mappings section. { \"Fn::FindInMap\" : [ \"MapName\" , \"TopLevelKey\" , \"SecondLevelKey\" ] } !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] GetAtt : returns the value of an attribute from a resource in the template { \"Fn::GetAtt\" : [ \"logicalNameOfResource\" , \"attributeName\" ] } !GetAtt logicalNameOfResource.attributeName Sub : substitutes variables in an input string with values that you specify when you create or update a stack, such as ${AWS::StackName} or ${AWS::Region} { \"Fn::Sub\" : String } !Sub String GetAZs : returns an array that lists AZs for a specified region in alaphabetical order. Specifying an empty string is equivalent to specifying AWS::Region. { \"Fn::GetAZs\" : \"region\" } ``` ```yml !GetAZs region Pseudo parameters \u00b6 Pseudo parameters AWS::AccountId : Returns the AWS account ID of the account in which the stack is being created AWS::NotificationARNs : Returns the list of notification ARNs ( A mazon R esource N ames) for the current stack. AWS::StackId : Returns the ID of the stack AWS::StackName : Returns the Name of the stack AWS::Region : Returns the AWS Region in which the resource is being created CloudFormation helper scripts \u00b6 Pyhon based helper scripts preinstalled on Amazon Linux (to avoid scripting): cfn-init: : read Metadata to execute AWS::CloudFormation::Init cfn-signal : signal with a CreationPolicy or WaitCondition when the resource or application is ready. cfn-get-metadata : retrieve metadata based on a specific key. cfn-hup : check for updates to metadata and execute custom hooks when changes are detected. single config key sample: AWS::CloudFormation::Init: config: packages: download and install pre-packaged applications and components groups: create Linux/Unix groups and assign group IDs users: create Linux/UNIX users on the EC2 instance sources: download an archive file and unpack it in the target directory on EC2 files: create files on the EC2 instance services: define enabled and siabled services when the instance is launch commands: execute commands on the EC2 instance configSets sample to install a web server: installweb : packages : yum : httpd : [] services : sysvinit : httpd : enabled : true ensureRunning : true installphp : packages : yum : php : [] Resources : EC2Instance : Metadata : AWS :: CloudFormation :: Init : configSets : webphp : - \"installphp\" - \"installweb\" How to setting up a full stack: From CloudFormation , create stack uploading the following template file: Parameters : myKeyPair : Description : Amazon EC2 Key Pair Type : AWS :: EC2 :: KeyPair :: KeyName VpcId : Description : Enter the VpcId Type : AWS :: EC2 :: VPC :: Id SubnetIds : Description : Enter the Subnets Type : List < AWS :: EC2 :: Subnet :: Id > Mappings : RegionMap : us - east - 1 : AMI : ami - 1853 ac65 us - west - 1 : AMI : ami - bf5540df eu - west - 1 : AMI : ami - 3 bfab942 ap - southeast - 1 : AMI : ami - e2adf99e ap - southeast - 2 : AMI : ami - 43874721 Resources : LoadBalancer : # Application Load Balancer Type : AWS :: ElasticLoadBalancingV2 :: LoadBalancer Properties : SecurityGroups : - ! Ref ALBSecurityGroup Subnets : ! Ref SubnetIds LoadBalancerListener : # Port 80 Listener for ALB Type : AWS :: ElasticLoadBalancingV2 :: Listener Properties : LoadBalancerArn : ! Ref LoadBalancer Port : 80 Protocol : HTTP DefaultActions : - Type : forward TargetGroupArn : Ref : TargetGroup TargetGroup : Type : AWS :: ElasticLoadBalancingV2 :: TargetGroup Properties : Port : 80 Protocol : HTTP VpcId : ! Ref VpcId AutoScalingGroup : Type : AWS :: AutoScaling :: AutoScalingGroup Properties : AvailabilityZones : ! GetAZs LaunchConfigurationName : ! Ref LaunchConfiguration MinSize : 1 MaxSize : 3 TargetGroupARNs : - ! Ref TargetGroup LaunchConfiguration : Type : AWS :: AutoScaling :: LaunchConfiguration Metadata : Comment : Install php and httpd AWS :: CloudFormation :: Init : config : packages : yum : httpd : [] php : [] files : /var/www/html/i ndex . php : content : ! Sub | <? php print \"Hello world Abs was here!\" ; ?> services : sysvinit : httpd : enabled : true ensureRunning : true Properties : KeyName : ! Ref myKeyPair InstanceType : t2 . micro SecurityGroups : - ! Ref EC2SecurityGroup ImageId : Fn :: FindInMap : - RegionMap - ! Ref AWS :: Region - AMI UserData : 'Fn::Base64' : ! Sub | # !/ bin / bash - xe # Ensure AWS CFN Bootstrap is the latest yum install - y aws - cfn - bootstrap # Install the files and packages from the metadata /opt/aws/bin/ cfn - init - v -- stack $ { AWS :: StackName } -- resource LaunchConfiguration -- region $ { AWS :: Region } ALBSecurityGroup : Type : AWS :: EC2 :: SecurityGroup Properties : GroupDescription : ALB Security Group VpcId : ! Ref VpcId SecurityGroupIngress : - IpProtocol : tcp FromPort : 80 ToPort : 80 CidrIp : 0.0 . 0.0 / 0 EC2SecurityGroup : Type : AWS :: EC2 :: SecurityGroup Properties : GroupDescription : EC2 Instance EC2InboundRule : # EC2 can only accept traffic from ALB Type : AWS :: EC2 :: SecurityGroupIngress Properties : IpProtocol : tcp FromPort : 80 ToPort : 80 SourceSecurityGroupId : ! GetAtt - ALBSecurityGroup - GroupId GroupId : ! GetAtt - EC2SecurityGroup - GroupId Outputs : PublicDns : Description : The Public DNS Value : ! Sub 'http://${LoadBalancer.DNSName}' Set stack name and parameters and finally create the stack Stak is created with defined resources Check the stack outputs and browse to the EC2 instance uri: ChangeSets \u00b6 Allow to preview how changes will impact to the resources. There are 4 changeSets operations: Create : create a change set. This operation does not modify the stack View : view proposed changes after creating Execute : execute the change set to update the stack Delete : delete change set. This operation does not modify the stack how to create a change set for current stack to update security groups: We are going to remove port 22 from security group in this sample From CloudFormation, select the stack and from stack actions, Create change sets for current stack Upload the template with changes and create See the JSON Changes result: [ { \"resourceChange\" : { \"logicalResourceId\" : \"MySecurityGroup\" , \"action\" : \"Modify\" , // Add | Modify | Remove \"physicalResourceId\" : \"MyFirstStack-MySecurityGroup-1AT0XTT24PZNP\" , \"resourceType\" : \"AWS::EC2::SecurityGroup\" , \"replacement\" : \"False\" , \"moduleInfo\" : null , \"details\" : [ { \"target\" : { \"name\" : \"SecurityGroupIngress\" , \"requiresRecreation\" : \"Never\" , \"attribute\" : \"Properties\" }, \"causingEntity\" : null , \"evaluation\" : \"Static\" , \"changeSource\" : \"DirectModification\" } ], \"changeSetId\" : null , \"scope\" : [ \"Properties\" ] }, \"type\" : \"Resource\" } ] Finally we can delete or execute the change set The actions depends on the resource. we should predict if replacement will be necesary based on these documents: AWS resource and property types reference Update behaviors of stack resources CloudWatch \u00b6 Monitor resources and applications performances : compute ec2 instances - every 5 minutes by default autoscaling groups ELB Route53 health checks storage and content delivery EBS - virtual hard disk Storage gateways CloudFront network CloudTrail \u00b6 Continuously log your AWS account activity monitoring API calls per account and region logs can be stored in a single S3 bucket (belonging to the paying account): turn on CloudTrail in paying account create a bucket policy that allows cross-account access turn on cloud trail in other accounts and use the bucket in the paying account Systems Manager \u00b6 View and manage AWS resources in the cloud (EC2 fleets) or on premise (virtual machines) AWS Organisations \u00b6 AWS Organizations enables you to centrally manage billing, control access, compliance, and security, and share resources across multiple accounts in the AWS Cloud. You can consolidate all your AWS accounts into an organization, and arrange all AWS accounts into distinct organizational units. Provides single payer and centralized cost tracking Lets you create and invite accounts Allows you to apply policy-based controls Helps you simplify organization-wide management of AWS services Or you can create an organization with only consolidated billing features. After you create an organization, you cannot join this account to another organization until you delete its current organization. From AWS Organisations, create organisation Enable AWS Single Sign On to centrally manage access to multiple AWS accounts and business applications. Create an organization trail in AWS CloudTrail to log all events for all AWS accounts in your organization. From Organize Accounts Tab, we create a new organisational units From Policies tab, enable service control policies and create a sample policy to block EC2 usage. Select Amazon EC2 statement and deny effect Apply the new policy to organisational units or to AWS Accounts Maximum of 20 Link accounts. Contact AWS for more Assuming all instances are in the same AWS Organization, the reserved instance pricing for the unused on demand instances will be applied. Landing Zone helps to quickly setup a secure, multi-account AWS environment based on AWS best practices. AWS Config \u00b6 AWS Config provides an inventory of your AWS resources and a history of configuration changes to these resources. You can use AWS Config to define rules that evaluate these configurations for compliance. How to get started: Specify the types of AWS resources you want AWS Config to record Define the Amazon S3 bucket to which it sends files Set the Amazon SNS topic to which it sends notifications Define config rules Restart EC2 instances and check AWS Config results You are charged based on the number of configuration items recorded, the number of active AWS Config rule evaluations and the number of conformance pack evaluations in your account Pricing \u00b6 Capex: Capital Expenditure: you pay up front. It's a fixed cost Opex: Operational Expenditure: you pay for what you use, like electricity, gas or water Budgets predict costs before they are incurred. Alarms can be set to monitor spending on your AWS account from AWS Billing service - Budgets Receive Billing Alerts must be enabled at Billing Preferences . Cost explorer is use to explore costs after they have been incurred. See Billing & Cost Management Dashboard . Creating a billing alarm at ClouldWatch/Alarms/Billing using SNS ( S imple N otification S ervice) topic to monitor estimated AWS charges. Application Integration/SNS is a messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Using AWS SNS topics, your publisher systems can fan-out messages to a large number of subscriber endpoints for parallel processing, including Amazon SQS queues, AWS Lambda functions, and HTTP/S webhooks. Additionally, SNS can be used to fan out notifications to end users using mobile push, SMS, and email. Application Integration/SQS ( S imple Q ueue S ervice) offers a queue that lets you integrate and decouple distributed software systems and components. Support Plans: All accounts receive billing support Pricing policies pay as you go pay less when you reserve pay even less per unit by using more pay even less as AWS grows custom pricing cost drivers: compute storage data outbound (data leavingn, not data entering) free services VPC Elastic Beanstalk Cloud Formation IAM Autoscaling Opsworks Consolidated billing Create a paying account for billing purposes only. Do not deploy resources into the paying account. Consolidated billing allows you to get volume discounts on all your accounts AWS Cost Calculators \u00b6 AWS Simple Monthly Calculator ( DEPRECATED ) AWS Total Cost of Ownership Calculator ( DEPRECATED ): comparing AWS VS on premise AWS Pricing Calculator \u00b6 Developer Tools - X-Ray AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application\u2019s underlying components.","title":"6. Management and Governance"},{"location":"AWS/6-Management/#6-management-and-governance","text":"","title":"6. Management and Governance"},{"location":"AWS/6-Management/#cloudformation","text":"AWS CloudFormation is IaaS tool to create , update and delete resources with templates (json or yaml). While Elastic Beanstalk we can create and manage resources based on code... Create stack using the sample template Wordpress blog, where stack is a set of related resources. enter rnietoe for every name and Abodroc83 for every password. Select instance type t2.micro and our key value pair rnietoe Create stack . This will create and configure an EC2 instance based on the wordpress template from Output tab, click on the value link. from Template tab, clik on View in Designer CloudFormation service is free, but the resources that it provisions have a cost CloudFormation has a wide set of supported resources, but does not support the creation of all AWS resources. AWS Quick Start is a way of deploying environments quickly using CloudFormation templates built by experts AWS Solucions Architects. Template anatomy : AWSTemplateFormatVersion : \"optional version date. sample: 2010-09-09\" Description : optional string Metadata : # optional template metadata Parameters : # optional set of parameters Mappings : # optional set of mappings Conditions : # set of conditions Transform : # set of transforms Resources : # required set of resources Outputs : # optional set of outputs When updating a stack, a change set (such as tags) is created with the summary of proposed changes. This change set is executed to update the stack. Only the change set is executed instead of all resources defined in the template. At the other hand, when adding a new resource (such as SecurityGroup) for our previous deployed resource (EC2), a new EC2 instance in created with the SecurityGroup, and the old EC2 instance is deleted automatically","title":"CloudFormation"},{"location":"AWS/6-Management/#resources","text":"Resources UserData property: perform actions on system startup only runs on the first boot cycle execution time impact on the startup time Base64 encoded on Windows: Run as local admin Batch commands and/or powersheel support Executed by EC2Config or EC2Launch #script echo Current date and time >> %SystemRoot% \\T em \\t est.log echo %DATE% %TIME% >> %SystemRoot% \\T em \\t est.log #powershell $file = $env :SystemRoot + \"\\Temp\\\" + (Get=Date).ToString(\" MM-dd-yy-hh-mm \") New-Item $file -ItemType file on Linux: Run as root (no need for sudo) Not run interactively (no user feedback) Logs output to /var/log/cloud-init-output.log #!/bin/bash # interpreter yum update -y yum install -y httpd service httpd start Sample template: Resources : EC2Instance : Type : AWS :: EC2 :: Instance Properties : UserData : ! Base64 : | # !/ bin / bash - xe yum update - y yum install httpd - y service httpd start","title":"Resources"},{"location":"AWS/6-Management/#mappings","text":"Mappings Mappings : RegionMap : # MapName us-east-1 : # TopLevelKey 1 AMI : ami-1853ac65 # SecondLevelKey 1 us-west-1 : # TopLevelKey 2 AMI : ami-bf5540df # SecondLevelKey 2 Resources : MyEc2Instance : Type : AWS::EC2::Instance Properties : InstanceType : t2.micro ImageId : !FindInMap : - RegionMap # MapName - !Ref AWS::Region # TopLevelKey - AMI # SecondLevelKey","title":"Mappings"},{"location":"AWS/6-Management/#input-parameters","text":"Parameters enable you to input custom values to your template each time you create or update a stack. Supported parameter types: String Number List CommaDelimitedList AWS-specific types(AWS::ec2::Image::Id) Parameters : InstanceTypeParameter : # Parameter ID Type : String # Parameter Type Default : t2.micro # Parameter Property 1 AllowedValues : # Parameter Property 2 - t2.micro - m1.small - m1.large Description : # ParameterProperty 3 EC2 Instance Type Resources : Ec2Instance : Type : AWS::EC2::Instance Properties : InstanceType : !Ref : InstanceTypeParameter # Parameter","title":"Input Parameters"},{"location":"AWS/6-Management/#metadata","text":"Metadata","title":"Metadata"},{"location":"AWS/6-Management/#conditions","text":"Conditions","title":"Conditions"},{"location":"AWS/6-Management/#transform","text":"Transform","title":"Transform"},{"location":"AWS/6-Management/#outputs","text":"Outputs information about resources, within a cloudformation stack. For example: the public IP or DNS of a EC2 instance the S3 bucket name for a stack Outputs : ServerDns : # OutputID Value : ! GetAtt # Value to return - Ec2Instance - PublicDnsName Export : Name : # Value to export","title":"Outputs"},{"location":"AWS/6-Management/#intrinsic-functions","text":"Intrinsic functions Join : appends a set of values into a single value { \"Fn::Join\" : [ \":\" , [ \"a\" , \"b\" , \"c\" ] ] } !Join [ \":\", [ a, b, c ] ] Ref : : returns the value of the specified resource. { \"Ref\" : \"resourceId\" } !Ref resourceId FindInMap : returns the value corresponding to keys in a two-level map that is declared in the Mappings section. { \"Fn::FindInMap\" : [ \"MapName\" , \"TopLevelKey\" , \"SecondLevelKey\" ] } !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] GetAtt : returns the value of an attribute from a resource in the template { \"Fn::GetAtt\" : [ \"logicalNameOfResource\" , \"attributeName\" ] } !GetAtt logicalNameOfResource.attributeName Sub : substitutes variables in an input string with values that you specify when you create or update a stack, such as ${AWS::StackName} or ${AWS::Region} { \"Fn::Sub\" : String } !Sub String GetAZs : returns an array that lists AZs for a specified region in alaphabetical order. Specifying an empty string is equivalent to specifying AWS::Region. { \"Fn::GetAZs\" : \"region\" } ``` ```yml !GetAZs region","title":"Intrinsic functions"},{"location":"AWS/6-Management/#pseudo-parameters","text":"Pseudo parameters AWS::AccountId : Returns the AWS account ID of the account in which the stack is being created AWS::NotificationARNs : Returns the list of notification ARNs ( A mazon R esource N ames) for the current stack. AWS::StackId : Returns the ID of the stack AWS::StackName : Returns the Name of the stack AWS::Region : Returns the AWS Region in which the resource is being created","title":"Pseudo parameters"},{"location":"AWS/6-Management/#cloudformation-helper-scripts","text":"Pyhon based helper scripts preinstalled on Amazon Linux (to avoid scripting): cfn-init: : read Metadata to execute AWS::CloudFormation::Init cfn-signal : signal with a CreationPolicy or WaitCondition when the resource or application is ready. cfn-get-metadata : retrieve metadata based on a specific key. cfn-hup : check for updates to metadata and execute custom hooks when changes are detected. single config key sample: AWS::CloudFormation::Init: config: packages: download and install pre-packaged applications and components groups: create Linux/Unix groups and assign group IDs users: create Linux/UNIX users on the EC2 instance sources: download an archive file and unpack it in the target directory on EC2 files: create files on the EC2 instance services: define enabled and siabled services when the instance is launch commands: execute commands on the EC2 instance configSets sample to install a web server: installweb : packages : yum : httpd : [] services : sysvinit : httpd : enabled : true ensureRunning : true installphp : packages : yum : php : [] Resources : EC2Instance : Metadata : AWS :: CloudFormation :: Init : configSets : webphp : - \"installphp\" - \"installweb\" How to setting up a full stack: From CloudFormation , create stack uploading the following template file: Parameters : myKeyPair : Description : Amazon EC2 Key Pair Type : AWS :: EC2 :: KeyPair :: KeyName VpcId : Description : Enter the VpcId Type : AWS :: EC2 :: VPC :: Id SubnetIds : Description : Enter the Subnets Type : List < AWS :: EC2 :: Subnet :: Id > Mappings : RegionMap : us - east - 1 : AMI : ami - 1853 ac65 us - west - 1 : AMI : ami - bf5540df eu - west - 1 : AMI : ami - 3 bfab942 ap - southeast - 1 : AMI : ami - e2adf99e ap - southeast - 2 : AMI : ami - 43874721 Resources : LoadBalancer : # Application Load Balancer Type : AWS :: ElasticLoadBalancingV2 :: LoadBalancer Properties : SecurityGroups : - ! Ref ALBSecurityGroup Subnets : ! Ref SubnetIds LoadBalancerListener : # Port 80 Listener for ALB Type : AWS :: ElasticLoadBalancingV2 :: Listener Properties : LoadBalancerArn : ! Ref LoadBalancer Port : 80 Protocol : HTTP DefaultActions : - Type : forward TargetGroupArn : Ref : TargetGroup TargetGroup : Type : AWS :: ElasticLoadBalancingV2 :: TargetGroup Properties : Port : 80 Protocol : HTTP VpcId : ! Ref VpcId AutoScalingGroup : Type : AWS :: AutoScaling :: AutoScalingGroup Properties : AvailabilityZones : ! GetAZs LaunchConfigurationName : ! Ref LaunchConfiguration MinSize : 1 MaxSize : 3 TargetGroupARNs : - ! Ref TargetGroup LaunchConfiguration : Type : AWS :: AutoScaling :: LaunchConfiguration Metadata : Comment : Install php and httpd AWS :: CloudFormation :: Init : config : packages : yum : httpd : [] php : [] files : /var/www/html/i ndex . php : content : ! Sub | <? php print \"Hello world Abs was here!\" ; ?> services : sysvinit : httpd : enabled : true ensureRunning : true Properties : KeyName : ! Ref myKeyPair InstanceType : t2 . micro SecurityGroups : - ! Ref EC2SecurityGroup ImageId : Fn :: FindInMap : - RegionMap - ! Ref AWS :: Region - AMI UserData : 'Fn::Base64' : ! Sub | # !/ bin / bash - xe # Ensure AWS CFN Bootstrap is the latest yum install - y aws - cfn - bootstrap # Install the files and packages from the metadata /opt/aws/bin/ cfn - init - v -- stack $ { AWS :: StackName } -- resource LaunchConfiguration -- region $ { AWS :: Region } ALBSecurityGroup : Type : AWS :: EC2 :: SecurityGroup Properties : GroupDescription : ALB Security Group VpcId : ! Ref VpcId SecurityGroupIngress : - IpProtocol : tcp FromPort : 80 ToPort : 80 CidrIp : 0.0 . 0.0 / 0 EC2SecurityGroup : Type : AWS :: EC2 :: SecurityGroup Properties : GroupDescription : EC2 Instance EC2InboundRule : # EC2 can only accept traffic from ALB Type : AWS :: EC2 :: SecurityGroupIngress Properties : IpProtocol : tcp FromPort : 80 ToPort : 80 SourceSecurityGroupId : ! GetAtt - ALBSecurityGroup - GroupId GroupId : ! GetAtt - EC2SecurityGroup - GroupId Outputs : PublicDns : Description : The Public DNS Value : ! Sub 'http://${LoadBalancer.DNSName}' Set stack name and parameters and finally create the stack Stak is created with defined resources Check the stack outputs and browse to the EC2 instance uri:","title":"CloudFormation helper scripts"},{"location":"AWS/6-Management/#changesets","text":"Allow to preview how changes will impact to the resources. There are 4 changeSets operations: Create : create a change set. This operation does not modify the stack View : view proposed changes after creating Execute : execute the change set to update the stack Delete : delete change set. This operation does not modify the stack how to create a change set for current stack to update security groups: We are going to remove port 22 from security group in this sample From CloudFormation, select the stack and from stack actions, Create change sets for current stack Upload the template with changes and create See the JSON Changes result: [ { \"resourceChange\" : { \"logicalResourceId\" : \"MySecurityGroup\" , \"action\" : \"Modify\" , // Add | Modify | Remove \"physicalResourceId\" : \"MyFirstStack-MySecurityGroup-1AT0XTT24PZNP\" , \"resourceType\" : \"AWS::EC2::SecurityGroup\" , \"replacement\" : \"False\" , \"moduleInfo\" : null , \"details\" : [ { \"target\" : { \"name\" : \"SecurityGroupIngress\" , \"requiresRecreation\" : \"Never\" , \"attribute\" : \"Properties\" }, \"causingEntity\" : null , \"evaluation\" : \"Static\" , \"changeSource\" : \"DirectModification\" } ], \"changeSetId\" : null , \"scope\" : [ \"Properties\" ] }, \"type\" : \"Resource\" } ] Finally we can delete or execute the change set The actions depends on the resource. we should predict if replacement will be necesary based on these documents: AWS resource and property types reference Update behaviors of stack resources","title":"ChangeSets"},{"location":"AWS/6-Management/#cloudwatch","text":"Monitor resources and applications performances : compute ec2 instances - every 5 minutes by default autoscaling groups ELB Route53 health checks storage and content delivery EBS - virtual hard disk Storage gateways CloudFront network","title":"CloudWatch"},{"location":"AWS/6-Management/#cloudtrail","text":"Continuously log your AWS account activity monitoring API calls per account and region logs can be stored in a single S3 bucket (belonging to the paying account): turn on CloudTrail in paying account create a bucket policy that allows cross-account access turn on cloud trail in other accounts and use the bucket in the paying account","title":"CloudTrail"},{"location":"AWS/6-Management/#systems-manager","text":"View and manage AWS resources in the cloud (EC2 fleets) or on premise (virtual machines)","title":"Systems Manager"},{"location":"AWS/6-Management/#aws-organisations","text":"AWS Organizations enables you to centrally manage billing, control access, compliance, and security, and share resources across multiple accounts in the AWS Cloud. You can consolidate all your AWS accounts into an organization, and arrange all AWS accounts into distinct organizational units. Provides single payer and centralized cost tracking Lets you create and invite accounts Allows you to apply policy-based controls Helps you simplify organization-wide management of AWS services Or you can create an organization with only consolidated billing features. After you create an organization, you cannot join this account to another organization until you delete its current organization. From AWS Organisations, create organisation Enable AWS Single Sign On to centrally manage access to multiple AWS accounts and business applications. Create an organization trail in AWS CloudTrail to log all events for all AWS accounts in your organization. From Organize Accounts Tab, we create a new organisational units From Policies tab, enable service control policies and create a sample policy to block EC2 usage. Select Amazon EC2 statement and deny effect Apply the new policy to organisational units or to AWS Accounts Maximum of 20 Link accounts. Contact AWS for more Assuming all instances are in the same AWS Organization, the reserved instance pricing for the unused on demand instances will be applied. Landing Zone helps to quickly setup a secure, multi-account AWS environment based on AWS best practices.","title":"AWS Organisations"},{"location":"AWS/6-Management/#aws-config","text":"AWS Config provides an inventory of your AWS resources and a history of configuration changes to these resources. You can use AWS Config to define rules that evaluate these configurations for compliance. How to get started: Specify the types of AWS resources you want AWS Config to record Define the Amazon S3 bucket to which it sends files Set the Amazon SNS topic to which it sends notifications Define config rules Restart EC2 instances and check AWS Config results You are charged based on the number of configuration items recorded, the number of active AWS Config rule evaluations and the number of conformance pack evaluations in your account","title":"AWS Config"},{"location":"AWS/6-Management/#pricing","text":"Capex: Capital Expenditure: you pay up front. It's a fixed cost Opex: Operational Expenditure: you pay for what you use, like electricity, gas or water Budgets predict costs before they are incurred. Alarms can be set to monitor spending on your AWS account from AWS Billing service - Budgets Receive Billing Alerts must be enabled at Billing Preferences . Cost explorer is use to explore costs after they have been incurred. See Billing & Cost Management Dashboard . Creating a billing alarm at ClouldWatch/Alarms/Billing using SNS ( S imple N otification S ervice) topic to monitor estimated AWS charges. Application Integration/SNS is a messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Using AWS SNS topics, your publisher systems can fan-out messages to a large number of subscriber endpoints for parallel processing, including Amazon SQS queues, AWS Lambda functions, and HTTP/S webhooks. Additionally, SNS can be used to fan out notifications to end users using mobile push, SMS, and email. Application Integration/SQS ( S imple Q ueue S ervice) offers a queue that lets you integrate and decouple distributed software systems and components. Support Plans: All accounts receive billing support Pricing policies pay as you go pay less when you reserve pay even less per unit by using more pay even less as AWS grows custom pricing cost drivers: compute storage data outbound (data leavingn, not data entering) free services VPC Elastic Beanstalk Cloud Formation IAM Autoscaling Opsworks Consolidated billing Create a paying account for billing purposes only. Do not deploy resources into the paying account. Consolidated billing allows you to get volume discounts on all your accounts","title":"Pricing"},{"location":"AWS/6-Management/#aws-cost-calculators","text":"AWS Simple Monthly Calculator ( DEPRECATED ) AWS Total Cost of Ownership Calculator ( DEPRECATED ): comparing AWS VS on premise AWS Pricing Calculator","title":"AWS Cost Calculators"},{"location":"AWS/6-Management/#_1","text":"Developer Tools - X-Ray AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application\u2019s underlying components.","title":""},{"location":"AWS/7-Architecture/","text":"HA Architecture \u00b6 Load Balancers \u00b6 ALB ( A pplication L oad B alancers) for intelligent routing - HTTP/HTTPS NLB ( Ne twork L oad B alancers) for extreme performance and static IPs - TCP/TLS CLB ( C lassic L oad B alancers) for test and dev - low cost - HTTP/HTTPS/TCP Error 504 means gateway timeout X-Forwarded-For header is used to get the IPv4 address of end users How to use classic load balancer Create load balancer from EC2 : Load Balancers of type CLB Register EC2 instances Select our Security Group (virtual firewall) Browse to the ELB ( E lastic L oad B alance) DNS name and see the result, instead of browsing to the EC2 IP address How to use application load balancer Create target group to group instances, IP addresses or Lamda functions for load balancing Create load balancer of type ALB Configure Load Balancer with name and select every AZ At least two public subnets are required to enable the LB Configure Routing with a Target Troup name and the following health check settings: healthy threshold: 3 times unhealthy threshold: 3 times timeout: 3 seconds interval: 5 seconds success code: 200 Register target adding our EC2 instances (to registered) Review and create sticky sessions bind a user's session to a specific EC2 instance. All user requests during the session are sent to the same intance (CLB) or target group (ALB)","title":"HA Architecture"},{"location":"AWS/7-Architecture/#ha-architecture","text":"","title":"HA Architecture"},{"location":"AWS/7-Architecture/#load-balancers","text":"ALB ( A pplication L oad B alancers) for intelligent routing - HTTP/HTTPS NLB ( Ne twork L oad B alancers) for extreme performance and static IPs - TCP/TLS CLB ( C lassic L oad B alancers) for test and dev - low cost - HTTP/HTTPS/TCP Error 504 means gateway timeout X-Forwarded-For header is used to get the IPv4 address of end users How to use classic load balancer Create load balancer from EC2 : Load Balancers of type CLB Register EC2 instances Select our Security Group (virtual firewall) Browse to the ELB ( E lastic L oad B alance) DNS name and see the result, instead of browsing to the EC2 IP address How to use application load balancer Create target group to group instances, IP addresses or Lamda functions for load balancing Create load balancer of type ALB Configure Load Balancer with name and select every AZ At least two public subnets are required to enable the LB Configure Routing with a Target Troup name and the following health check settings: healthy threshold: 3 times unhealthy threshold: 3 times timeout: 3 seconds interval: 5 seconds success code: 200 Register target adding our EC2 instances (to registered) Review and create sticky sessions bind a user's session to a specific EC2 instance. All user requests during the session are sent to the same intance (CLB) or target group (ALB)","title":"Load Balancers"},{"location":"AWS/8-Databases/","text":"8. Databases \u00b6 RDS \u00b6 AWS RDS ( R elational D atabase S ervice): MS SQL Server (up to 16TB of storage when using the Provisioned IOPS and General Purpose - SSD - storage) Oracle. Includes license model MySQL (port 3306) Amazon Aurora. This is up to 5X faster than a traditional MySQL database. MySQL and PostgreSQL compatibility. No free tier MariaDB PostgreSQL (port 5432) Some features: Multi-AZ for disaster recovery and high availability . By default in production RDS. Primary host replicates to a secondary host when failover. Enable multiAZ impact to the RDS instance if running host -t NS database_endpoint # Query DNS Records on Linux nslookup database_endpoint # Query DNS Records on Windows Multi-Region deployment will best ensure global availability . Backup retention from 0 to 35 days Read Replicas for performance improvement. Quering read replica can have a delay of less than a minute. They are usefull when: scaling due to excess read traffic source db unavailable reporting and data warehousing disaster recovery hosting a cross region with lower latency MS SQL Server cannot be read replica From the RDS instance, select the action Create read replica (must have backups turned on) and check the option Public accesible. A new RDS instance is created for the read replica wihout any impact . A read replica can be promoted as a standalone instance. Note that the promotion process is irreversible. Use of AWS EBS volumes for database and log storage. Only Aurora uses its storage system. We can manage RDS instances (CPU and Memory) using AWS CLI, AWS RDS API or management console. RDS run on VM, but logging is not allowed. Patching RDS is Amazon's responsability Scaling vertical scaling (compute -CPU- OR memory) with a new RDS instance of DB instance class. Instance may shutdown scaling with AWS EBS storage horizontal scaling with read replicas multiAZ takes more time for vertical scaling, but RDS is shutted down less time than working on a single AZ Backup options backups in S3 are stored in an RDS own bucket. backup is for the db host, not only the databases. only the differences are stored in the new snapshot. in multiAZ there should not be impact. In single AZ, I/O is suspend from few seconds to few minutes. restoring a backup create a new RDS instance. you cannot restore to an existing db instance. retain Parameter Group and Security Group. Inbound rules should allow the connection using the DB port Backup types: automated backup: in a multiAZ scenario, data is backed up dialy taking a snapshot from the EBS volume of the secondary RDS instance scheduled window retention data can be restored to point in time on intervals of 5 minutes based on transactions logs . manual snapshots. From RDS instance, Take DB Snapshot kept till deleted only recommended before large changes we can copy snapshot to different regions (crossregion copy) To mitigate the slow restore process: Restore a RDS instance with high I/O capacity Maximize the I/O during the restore process Pricing DB Engine and version License model DB Engine class Multi-AZ: the price is twice Storage type and allocation Instance hours Database Storage: EBS VS Aurora Size of bakcup storage Outgoing data transfer Data transferred between Availability Zones for replication of Multi-AZ deployments is free Reserve instance when long terms for both Multi-AZ and Single-AZ configurations Security Network isolation, using VPC: Private subnet Security group (firewall) Public accessibility (turn off) ClassicLink Direct Connect : dedicated line from on premise to AWS to improve the network connection VPC Peering IAM for access control: Do not use AWS root credentials IAM for RDS management MFA for extra level of protection Integrated security Active directory for SQL Server IAM Authentication MySQL, PostgreSQL, Aurora Encryption at rest: it is free to encrypt data using KMS ( K ey M anagement S ystem) and AES-256 encryption is replicated to all nodes, backups and snapshots cannot decrypt once encrypted two tier encryption: enable key rotation SSL for db conectivity Monitoring RDS sends metrics to CloudWatch 15-18 metrics based on the instance class (CPU, free storage space, network traffic, database connections and IOPS) Performance Insights is a free database performance dashboard, but it is not available for db.t2 instances. AWS RDS Events to be notified when events occurs AWS Config for configuration changes AWS CloudTrail for audit logs of API calls AWS Trusted Advisor for cost optimization, security, fault tolerance and performance improvements recommendations Script to test monitoring: CREATE TABLE scale_data ( section NUMERIC NOT NULL , id1 NUMERIC NOT NULL , id2 NUMERIC NOT NULL ); INSERT INTO scale_data SELECT sections . * , gen . * , CEIL ( RANDOM () * 100 ) FROM GENERATE_SERIES ( 1 , 300 ) sections , GENERATE_SERIES ( 1 , 900000 ) gen WHERE gen <= sections * 3000 ; ``` ### How to create a secured RDS instance 1 . From RDS , ` create a parameter group ` with family postgres10 as PostgresSSL . 2 . Edit PostgresSSL parameter group to enable and enforce ssl ( ssl and rds . force_ssl = 1 ) !!! warning \"Encryption at rest is not enabled for a db.t2.micro, so a dev/test RDS is required\" 3 . Set No Public Accessible and enable encryption 4 . Edit security group inbound / outbound rules filling our private subnet ( 10 . 0 . 1 . 0 / 24 and 10 . 0 . 3 . 0 / 24 ) as source for PostgreSQL type 5 . From IAM , create a new Lambda Role using policy ` AWSLambdaVPCAccessExecutionRole ` named LambdaVPC 6 . From Lambda , ` create function ` called ConnectSecureRDS with runtime as node . js 10 . x and LambdaVPC role 7 . Select our VPC with private subnets and the security group already edited with inbound / outbound rules filling our private subnet ( 10 . 0 . 1 . 0 / 24 and 10 . 0 . 3 . 0 / 24 ) as source for PostgreSQL type 8 . Upload the . zip file for the function code : ``` js const { Client } = require ( 'pg' ) // import node . js Postgres library exports . handler = function ( event , context , callback ) { const client = new Client () client . connect () client . query ( 'SELECT $1::text as message' , [ 'Hello world!' ], ( err , res ) => { console . log ( err ? err . stack : res . rows [ 0 ]. message ) // Hello World ! client . end () callback ( err , res ); } ); } ``` 9 . Edit Environment variables * PGHOST = RDS instance endpoint * PGUSER = acgrds * PGPASSWORD = test123123 * PGDATABASE = ACGRDS * PGSSLMODE = require 10 . Test the lambda function 11 . Change VPV and test again 12 . Change VPC subnet and test again . 13 . Remove PGSSLMODE variable and test again . ### How to create an RDS instance ( PostgreSQL ) 1 . Download [ pgAdmin ]( https : // www . postgresql . org / ftp / pgadmin / pgadmin4 / v4 . 27 / windows / ) and install -> [ http : // 127 . 0 . 0 . 1 : 54689 / browser ]( http : // 127 . 0 . 0 . 1 : 54689 / browser ) 2 . Create a VPC using the wizard : * Select VPC with Public and Private Subnets * Select all default settings but Elastic IP Allocation ID . If not found , copy and paste 3 . Configure RDS subnet groups : * Create DB Subnet groups ( public and private ) !!! error \"DB Subnet Group doesn't meet availability zone coverage requirement. Please add subnets to cover at least 2 availability zones. Current coverage: 1\" * Create a new public ( 10 . 0 . 2 . 0 / 24 ) and private ( 10 . 0 . 3 . 0 / 24 ) subnets in a different AZ . Then try to create the subnet group again * Edit route table association for the two new subnets . private subnets with nat and public subnets wit igw ( internet gateway ) 4 . From ` RDS ` , click ` Create Database ` button 5 . Select PostgreSQL engine type , previous to last version , free tier template , db . t2 . micro as db instance class , no multi - AZ 6 . Use rnietoe and Abodroc83 for credencials for db name mypostgresql 7 . Select the VPC created above and the public subnet group . ** Public access ** 8 . ** Security groups are required for publicly accessible databases ** . Create new VPC security group ., so port 5432 will be opened to allow the PostgreSQL access . 9 . Run pgAdmin and create a new server Lab1 using the RDS endpoint as hostname ( ` mypostgresql . cmr9irlg1qe3 . us - east - 1 . rds . amazonaws . com ` ), and the user and pwd ! []( img / PostgreSQL . PNG ) ``` sql CREATE TABLE test_data ( section NUMERIC NOT NULL , id1 NUMERIC NOT NULL , id2 NUMERIC NOT NULL ); INSERT INTO test_data SELECT sections . * , gen . * , CEIL ( RANDOM () * 100 ) FROM GENERATE_SERIES ( 1 , 30 ) sections , GENERATE_SERIES ( 1 , 90 ) gen WHERE gen <= sections * 3000 ; How to create an RDS instance for WordPress \u00b6 From RDS , click Create Database button Select MySQL engine type, free tier template (without Multi-AZ) and fill the database name (db instance identifier) Use rnietoe and Abodroc83 for credencials Set initial db name as rnietoe in the additional configuration. If you do not specify a database name, Amazon RDS does not create a database. When creating an RDS instance, you can select the Availability Zone into which you deploy it. automated backups are enabled by default, till 35 days Create a new EC2 instance with our WebDMZ security group, our key pair rnietoe and the following advanced details: #!/bin/bash yum install httpd php php-mysql -y amazon-linux-extras install -y php7.2 cd /var/www/html wget https://wordpress.org/wordpress-5.4.1.tar.gz tar -xzf wordpress-5.4.1.tar.gz cp -r wordpress/* /var/www/html/ rm -rf wordpress rm -rf wordpress-5.4.1.tar.gz chmod -R 755 wp-content chown -R apache:apache wp-content service httpd start chkconfig httpd on Allow connection between EC2 and RDS clicking on defult VPC security group, inbound rules , edit to add a rule with: MySQL/Aurora type protocol TCP port 3306 WebDMZ security group Browse to the EC2 public IP and see the wordpress home page. Fill db name, user name, pwd and RDS endpoint (rnietoe.cmr9irlg1qe3.us-east-1.rds.amazonaws.com) Create the wp-config.php file manually: ssh ec2-user@52.87.161.80 -i rnietoe.pem cd /var/www/html nano wp-config.php # here paste wp-config.php from the wordpress home page. Ctrl+X to save it From the wordpress home page, Run the installation , fill the same credentials and Install WordPress Login to wordpress Configure a ELB target group to set WordPress settings URL with a DNS address instead of a public IP Finally we create a EC2 instance image, like a snapshot . this is called AMI ( A mazon M achine I mange) How to create a fault-tolerance wordpress app \u00b6 Create Launch Configuration named MyLaunchConfigurationGroup select AMI MyworkPressTemplate select instance type t2.micro advance settings: #!/bin/bash yum update -y select our Security group (virtual firewall) and key pair Create Auto Scaling Group named MyAutoScalingGroup Select MyLaunchConfigurationGroup Select all subnets (AZ) Enable ELB and select my target group Scale between 2 and 3 instances (group size) Set a scaling policy. for example, when cpu is 90% notifications and tags not required here Select the ASG ( A uto S caling G roup) created and see the Activity tab. Two instances are created. Browse to the ELB DNS adress and create new post in wordpress . Delete instances, wait ASG create new instances and check the post is still there Amazon Aurora \u00b6 It has a hight performance with a low cost. 2-3x faster thant postgreSQL and 5x faster than MySQL Move Logging and Storage layers into a multi tented scale out database optimized service Storage from 10 gb to 64tb Continuous backup to S3 Aurora DB Cluster till 15 replicas Cluster endpoint connection allow write operations Reader endpoint connection allow read operations Custom endpoint connection allow load balancer Instance endpoint connection to a specific instance Aurora global databases primary region - read and write secondary region - read only. Promoted when failure Aurora store by default 6 copies of my data (2x3) From RDS , create Amazon Aurora database with PostgreSQL compatibility Tree instances are created: regional - with writer and read endpoints writer reader From actions, create aurora read replica (writer and reader node) Aurora Serverless capacity type: Serverless You specify the minimum and maximum amount of resources needed, and Aurora scales the capacity based on database load. This is a good option for intermittent or unpredictable workloads. only an endpoint is created //test lambda function var mysql = require ( 'mysql' ); exports . handler = ( event , context ) => { var connection = mysql . createConnection ({ host : '' , user : '' , password : '' }); connection . connect ( function ( err ) { if ( err ) throw err ; console . log ( \"Connected!\" ); }); connection . query ( \"SELECT 'Hello World!';\" , function ( err , result ) { if ( err ) throw err ; console . log ( result ); context . succeed ( 'Success' ); }); }; DynamoDB \u00b6 DynamoDB (Non Relational Databases) is a key-value and document database that delivers single-digit millisecond performance at any scale. DynamoDB provide automatic replication across AZs. DynamoDB is distributed across three geographically distinct datacentres by default Eventual consistent reads (default) sample of 2 seconds Strongly consistent reads. sample of less than 1 second DAX (DynamoDB Accelerator) is an advanced DynamoDB There will always be a charge for: provisioning read and write capacity the storage of data RedShift \u00b6 OLAP (OnLine Analytics Processing) Amazon's Data WareHousing used for Online Anaylitcs Processing Used for Business Intelligence availabled in 1 AZ EMR \u00b6 EMR ( E lastic M ap R educe) is a web service that makes it easy to process large amounts of data efficiently. sample of big data cluster with: master node (store logs by default) core node task node (optional) logs must be defined on cluster creation ElastiCache \u00b6 Improve performance with cache for the most common queries: Memcached Redis (muti AZ) Graph Databases \u00b6 Amazon Neptune","title":"8. Databases"},{"location":"AWS/8-Databases/#8-databases","text":"","title":"8. Databases"},{"location":"AWS/8-Databases/#rds","text":"AWS RDS ( R elational D atabase S ervice): MS SQL Server (up to 16TB of storage when using the Provisioned IOPS and General Purpose - SSD - storage) Oracle. Includes license model MySQL (port 3306) Amazon Aurora. This is up to 5X faster than a traditional MySQL database. MySQL and PostgreSQL compatibility. No free tier MariaDB PostgreSQL (port 5432) Some features: Multi-AZ for disaster recovery and high availability . By default in production RDS. Primary host replicates to a secondary host when failover. Enable multiAZ impact to the RDS instance if running host -t NS database_endpoint # Query DNS Records on Linux nslookup database_endpoint # Query DNS Records on Windows Multi-Region deployment will best ensure global availability . Backup retention from 0 to 35 days Read Replicas for performance improvement. Quering read replica can have a delay of less than a minute. They are usefull when: scaling due to excess read traffic source db unavailable reporting and data warehousing disaster recovery hosting a cross region with lower latency MS SQL Server cannot be read replica From the RDS instance, select the action Create read replica (must have backups turned on) and check the option Public accesible. A new RDS instance is created for the read replica wihout any impact . A read replica can be promoted as a standalone instance. Note that the promotion process is irreversible. Use of AWS EBS volumes for database and log storage. Only Aurora uses its storage system. We can manage RDS instances (CPU and Memory) using AWS CLI, AWS RDS API or management console. RDS run on VM, but logging is not allowed. Patching RDS is Amazon's responsability Scaling vertical scaling (compute -CPU- OR memory) with a new RDS instance of DB instance class. Instance may shutdown scaling with AWS EBS storage horizontal scaling with read replicas multiAZ takes more time for vertical scaling, but RDS is shutted down less time than working on a single AZ Backup options backups in S3 are stored in an RDS own bucket. backup is for the db host, not only the databases. only the differences are stored in the new snapshot. in multiAZ there should not be impact. In single AZ, I/O is suspend from few seconds to few minutes. restoring a backup create a new RDS instance. you cannot restore to an existing db instance. retain Parameter Group and Security Group. Inbound rules should allow the connection using the DB port Backup types: automated backup: in a multiAZ scenario, data is backed up dialy taking a snapshot from the EBS volume of the secondary RDS instance scheduled window retention data can be restored to point in time on intervals of 5 minutes based on transactions logs . manual snapshots. From RDS instance, Take DB Snapshot kept till deleted only recommended before large changes we can copy snapshot to different regions (crossregion copy) To mitigate the slow restore process: Restore a RDS instance with high I/O capacity Maximize the I/O during the restore process Pricing DB Engine and version License model DB Engine class Multi-AZ: the price is twice Storage type and allocation Instance hours Database Storage: EBS VS Aurora Size of bakcup storage Outgoing data transfer Data transferred between Availability Zones for replication of Multi-AZ deployments is free Reserve instance when long terms for both Multi-AZ and Single-AZ configurations Security Network isolation, using VPC: Private subnet Security group (firewall) Public accessibility (turn off) ClassicLink Direct Connect : dedicated line from on premise to AWS to improve the network connection VPC Peering IAM for access control: Do not use AWS root credentials IAM for RDS management MFA for extra level of protection Integrated security Active directory for SQL Server IAM Authentication MySQL, PostgreSQL, Aurora Encryption at rest: it is free to encrypt data using KMS ( K ey M anagement S ystem) and AES-256 encryption is replicated to all nodes, backups and snapshots cannot decrypt once encrypted two tier encryption: enable key rotation SSL for db conectivity Monitoring RDS sends metrics to CloudWatch 15-18 metrics based on the instance class (CPU, free storage space, network traffic, database connections and IOPS) Performance Insights is a free database performance dashboard, but it is not available for db.t2 instances. AWS RDS Events to be notified when events occurs AWS Config for configuration changes AWS CloudTrail for audit logs of API calls AWS Trusted Advisor for cost optimization, security, fault tolerance and performance improvements recommendations Script to test monitoring: CREATE TABLE scale_data ( section NUMERIC NOT NULL , id1 NUMERIC NOT NULL , id2 NUMERIC NOT NULL ); INSERT INTO scale_data SELECT sections . * , gen . * , CEIL ( RANDOM () * 100 ) FROM GENERATE_SERIES ( 1 , 300 ) sections , GENERATE_SERIES ( 1 , 900000 ) gen WHERE gen <= sections * 3000 ; ``` ### How to create a secured RDS instance 1 . From RDS , ` create a parameter group ` with family postgres10 as PostgresSSL . 2 . Edit PostgresSSL parameter group to enable and enforce ssl ( ssl and rds . force_ssl = 1 ) !!! warning \"Encryption at rest is not enabled for a db.t2.micro, so a dev/test RDS is required\" 3 . Set No Public Accessible and enable encryption 4 . Edit security group inbound / outbound rules filling our private subnet ( 10 . 0 . 1 . 0 / 24 and 10 . 0 . 3 . 0 / 24 ) as source for PostgreSQL type 5 . From IAM , create a new Lambda Role using policy ` AWSLambdaVPCAccessExecutionRole ` named LambdaVPC 6 . From Lambda , ` create function ` called ConnectSecureRDS with runtime as node . js 10 . x and LambdaVPC role 7 . Select our VPC with private subnets and the security group already edited with inbound / outbound rules filling our private subnet ( 10 . 0 . 1 . 0 / 24 and 10 . 0 . 3 . 0 / 24 ) as source for PostgreSQL type 8 . Upload the . zip file for the function code : ``` js const { Client } = require ( 'pg' ) // import node . js Postgres library exports . handler = function ( event , context , callback ) { const client = new Client () client . connect () client . query ( 'SELECT $1::text as message' , [ 'Hello world!' ], ( err , res ) => { console . log ( err ? err . stack : res . rows [ 0 ]. message ) // Hello World ! client . end () callback ( err , res ); } ); } ``` 9 . Edit Environment variables * PGHOST = RDS instance endpoint * PGUSER = acgrds * PGPASSWORD = test123123 * PGDATABASE = ACGRDS * PGSSLMODE = require 10 . Test the lambda function 11 . Change VPV and test again 12 . Change VPC subnet and test again . 13 . Remove PGSSLMODE variable and test again . ### How to create an RDS instance ( PostgreSQL ) 1 . Download [ pgAdmin ]( https : // www . postgresql . org / ftp / pgadmin / pgadmin4 / v4 . 27 / windows / ) and install -> [ http : // 127 . 0 . 0 . 1 : 54689 / browser ]( http : // 127 . 0 . 0 . 1 : 54689 / browser ) 2 . Create a VPC using the wizard : * Select VPC with Public and Private Subnets * Select all default settings but Elastic IP Allocation ID . If not found , copy and paste 3 . Configure RDS subnet groups : * Create DB Subnet groups ( public and private ) !!! error \"DB Subnet Group doesn't meet availability zone coverage requirement. Please add subnets to cover at least 2 availability zones. Current coverage: 1\" * Create a new public ( 10 . 0 . 2 . 0 / 24 ) and private ( 10 . 0 . 3 . 0 / 24 ) subnets in a different AZ . Then try to create the subnet group again * Edit route table association for the two new subnets . private subnets with nat and public subnets wit igw ( internet gateway ) 4 . From ` RDS ` , click ` Create Database ` button 5 . Select PostgreSQL engine type , previous to last version , free tier template , db . t2 . micro as db instance class , no multi - AZ 6 . Use rnietoe and Abodroc83 for credencials for db name mypostgresql 7 . Select the VPC created above and the public subnet group . ** Public access ** 8 . ** Security groups are required for publicly accessible databases ** . Create new VPC security group ., so port 5432 will be opened to allow the PostgreSQL access . 9 . Run pgAdmin and create a new server Lab1 using the RDS endpoint as hostname ( ` mypostgresql . cmr9irlg1qe3 . us - east - 1 . rds . amazonaws . com ` ), and the user and pwd ! []( img / PostgreSQL . PNG ) ``` sql CREATE TABLE test_data ( section NUMERIC NOT NULL , id1 NUMERIC NOT NULL , id2 NUMERIC NOT NULL ); INSERT INTO test_data SELECT sections . * , gen . * , CEIL ( RANDOM () * 100 ) FROM GENERATE_SERIES ( 1 , 30 ) sections , GENERATE_SERIES ( 1 , 90 ) gen WHERE gen <= sections * 3000 ;","title":"RDS"},{"location":"AWS/8-Databases/#how-to-create-an-rds-instance-for-wordpress","text":"From RDS , click Create Database button Select MySQL engine type, free tier template (without Multi-AZ) and fill the database name (db instance identifier) Use rnietoe and Abodroc83 for credencials Set initial db name as rnietoe in the additional configuration. If you do not specify a database name, Amazon RDS does not create a database. When creating an RDS instance, you can select the Availability Zone into which you deploy it. automated backups are enabled by default, till 35 days Create a new EC2 instance with our WebDMZ security group, our key pair rnietoe and the following advanced details: #!/bin/bash yum install httpd php php-mysql -y amazon-linux-extras install -y php7.2 cd /var/www/html wget https://wordpress.org/wordpress-5.4.1.tar.gz tar -xzf wordpress-5.4.1.tar.gz cp -r wordpress/* /var/www/html/ rm -rf wordpress rm -rf wordpress-5.4.1.tar.gz chmod -R 755 wp-content chown -R apache:apache wp-content service httpd start chkconfig httpd on Allow connection between EC2 and RDS clicking on defult VPC security group, inbound rules , edit to add a rule with: MySQL/Aurora type protocol TCP port 3306 WebDMZ security group Browse to the EC2 public IP and see the wordpress home page. Fill db name, user name, pwd and RDS endpoint (rnietoe.cmr9irlg1qe3.us-east-1.rds.amazonaws.com) Create the wp-config.php file manually: ssh ec2-user@52.87.161.80 -i rnietoe.pem cd /var/www/html nano wp-config.php # here paste wp-config.php from the wordpress home page. Ctrl+X to save it From the wordpress home page, Run the installation , fill the same credentials and Install WordPress Login to wordpress Configure a ELB target group to set WordPress settings URL with a DNS address instead of a public IP Finally we create a EC2 instance image, like a snapshot . this is called AMI ( A mazon M achine I mange)","title":"How to create an RDS instance for WordPress"},{"location":"AWS/8-Databases/#how-to-create-a-fault-tolerance-wordpress-app","text":"Create Launch Configuration named MyLaunchConfigurationGroup select AMI MyworkPressTemplate select instance type t2.micro advance settings: #!/bin/bash yum update -y select our Security group (virtual firewall) and key pair Create Auto Scaling Group named MyAutoScalingGroup Select MyLaunchConfigurationGroup Select all subnets (AZ) Enable ELB and select my target group Scale between 2 and 3 instances (group size) Set a scaling policy. for example, when cpu is 90% notifications and tags not required here Select the ASG ( A uto S caling G roup) created and see the Activity tab. Two instances are created. Browse to the ELB DNS adress and create new post in wordpress . Delete instances, wait ASG create new instances and check the post is still there","title":"How to create a fault-tolerance wordpress app"},{"location":"AWS/8-Databases/#amazon-aurora","text":"It has a hight performance with a low cost. 2-3x faster thant postgreSQL and 5x faster than MySQL Move Logging and Storage layers into a multi tented scale out database optimized service Storage from 10 gb to 64tb Continuous backup to S3 Aurora DB Cluster till 15 replicas Cluster endpoint connection allow write operations Reader endpoint connection allow read operations Custom endpoint connection allow load balancer Instance endpoint connection to a specific instance Aurora global databases primary region - read and write secondary region - read only. Promoted when failure Aurora store by default 6 copies of my data (2x3) From RDS , create Amazon Aurora database with PostgreSQL compatibility Tree instances are created: regional - with writer and read endpoints writer reader From actions, create aurora read replica (writer and reader node) Aurora Serverless capacity type: Serverless You specify the minimum and maximum amount of resources needed, and Aurora scales the capacity based on database load. This is a good option for intermittent or unpredictable workloads. only an endpoint is created //test lambda function var mysql = require ( 'mysql' ); exports . handler = ( event , context ) => { var connection = mysql . createConnection ({ host : '' , user : '' , password : '' }); connection . connect ( function ( err ) { if ( err ) throw err ; console . log ( \"Connected!\" ); }); connection . query ( \"SELECT 'Hello World!';\" , function ( err , result ) { if ( err ) throw err ; console . log ( result ); context . succeed ( 'Success' ); }); };","title":"Amazon Aurora"},{"location":"AWS/8-Databases/#dynamodb","text":"DynamoDB (Non Relational Databases) is a key-value and document database that delivers single-digit millisecond performance at any scale. DynamoDB provide automatic replication across AZs. DynamoDB is distributed across three geographically distinct datacentres by default Eventual consistent reads (default) sample of 2 seconds Strongly consistent reads. sample of less than 1 second DAX (DynamoDB Accelerator) is an advanced DynamoDB There will always be a charge for: provisioning read and write capacity the storage of data","title":"DynamoDB"},{"location":"AWS/8-Databases/#redshift","text":"OLAP (OnLine Analytics Processing) Amazon's Data WareHousing used for Online Anaylitcs Processing Used for Business Intelligence availabled in 1 AZ","title":"RedShift"},{"location":"AWS/8-Databases/#emr","text":"EMR ( E lastic M ap R educe) is a web service that makes it easy to process large amounts of data efficiently. sample of big data cluster with: master node (store logs by default) core node task node (optional) logs must be defined on cluster creation","title":"EMR"},{"location":"AWS/8-Databases/#elasticache","text":"Improve performance with cache for the most common queries: Memcached Redis (muti AZ)","title":"ElastiCache"},{"location":"AWS/8-Databases/#graph-databases","text":"Amazon Neptune","title":"Graph Databases"},{"location":"Git/1-Git-Essential-Training_The-Basics/","text":"Git Essential Training: The Basics \u00b6 Configuration \u00b6 Install git from https://git-scm.com System : C:\\Program Files\\Git\\etc\\gitconfig: git config --system --list User : C:\\Users\\rniet.gitconfig: git config --global --list Project : my_project/.git/config: git config --list Other configuration settings: git config --global user.name \"Rafael Nieto\" git config --global user.email \"rnietoe@gmail.com\" git config --global core.editor \"code --wait\"` git config --list Other basic commands: change to my user directory: cd ~ list all items (hidden as well): ls -la display a text file cat .gitconfig Getting started \u00b6 Initialize and clone a repository git init git clone https://github.com/rnietoe/Training.git git clone https://github.com/rnietoe/Training.git local_repository_name Stage and commit changes git add . git commit -m \"commit message\" git log commands git log git log - 2 git log --since=2020-04-04 git log --until=2020-04-04 git log --until=\"3 days ago\" git log - L 100 , 150 : filename . txt git log --author=\"Rafael\" git log --grep=\"bug\" git log --oneline git log --stat git log --format=medium git log --format=short git log --format=oneline git log --graph --all --oneline --decorated List logs as patches (diff) git log --pathces git log -p Git Concepts and Architecture \u00b6 Make changes to files \u00b6 Status \u00b6 git status Untracked : created but unstagged yet Added : created and stagged (\"cached\") Modified Deleted Renamed Differences \u00b6 compare the staging tree and your working directory git diff compare the repository and the staging tree: git diff --stagged show only the words that are different: git diff --color-words Other changes \u00b6 remove the file from the working directory and stage the change in a single git command: git rm filename rename filename as newname and stage the change in a single git command: git mv filename newname Commit all \u00b6 Stage and commit all changes from working directory directly to the repository (instead to the staging tree) git commit -all git commit -am \"commit message\" it does not include untracked files Multiline commit message \u00b6 git commit -a Because of we didn't specify the message parameter, the .git/COMMIT_EDITMSG file is opened in our editor to enter a multiline commit message. When git/COMMIT_EDITMSG file is closed, the commit is executed. The commit is aborted if commit message is empty. Inspect a commit \u00b6 git show HEAD git show commitID git show commitID --color-words where commitID is the first characters of commit Id (6 to 8 characters are enough) Use space or f to go forward in the paginator, b to go backward, / to search words and q to quit. Compare commits \u00b6 git diff commitID1..HEAD git diff commitID1..commitID2 git diff commitID1..commitID2 --color-words Undo changes \u00b6 To discard changes in the working directory: git checkout -- \"filename\" git checkout -- . Above command does not check out any branch. Instead, it checks out a single file or every file, from the repository To discard changes in the staging tree (to the working directory): git reset HEAD filename Amend/Edit commits \u00b6 git commit --amend -m \"new commit message\" It takes what's in staging and add it to the latest commit amend command is also usefull to update the message of the latest commit Retrieve old versions \u00b6 Retrieve files with changes of desired commit ID git checkout commitId -- . git checkout commitId -- filename Revert a commit \u00b6 git revert commitID Remove untracked \u00b6 Remove untracked files from the working directory git clean -n # display what would be removed git clean -f # remove untracked files Ignore files \u00b6 project/.gitignore using: regexp with: * ? [aeiou] [0-9]. Sample of ignore all log files from logs directory: logs/*.log.[0-9] negative expressions with !: Sample of ignore all php files with *.php, but do not ignore index file with !index.php trailing slash /: Sample of ignore all files in a directory: asset/videos/ # This is a comment in the project/.gitignore file We should ignore: compiled source code packages and compressed files logs and databases Operative System generated files User-uploaded assets (images, PDFs, videos) .gitignore templates Ignore files globally \u00b6 Ignore files globally (instead by project): git config --global core.excludesfile ~/.gitignore_global Ignore files to be tracked \u00b6 git rm --cached filename Track empty directories \u00b6 Create .gitkeep file to track empty directories touch dirname/.gitkeep","title":"Git Essential Training: The Basics"},{"location":"Git/1-Git-Essential-Training_The-Basics/#git-essential-training-the-basics","text":"","title":"Git Essential Training: The Basics"},{"location":"Git/1-Git-Essential-Training_The-Basics/#configuration","text":"Install git from https://git-scm.com System : C:\\Program Files\\Git\\etc\\gitconfig: git config --system --list User : C:\\Users\\rniet.gitconfig: git config --global --list Project : my_project/.git/config: git config --list Other configuration settings: git config --global user.name \"Rafael Nieto\" git config --global user.email \"rnietoe@gmail.com\" git config --global core.editor \"code --wait\"` git config --list Other basic commands: change to my user directory: cd ~ list all items (hidden as well): ls -la display a text file cat .gitconfig","title":"Configuration"},{"location":"Git/1-Git-Essential-Training_The-Basics/#getting-started","text":"Initialize and clone a repository git init git clone https://github.com/rnietoe/Training.git git clone https://github.com/rnietoe/Training.git local_repository_name Stage and commit changes git add . git commit -m \"commit message\" git log commands git log git log - 2 git log --since=2020-04-04 git log --until=2020-04-04 git log --until=\"3 days ago\" git log - L 100 , 150 : filename . txt git log --author=\"Rafael\" git log --grep=\"bug\" git log --oneline git log --stat git log --format=medium git log --format=short git log --format=oneline git log --graph --all --oneline --decorated List logs as patches (diff) git log --pathces git log -p","title":"Getting started"},{"location":"Git/1-Git-Essential-Training_The-Basics/#git-concepts-and-architecture","text":"","title":"Git Concepts and Architecture"},{"location":"Git/1-Git-Essential-Training_The-Basics/#make-changes-to-files","text":"","title":"Make changes to files"},{"location":"Git/1-Git-Essential-Training_The-Basics/#status","text":"git status Untracked : created but unstagged yet Added : created and stagged (\"cached\") Modified Deleted Renamed","title":"Status"},{"location":"Git/1-Git-Essential-Training_The-Basics/#differences","text":"compare the staging tree and your working directory git diff compare the repository and the staging tree: git diff --stagged show only the words that are different: git diff --color-words","title":"Differences"},{"location":"Git/1-Git-Essential-Training_The-Basics/#other-changes","text":"remove the file from the working directory and stage the change in a single git command: git rm filename rename filename as newname and stage the change in a single git command: git mv filename newname","title":"Other changes"},{"location":"Git/1-Git-Essential-Training_The-Basics/#commit-all","text":"Stage and commit all changes from working directory directly to the repository (instead to the staging tree) git commit -all git commit -am \"commit message\" it does not include untracked files","title":"Commit all"},{"location":"Git/1-Git-Essential-Training_The-Basics/#multiline-commit-message","text":"git commit -a Because of we didn't specify the message parameter, the .git/COMMIT_EDITMSG file is opened in our editor to enter a multiline commit message. When git/COMMIT_EDITMSG file is closed, the commit is executed. The commit is aborted if commit message is empty.","title":"Multiline commit message"},{"location":"Git/1-Git-Essential-Training_The-Basics/#inspect-a-commit","text":"git show HEAD git show commitID git show commitID --color-words where commitID is the first characters of commit Id (6 to 8 characters are enough) Use space or f to go forward in the paginator, b to go backward, / to search words and q to quit.","title":"Inspect a commit"},{"location":"Git/1-Git-Essential-Training_The-Basics/#compare-commits","text":"git diff commitID1..HEAD git diff commitID1..commitID2 git diff commitID1..commitID2 --color-words","title":"Compare commits"},{"location":"Git/1-Git-Essential-Training_The-Basics/#undo-changes","text":"To discard changes in the working directory: git checkout -- \"filename\" git checkout -- . Above command does not check out any branch. Instead, it checks out a single file or every file, from the repository To discard changes in the staging tree (to the working directory): git reset HEAD filename","title":"Undo changes"},{"location":"Git/1-Git-Essential-Training_The-Basics/#amendedit-commits","text":"git commit --amend -m \"new commit message\" It takes what's in staging and add it to the latest commit amend command is also usefull to update the message of the latest commit","title":"Amend/Edit commits"},{"location":"Git/1-Git-Essential-Training_The-Basics/#retrieve-old-versions","text":"Retrieve files with changes of desired commit ID git checkout commitId -- . git checkout commitId -- filename","title":"Retrieve old versions"},{"location":"Git/1-Git-Essential-Training_The-Basics/#revert-a-commit","text":"git revert commitID","title":"Revert a commit"},{"location":"Git/1-Git-Essential-Training_The-Basics/#remove-untracked","text":"Remove untracked files from the working directory git clean -n # display what would be removed git clean -f # remove untracked files","title":"Remove untracked"},{"location":"Git/1-Git-Essential-Training_The-Basics/#ignore-files","text":"project/.gitignore using: regexp with: * ? [aeiou] [0-9]. Sample of ignore all log files from logs directory: logs/*.log.[0-9] negative expressions with !: Sample of ignore all php files with *.php, but do not ignore index file with !index.php trailing slash /: Sample of ignore all files in a directory: asset/videos/ # This is a comment in the project/.gitignore file We should ignore: compiled source code packages and compressed files logs and databases Operative System generated files User-uploaded assets (images, PDFs, videos) .gitignore templates","title":"Ignore files"},{"location":"Git/1-Git-Essential-Training_The-Basics/#ignore-files-globally","text":"Ignore files globally (instead by project): git config --global core.excludesfile ~/.gitignore_global","title":"Ignore files globally"},{"location":"Git/1-Git-Essential-Training_The-Basics/#ignore-files-to-be-tracked","text":"git rm --cached filename","title":"Ignore files to be tracked"},{"location":"Git/1-Git-Essential-Training_The-Basics/#track-empty-directories","text":"Create .gitkeep file to track empty directories touch dirname/.gitkeep","title":"Track empty directories"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/","text":"Git: Branches, Merges, and Remotes \u00b6 Tree-ish \u00b6 Git references \u00b6 SHA1 hash: the commit ID HEAD Branch Tag Ancestry Parents: abcd1234^, master^, HEAD^, HEAD~1, HEAD~ Grandparent: abcd1234^^, master^^, HEAD^^, HEAD~2 Grat-Grandprarents: abcd1234^^^, master^^^, HEAD^^^, HEAD~3 git show HEAD^ git show HEAD^^ git show HEAD~3 Tree Listing \u00b6 List of blobs ( b inary l arge ob ject = file) and trees (tree = directory) of a tree git ls-tree HEAD git ls-tree HEAD^ git ls-tree HEAD tree/ Filter the commit log after a commitID to HEAD git log abcd1234.. git log abcd1234..HEAD git log <SHA>..<SHA> Filter the commit log by file or directory git log filename git log dirname Branches \u00b6 Create a new branch: git branch branch_name Note that new branch and its parent branch are the same at the moment. There are not differences because there were not commits yet. List all branches. Currently checkout branch is shown in green color: git branch Switch branches git checkout branch_name Switch to a new branch git checkout -b branch_name Git rejects to switch with uncommited changes. The options are: commit the changes to the current branch remove the changes checking out the file stash the changes Compare branches \u00b6 git diff master..new_feature git diff --color-words <SHA>..<SHA> As a general rule, the older branch should go firsts, so changes shown have occurred since that point in time. Rename branches git branch -m new_branch_name Delete branches git branch -d branch_name Cannot delete the checked out branch Cannot delete branches not fully merged. Instead, use capital -D , but commits in the branch will be lost. Reset Branches \u00b6 Back up the latest commitID before, in case you want to undo the reset execution Soft reset: Latest commits changes are staged and pending to commit. Useful when you want to back things up in the commit timeline. git reset --soft <SHA> Mixed reset (default reset option): Latest commits changes are unstaged, pending to stage and commit. git reset --mixed <SHA> git reset <SHA> Hard reset: Latest commits changes are nowhere. Useful to permanently undo commits (SHA=commitID) or if you want to make one branch look like another (sha=branchID) git reset --hard <SHA> Create a new branch as backup before a hard reset Merge Branches \u00b6 The merge concept is like a commit to the checked out branch from another branch. Run a merge with a clean working directory: git merge <branch_name> Find out which other branches already have merged all their commits into this branch git branch --merged Find out which other branches have NOT merged their commits into this branch yet git branch --no-merged Use git log --graph --all --oneline --decorated to check how branches were merged A conflict occurs when two different commits have changes in the same line or set of lines. Solutions: Abort merge git merge --abort Resolve the conflict manually git show --color-words git commit Use the merge tool git mergetool Use a graphical user interface tool, like visual studio To reduce conflicts: Keep lines short Keep commmits small and focused Beware edits to whitespace (spaces, tabs, line return) Merge to master often Merge from master (tracking) Stash Changes \u00b6 Create and view stash: git stash save \"stash_name\" git stash list git stash show stash@{0} git stash show -p stash@{0} Apply stash changes to the workgin directory, removing the single stash: git stash pop git stash pop stash@{0} Apply stash changes to the workgin directory, keeping the single stash: git stash apply git stash apply stash@{0} Delete stash (stash that was not deleted using apply ) git stash drop stash@{0} git stash clear ??? git stash -u git stash --include-untracked Colaborate remotely \u00b6 git commit -m \"from-my-computer\" git push git push origin master If you try to do a push and Git rejects it, then you need to fetch, merge, and then push again. git commit -m \"from-remote-server\" git fetch git fetch does not change the local branch. fetch before you start to work everyday fetch before you push fetch before you go offline fetch often git fetch + git merge = git pull git merge Remote repositories \u00b6 Add a remote repository echo \"# repository_name\" >> README.md git init git add README.md git commit -m \"first commit\" git remote add origin git@github.com:rnietoe/repository_name.git git push -u origin master List remote repositories git remote git remote -v cat .git/config Remote branches \u00b6 Add a remote branch git push -u origin master List remote and all branches git branch -r git branch -a Delete remote branches git push origin :branch_name git push origin --delete branch_name Tracking/Untraking remote branch \u00b6 git branch branch_name origin/branch_name git checkout -b branch_name origin/branch_name ??? git branch -u origin/branch_name branch_name git branch --unset-upstream branch_name Push workflow \u00b6 git checkout master git fetch git merge origin/master git merge local_branch git push Next steps \u00b6 Git Aliases for commonly-used commands \u00b6 Add alias \"st = status\" to ~/.gitconfig git config --global alias.st \"status\" Other examples: st = status co = checkout ci = commit br = branch df = diff dfs = diff -stagged logg = log --graph --decorate --oneline --all Set up SSH keys for remote login \u00b6 Authenticating with GitHub from Git Integrated Development Environments (IDEs) \u00b6 Integrate source code editing with Git features Graphical User Interfaces (GUIs) \u00b6 They have a point-and-click interface for performing Git actions.","title":"Git: Branches, Merges, and Remotes"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#git-branches-merges-and-remotes","text":"","title":"Git: Branches, Merges, and Remotes"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#tree-ish","text":"","title":"Tree-ish"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#git-references","text":"SHA1 hash: the commit ID HEAD Branch Tag Ancestry Parents: abcd1234^, master^, HEAD^, HEAD~1, HEAD~ Grandparent: abcd1234^^, master^^, HEAD^^, HEAD~2 Grat-Grandprarents: abcd1234^^^, master^^^, HEAD^^^, HEAD~3 git show HEAD^ git show HEAD^^ git show HEAD~3","title":"Git references"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#tree-listing","text":"List of blobs ( b inary l arge ob ject = file) and trees (tree = directory) of a tree git ls-tree HEAD git ls-tree HEAD^ git ls-tree HEAD tree/ Filter the commit log after a commitID to HEAD git log abcd1234.. git log abcd1234..HEAD git log <SHA>..<SHA> Filter the commit log by file or directory git log filename git log dirname","title":"Tree Listing"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#branches","text":"Create a new branch: git branch branch_name Note that new branch and its parent branch are the same at the moment. There are not differences because there were not commits yet. List all branches. Currently checkout branch is shown in green color: git branch Switch branches git checkout branch_name Switch to a new branch git checkout -b branch_name Git rejects to switch with uncommited changes. The options are: commit the changes to the current branch remove the changes checking out the file stash the changes","title":"Branches"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#compare-branches","text":"git diff master..new_feature git diff --color-words <SHA>..<SHA> As a general rule, the older branch should go firsts, so changes shown have occurred since that point in time. Rename branches git branch -m new_branch_name Delete branches git branch -d branch_name Cannot delete the checked out branch Cannot delete branches not fully merged. Instead, use capital -D , but commits in the branch will be lost.","title":"Compare branches"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#reset-branches","text":"Back up the latest commitID before, in case you want to undo the reset execution Soft reset: Latest commits changes are staged and pending to commit. Useful when you want to back things up in the commit timeline. git reset --soft <SHA> Mixed reset (default reset option): Latest commits changes are unstaged, pending to stage and commit. git reset --mixed <SHA> git reset <SHA> Hard reset: Latest commits changes are nowhere. Useful to permanently undo commits (SHA=commitID) or if you want to make one branch look like another (sha=branchID) git reset --hard <SHA> Create a new branch as backup before a hard reset","title":"Reset Branches"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#merge-branches","text":"The merge concept is like a commit to the checked out branch from another branch. Run a merge with a clean working directory: git merge <branch_name> Find out which other branches already have merged all their commits into this branch git branch --merged Find out which other branches have NOT merged their commits into this branch yet git branch --no-merged Use git log --graph --all --oneline --decorated to check how branches were merged A conflict occurs when two different commits have changes in the same line or set of lines. Solutions: Abort merge git merge --abort Resolve the conflict manually git show --color-words git commit Use the merge tool git mergetool Use a graphical user interface tool, like visual studio To reduce conflicts: Keep lines short Keep commmits small and focused Beware edits to whitespace (spaces, tabs, line return) Merge to master often Merge from master (tracking)","title":"Merge Branches"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#stash-changes","text":"Create and view stash: git stash save \"stash_name\" git stash list git stash show stash@{0} git stash show -p stash@{0} Apply stash changes to the workgin directory, removing the single stash: git stash pop git stash pop stash@{0} Apply stash changes to the workgin directory, keeping the single stash: git stash apply git stash apply stash@{0} Delete stash (stash that was not deleted using apply ) git stash drop stash@{0} git stash clear ??? git stash -u git stash --include-untracked","title":"Stash Changes"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#colaborate-remotely","text":"git commit -m \"from-my-computer\" git push git push origin master If you try to do a push and Git rejects it, then you need to fetch, merge, and then push again. git commit -m \"from-remote-server\" git fetch git fetch does not change the local branch. fetch before you start to work everyday fetch before you push fetch before you go offline fetch often git fetch + git merge = git pull git merge","title":"Colaborate remotely"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#remote-repositories","text":"Add a remote repository echo \"# repository_name\" >> README.md git init git add README.md git commit -m \"first commit\" git remote add origin git@github.com:rnietoe/repository_name.git git push -u origin master List remote repositories git remote git remote -v cat .git/config","title":"Remote repositories"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#remote-branches","text":"Add a remote branch git push -u origin master List remote and all branches git branch -r git branch -a Delete remote branches git push origin :branch_name git push origin --delete branch_name","title":"Remote branches"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#trackinguntraking-remote-branch","text":"git branch branch_name origin/branch_name git checkout -b branch_name origin/branch_name ??? git branch -u origin/branch_name branch_name git branch --unset-upstream branch_name","title":"Tracking/Untraking remote branch"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#push-workflow","text":"git checkout master git fetch git merge origin/master git merge local_branch git push","title":"Push workflow"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#next-steps","text":"","title":"Next steps"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#git-aliases-for-commonly-used-commands","text":"Add alias \"st = status\" to ~/.gitconfig git config --global alias.st \"status\" Other examples: st = status co = checkout ci = commit br = branch df = diff dfs = diff -stagged logg = log --graph --decorate --oneline --all","title":"Git Aliases for commonly-used commands"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#set-up-ssh-keys-for-remote-login","text":"Authenticating with GitHub from Git","title":"Set up SSH keys for remote login"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#integrated-development-environments-ides","text":"Integrate source code editing with Git features","title":"Integrated Development Environments (IDEs)"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#graphical-user-interfaces-guis","text":"They have a point-and-click interface for performing Git actions.","title":"Graphical User Interfaces (GUIs)"},{"location":"Git/3-Git-Intermediate-Techniques/","text":"Git Intermediate Techniques \u00b6 Prune \u00b6 Delete remote-tracking branches git remote prune origin --dry-run git remote prune origin git fetch --prune git fetch - p Prune all unreachable objects. Do not need to use git prune Tags \u00b6 Reference names to a commit (v1.0, v1.1, v2.0): Lightweight tag git tag tag_name commit_id Annotated tag (most common) git tag -a v1.1 -m \"Version 1.0\" commitId git tag -am \"Version 1.0\" v1.1 commitId List tags git tag git tag --list git tag -l git tag -l \"v1*\" git tag -l -n Work with tags git show v1.1 git diff v1.0..v1.1 Delete a tag git tag --delete v1.1 git tag -d v1.1 Push tags to remote server git push origin v1.1 git push origin --tags Delete remote tags git push origin :v1.1 git push --delete origin v1.1 git push -d origin v1.1 Check out tags git checkout -b new_branch v1.1 Cherry-pick \u00b6 copy a single commit or a range of commits paste in the branch (new commits have different SHAs). git cherry-pick SHA git cherry-pick SHA --edit \"edit commit message\" git cherry-pick SHA -e \"edit commit message\" git cherry-pick SHA..SHA when there are conflicts: git cherry-pick --continue git cherry-pick --abort Stage changes interactively \u00b6 Interactive way to stage changes from the working directory to the staging area: git add --interactive git add -i Hunk \u00b6 Hunk is an area where two files differ. It can be staged, skipped or split. In interactive mode, select option 5 (Patch) and use the wizard with options e (edit), s (split), y (yes) and n (not) to patch portions of codes. Patch mode is not only for interactive mode: git add --patch git add -p Patch parameter is also used in other commands: git stash -p git reset -p git checkout -p git commit -p Patches \u00b6 Diff Patches \u00b6 git diff from-commit to-commit > output.diff Apply difference to our working directory git apply output.diff Formatted Patches \u00b6 Export each commit in Unix mailbox format, for example: export all commits in a range (optional to a file) git format-patch from-commit..to-commit git format-patch from-commit..to-commit --stdout > feature.patch export a single commit git format-patch -1 commitId export all commits on current branch, which are not in master branch (optional to a directory) git format-patch master git format-patch master -o output_directory Apply formatted patches ( a pply m ailbox): git am path/file.patch git am path/*.patch Rebase \u00b6 Usefull to clean commits and keep the history cleaner. Copy commits from one branch, one-by-one, and then replaying/incorporating them at the end of another branch (commits are moved). git rebase master git rebase master new_feature Get the commitID where branch diverges (the latest commit in common) git merge-base master new_feature Merge VS rebase \u00b6 Merge and Rebase have similar ends: they incorporate changes from one branch into another branch Use merge to bring large branches back into master Use rebase to add minor commits in master to a branch Rebasing should be only used on your local, private branches or on branches that you use exclusively; not branches that others are using. Rebase is destructive when there are conflicts. Conflicts will be resolved with new changes in commits, and then: git rebase --continue git rebase --skip git rebase --abort Rebase onto other branches \u00b6 git rebase --onto base upstream branch For example: git rebase --onto master camping expenses Undo rebase \u00b6 Undoing complex rebases may loses SHAs, commit messages, change sets, etc. git reset --hard ORIG_HEAD git rebase --onto camping_commitID master expenses Interactive rebase \u00b6 git rebase -i master new_feature git rebase -i HEAD~3 with options: pick : use commit drop : remove commit reword : use commit, but edit the commit message edit : use commit, but stop for amending squash : use commit, but meld into previous commit fixup : like squash , but discard this commit's log message exec : Pull rebase \u00b6 Fetch from remote, and then rebase instead or merge git pull --rebase git pull -r git pull --rebase=preserve git pull --rebase=interactive Track Problems \u00b6 Blame \u00b6 How wrote this code and when? - who I should blame? git blame filename.txt git blame -w filename.txt git blame -L 100,150 filename.txt git blame -L 100,+50 filename.txt git blame SHA filename.txt where -w ignore whitespace Add a global alias for blame as praise git config --global alias.praise blame Annotate \u00b6 Similar to blame, with different output format git annotate filename.txt Bisect \u00b6 Find the commit that introduced a bug or regression. The last good revision and the first bad revision are marked git bisect start git bisect bad <treeish> git bisect good <treeish> git bisect reset","title":"Git Intermediate Techniques"},{"location":"Git/3-Git-Intermediate-Techniques/#git-intermediate-techniques","text":"","title":"Git Intermediate Techniques"},{"location":"Git/3-Git-Intermediate-Techniques/#prune","text":"Delete remote-tracking branches git remote prune origin --dry-run git remote prune origin git fetch --prune git fetch - p Prune all unreachable objects. Do not need to use git prune","title":"Prune"},{"location":"Git/3-Git-Intermediate-Techniques/#tags","text":"Reference names to a commit (v1.0, v1.1, v2.0): Lightweight tag git tag tag_name commit_id Annotated tag (most common) git tag -a v1.1 -m \"Version 1.0\" commitId git tag -am \"Version 1.0\" v1.1 commitId List tags git tag git tag --list git tag -l git tag -l \"v1*\" git tag -l -n Work with tags git show v1.1 git diff v1.0..v1.1 Delete a tag git tag --delete v1.1 git tag -d v1.1 Push tags to remote server git push origin v1.1 git push origin --tags Delete remote tags git push origin :v1.1 git push --delete origin v1.1 git push -d origin v1.1 Check out tags git checkout -b new_branch v1.1","title":"Tags"},{"location":"Git/3-Git-Intermediate-Techniques/#cherry-pick","text":"copy a single commit or a range of commits paste in the branch (new commits have different SHAs). git cherry-pick SHA git cherry-pick SHA --edit \"edit commit message\" git cherry-pick SHA -e \"edit commit message\" git cherry-pick SHA..SHA when there are conflicts: git cherry-pick --continue git cherry-pick --abort","title":"Cherry-pick"},{"location":"Git/3-Git-Intermediate-Techniques/#stage-changes-interactively","text":"Interactive way to stage changes from the working directory to the staging area: git add --interactive git add -i","title":"Stage changes interactively"},{"location":"Git/3-Git-Intermediate-Techniques/#hunk","text":"Hunk is an area where two files differ. It can be staged, skipped or split. In interactive mode, select option 5 (Patch) and use the wizard with options e (edit), s (split), y (yes) and n (not) to patch portions of codes. Patch mode is not only for interactive mode: git add --patch git add -p Patch parameter is also used in other commands: git stash -p git reset -p git checkout -p git commit -p","title":"Hunk"},{"location":"Git/3-Git-Intermediate-Techniques/#patches","text":"","title":"Patches"},{"location":"Git/3-Git-Intermediate-Techniques/#diff-patches","text":"git diff from-commit to-commit > output.diff Apply difference to our working directory git apply output.diff","title":"Diff Patches"},{"location":"Git/3-Git-Intermediate-Techniques/#formatted-patches","text":"Export each commit in Unix mailbox format, for example: export all commits in a range (optional to a file) git format-patch from-commit..to-commit git format-patch from-commit..to-commit --stdout > feature.patch export a single commit git format-patch -1 commitId export all commits on current branch, which are not in master branch (optional to a directory) git format-patch master git format-patch master -o output_directory Apply formatted patches ( a pply m ailbox): git am path/file.patch git am path/*.patch","title":"Formatted Patches"},{"location":"Git/3-Git-Intermediate-Techniques/#rebase","text":"Usefull to clean commits and keep the history cleaner. Copy commits from one branch, one-by-one, and then replaying/incorporating them at the end of another branch (commits are moved). git rebase master git rebase master new_feature Get the commitID where branch diverges (the latest commit in common) git merge-base master new_feature","title":"Rebase"},{"location":"Git/3-Git-Intermediate-Techniques/#merge-vs-rebase","text":"Merge and Rebase have similar ends: they incorporate changes from one branch into another branch Use merge to bring large branches back into master Use rebase to add minor commits in master to a branch Rebasing should be only used on your local, private branches or on branches that you use exclusively; not branches that others are using. Rebase is destructive when there are conflicts. Conflicts will be resolved with new changes in commits, and then: git rebase --continue git rebase --skip git rebase --abort","title":"Merge VS rebase"},{"location":"Git/3-Git-Intermediate-Techniques/#rebase-onto-other-branches","text":"git rebase --onto base upstream branch For example: git rebase --onto master camping expenses","title":"Rebase onto other branches"},{"location":"Git/3-Git-Intermediate-Techniques/#undo-rebase","text":"Undoing complex rebases may loses SHAs, commit messages, change sets, etc. git reset --hard ORIG_HEAD git rebase --onto camping_commitID master expenses","title":"Undo rebase"},{"location":"Git/3-Git-Intermediate-Techniques/#interactive-rebase","text":"git rebase -i master new_feature git rebase -i HEAD~3 with options: pick : use commit drop : remove commit reword : use commit, but edit the commit message edit : use commit, but stop for amending squash : use commit, but meld into previous commit fixup : like squash , but discard this commit's log message exec :","title":"Interactive rebase"},{"location":"Git/3-Git-Intermediate-Techniques/#pull-rebase","text":"Fetch from remote, and then rebase instead or merge git pull --rebase git pull -r git pull --rebase=preserve git pull --rebase=interactive","title":"Pull rebase"},{"location":"Git/3-Git-Intermediate-Techniques/#track-problems","text":"","title":"Track Problems"},{"location":"Git/3-Git-Intermediate-Techniques/#blame","text":"How wrote this code and when? - who I should blame? git blame filename.txt git blame -w filename.txt git blame -L 100,150 filename.txt git blame -L 100,+50 filename.txt git blame SHA filename.txt where -w ignore whitespace Add a global alias for blame as praise git config --global alias.praise blame","title":"Blame"},{"location":"Git/3-Git-Intermediate-Techniques/#annotate","text":"Similar to blame, with different output format git annotate filename.txt","title":"Annotate"},{"location":"Git/3-Git-Intermediate-Techniques/#bisect","text":"Find the commit that introduced a bug or regression. The last good revision and the first bad revision are marked git bisect start git bisect bad <treeish> git bisect good <treeish> git bisect reset","title":"Bisect"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/","text":"Building React and ASP.NET Core Applications \u00b6 Create new Application \u00b6 dotnet new react dotnet run Other Web Application Templates: dotnet new mvc webapp angular react reactredux webapi ... index.html \u00b6 All the React related files can be found inside the ClientApp folder. Inside the public folder, we can see the index.html file. This is the first page that is loaded when the application starts. < div id = \"root\" ></ div > This will be the only html file in the entire application since React is generally written in JavaScript index.js \u00b6 Inside the src folder, you can see the index.js file. This is the JavaScript file that corresponds to the index.html . const rootElement = document . getElementById ( 'root' ); ReactDOM . render ( < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter>, rootElement ); App.js \u00b6 The app component is the main component in React which acts as a container for all the other components. render () { return ( < Layout > < Route exact path = '/' component = { Home } /> < Route path = '/create' component = { Create } /> < Route path = '/trips' component = { Trips } /> < /Layout> ); } Web API \u00b6 We can create a folder as \"Data\" to put in all the data-related files such as models, services, etc. Then create: /Data/Models /Data/Services /Controllers Getting starting with React \u00b6 In React, the component is an individual item that can obtain other items. Extension file is .jsx Getting data into components using: props : html attribute context : data through many levels state : place to store data JSX lifecycle methods ( componentDidMount , etc.) You can only return one single parent module from a react component. This is why we always return a parents div in the render method: render () { return ( < div >< /div> ); } List View \u00b6 axios is required: npm install axios --save componentDidMount lifecycle method is used to get the data from the API endpoint once the UI has been loaded: import React , { Component } from 'react' ; import axios from 'axios' ; export class Trips extends Component { constructor ( props ){ super ( props ); this . state = { trips : [], loading : true } } //Once the component is mounted, it sends a request to get all data componentDidMount (){ this . populateTripsData (); } populateTripsData (){ axios . get ( \"api/Trips/GetTrips\" ). then ( result => { const response = result . data ; this . setState ({ trips : response , loading : false }); }) } renderAllTripsTable ( trips ){ return ( < table className = \"table table-striped\" > < thead > < tr > < th > Name < /th> < th > Description < /th> < th > Date started < /th> < th > Date completed < /th> < th > Action < /th> < /tr> < /thead> < tbody > { trips . map ( trip => ( < tr key = { trip . id } > < td > { trip . name } < /td> < td > { trip . description } < /td> < td > { new Date ( trip . dateStarted ). toISOString (). slice ( 0 , 10 )} < /td> < td > { trip . dateCompleted ? new Date ( trip . dateCompleted ). toISOString (). slice ( 0 , 10 ) : '-' } < /td> < td > - < /td> < /tr> )) } < /tbody> < /table> ); } render (){ let content = this . state . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( this . renderAllTripsTable ( this . state . trips ) ) return ( < div > < h1 > All trips < /h1> < p > Here you can see all trips < /p> { content } < /div> ); } } Create View \u00b6 import React , { Component } from 'react' ; import axios from 'axios' ; export class Create extends Component { constructor ( props ){ super ( props ); this . onChange = this . onChange . bind ( this ); this . onSubmit = this . onSubmit . bind ( this ); this . state = { name : '' , description : '' , dateStarted : null , dateCompleted : null } } onChange ( value , event ) { this . setState ({ [ event . target . name ] : value }); } onSubmit ( e ){ e . preventDefault (); let tripObject = { Id : Math . floor ( Math . random () * 1000 ), name : this . state . name , description : this . state . description , dateStarted : this . state . dateStarted , dateCompleted : this . state . dateCompleted } //post request and redirect to /trips component const { history } = this . props ; axios . post ( \"api/Trips/AddTrip\" , tripObject ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div className = \"trip-form\" > < h3 > Add new trip < /h3> < form onSubmit = { this . onSubmit } > < div className = \"form-group\" > < label > Trip name : < /label> < input type = \"text\" className = \"form-control\" value = { this . state . name } onChange = { this . onChange } /> < /div> < div className = \"form-group\" > < label > Trip description : < /label> < textarea type = \"text\" className = \"form-control\" value = { this . state . description } onChange = { this . onChange } /> < /div> < div className = \"row\" > < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of start : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateStarted } onChange = { this . onChange } /> < /div> < /div> < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of completion : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateCompleted } onChange = { this . onChange } /> < /div> < /div> < /div> < div className = \"form-group\" > < input type = \"submit\" value = \"Add trip\" className = \"btn btn-primary\" /> < /div> < /form> < /div> ) } } Edit View \u00b6 Trips.js onTripUpdate ( id ){ const { history } = this . props ; history . push ( '/update/' + id ); } ... < div className = \"form-group\" > < button onClick = {() => this . onTripUpdate ( trip . id )} className = \"btn btn-success\" > Update < /button> < /div> Apps.js render () { return ( < Layout > ... < Route path = '/update/:id' component = { Update } /> < /Layout> ); } Update.jsx import React , { Component } from 'react' ; import axios from 'axios' ; export class Update extends Component { constructor ( props ){ super ( props ); this . onChange = this . onChange . bind ( this ); this . onUpdateCancel = this . onUpdateCancel . bind ( this ); this . onSubmit = this . onSubmit . bind ( this ); this . state = { name : '' , description : '' , dateStarted : null , dateCompleted : null } } //Once the component is mounted, it sends a request to get data by id componentDidMount (){ const { id } = this . props . match . params ; axios . get ( \"api/Trips/SingleTrip/\" + id ). then ( trip => { const response = trip . data ; this . setState ({ name : response . name , description : response . description , dateStarted : new Date ( response . dateStarted ). toISOString (). slice ( 0 , 10 ), dateCompleted : response . dateCompleted ? new Date ( response . dateCompleted ). toISOString (). slice ( 0 , 10 ) : null }) }) } onChange ( value , event ) { this . setState ({ [ event . target . name ] : value }); } onUpdateCancel (){ //cancel and redirect to /trips component const { history } = this . props ; history . push ( '/trips' ); } onSubmit ( e ){ e . preventDefault (); let tripObject = { name : this . state . name , description : this . state . description , dateStarted : new Date ( this . state . dateStarted ). toISOString (), dateCompleted : this . state . dateCompleted ? new Date ( this . state . dateCompleted ). toISOString () : null } //PUT request and redirect to /trips component const { history } = this . props ; const { id } = this . props . match . params ; axios . put ( \"api/Trips/updateTrip/\" + id , tripObject ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div className = \"trip-form\" > < h3 > Add new trip < /h3> < form onSubmit = { this . onSubmit } > < div className = \"form-group\" > < label > Trip name : < /label> < input type = \"text\" className = \"form-control\" value = { this . state . name } onChange = { this . onChangeName } /> < /div> < div className = \"form-group\" > < label > Trip description : < /label> < textarea type = \"text\" className = \"form-control\" value = { this . state . description } onChange = { this . onChangeDescription } /> < /div> < div className = \"row\" > < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of start : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateStarted } onChange = { this . onChangeDateStarted } /> < /div> < /div> < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of completion : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateCompleted } onChange = { this . onChangeDateCompleted } /> < /div> < /div> < /div> < div className = \"form-group\" > < button onClick = { this . onUpdateCancel } className = \"btn btn-default\" > Cancel < /button> < button type = \"submit\" className = \"btn btn-success\" > Update < /button> < /div> < /form> < /div> ) } } Delete View \u00b6 Trips.js onTripDelete ( id ){ const { history } = this . props ; history . push ( '/delete/' + id ); } ... < div className = \"form-group\" > < button onClick = {() => this . onTripDelete ( trip . id )} className = \"btn btn-danger\" > Delete < /button> < /div> Apps.js render () { return ( < Layout > ... < Route path = '/delete/:id' component = { Delete } /> < /Layout> ); } Delete.jsx import React , { Component } from 'react' ; import axios from 'axios' ; export class Delete extends Component { constructor ( props ){ super ( props ); this . onCancel = this . onCancel . bind ( this ); this . onConfirmation = this . onConfirmation . bind ( this ); this . state = { name : \"\" , description : \"\" } } //Once the component is mounted, it sends a request to get data by id componentDidMount (){ const { id } = this . props . match . params ; axios . get ( \"api/Trips/SingleTrip/\" + id ). then ( trip => { const response = trip . data ; this . setState ({ name : response . name , description : response . description }) }) } onCancel ( e ){ //cancel and redirect to /trips component const { history } = this . props ; history . push ( '/trips' ); } onConfirmation ( e ){ const { id } = this . props . match . params ; const { history } = this . props ; axios . delete ( \"api/Trips/DeleteTrip/\" + id ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div style = {{ marginTop : 10 }} > < h2 > Delete trip confirmation < /h2> < div class = \"card\" > < div class = \"card-body\" > < h4 class = \"card-title\" > { this . state . name } < /h4> < p class = \"card-text\" > { this . state . description } < /p> < button onClick = { this . onCancel } class = \"btn btn-default\" > Cancel < /button> < button onClick = { this . onConfirmation } class = \"btn btn-danger\" > Confirm < /button> < /div> < /div> < /div> ) } } Handling errors \u00b6 Using axios.catch(error) import React , { Component } from 'react' ; import axios from 'axios' ; export class Trips extends Component { constructor ( props ){ super ( props ); this . onTripUpdate = this . onTripUpdate . bind ( this ); this . onTripDelete = this . onTripDelete . bind ( this ); this . state = { trips : [], loading : true , failed : false , error : '' //error message } } ... populateTripsData (){ axios . get ( \"api/Trips/GetTrips\" ). then ( result => { const response = result . data ; this . setState ({ trips : response , loading : false , failed : false , error : \"\" }); }). catch ( error => { this . setState ({ trips : [], loading : false , failed : true , error : \"Trips could not be loaded\" }); }); } ... render (){ let content = this . state . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( this . state . failed ? ( < div className = \"text-danger\" > < em > { this . state . error } < /em> < /div> ) : ( this . renderAllTripsTable ( this . state . trips )) ) return ( < div > < h1 > All trips < /h1> < p > Here you can see all trips < /p> { content } < /div> ); } } Redux \u00b6 Redux is a state magement tool for JS applications. The entire state of an application is stored in one central location: The stores store the whole state. The reducers return parts of this state The actions are predefined, user-triggered events that define how a state should change. Require: npm install redux react-redux react-thunk npm install react-thunk --save Actions \u00b6 Actions are payloads of information that send data from your application to your store .\\src\\actions\\tripsActions.js import axios from 'axios' ; export const GET_ALL_TRIPS_REQUEST = 'GET_ALL_TRIPS_REQUEST' ; export const GET_ALL_TRIPS_SUCCESS = 'GET_ALL_TRIPS_SUCCESS' ; export const GET_ALL_TRIPS_ERROR = 'GET_ALL_TRIPS_ERROR' ; const getTripsSuccess = payload => ({ type : GET_ALL_TRIPS_SUCCESS , payload }); const getTripsError = payload => ({ type : GET_ALL_TRIPS_ERROR , payload }); //export the action to be able to use it export const getAllTrips = () => dispatch => { dispatch ({ type : GET_ALL_TRIPS_REQUEST }); return axios . get ( 'api/Trips/GetTrips' ). then ( res => { const response = res . data ; dispatch ( getTripsSuccess ( response )); }). catch ( error => { dispatch ( getTripsError ( \"Something went wrong!\" )); return Promise . reject ({}); }) } Reducers \u00b6 Reducers specify how the application's state change .\\src\\reducers\\tripsReducers.js import { GET_ALL_TRIPS_REQUEST , GET_ALL_TRIPS_SUCCESS , GET_ALL_TRIPS_ERROR } from '../actions/tripActions' ; const INITIAL_STATE = { loading : false , hasError : false , error : null , data : [] } export default ( state = INITIAL_STATE , action ) => { switch ( action . type ){ case GET_ALL_TRIPS_REQUEST : return { ... state , loading : true }; case GET_ALL_TRIPS_SUCCESS : return { ... state , loading : false , hasError : false , data : action . payload }; case GET_ALL_TRIPS_ERROR : return { ... state , loading : false , hasError : true , error : action . payload }; default : return state ; } } .\\src\\reducers\\index.js import { combineReducers } from 'redux' ; import tripReducers from './tripReducers' ; const rootReducer = combineReducers ({ trips : tripReducers }); //export the root reducer to be able to use it export default rootReducer ; Store \u00b6 getSate() - access application state dispatch(action) - udpate application state subscribe(listener) - register and unregister listeners Only have a single store in a Redux application: .\\src\\store\\store.js import { createStore , applyMiddleware } from 'redux' import thunk from 'redux-thunk' ; import rootReducer from '../reducers' ; const configureStore = () => applyMiddleware ( thunk )( createStore )( rootReducer ); //export the configure store to be able to use it export default configureStore ; List View with redux \u00b6 We want to load all the trips using the redux instead of just calling them directly from our component to web API: .\\src\\index.js import { Provider } from 'react-redux' ; //required for redux import configureStore from './store/store' ; //required for redux ... const store = configureStore ({}); ... ReactDOM . render ( < Provider store = { store } > //using the store < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter> < /Provider>, rootElement ); ... .\\src\\Apps.js ... import Trips from './components/Trip/Trips' ; //instead of { Trips } ... .\\src\\components\\Trip\\Trips.jsx import { connect } from 'react-redux' ; import { getAllTrips } from '../../actions/tripActions' ; ... componentDidMount (){ this . props . getAllTrips (); //get trips from tripActions } componentDidUpdate ( prevProps ){ if ( prevProps . trips . data != this . props . trips . data ){ this . setState ({ trips : this . props . trips . data }); } } ... render (){ let content = this . props . trips . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( //build the table if there is one or more trips this . state . trips . length && this . renderAllTripsTable ( this . state . trips ) ); ... const mapStateToProps = ({ trips }) => ({ trips }); //Allow the Trips component to access data from store. export default connect ( mapStateToProps , { getAllTrips })( Trips ); Authentication using Auth0 \u00b6 Log in into https://auth0.com . signed in as rnietoe.eu.auth0.com Create Single Page Web Application and configure Auth0 from Settings Set Allowed Callback Urls equals https://localhost:5001 Set Allowed Web Origins equals https://localhost:5001 Set Allowed Logout Urls equals https://localhost:5001 Install dependencies: npm install @auth0-spa-js --save Install the Auth0 React wrapper ( react-auth0-spa.js ) in our src directory. Copy paste from documenation . Create/Update the NavMenu component (./src/components/NavMenu.js) import React , { Component } from 'react' ; import { Collapse , Container , Navbar , NavbarBrand , NavbarToggler , NavItem , NavLink } from 'reactstrap' ; import { Link } from 'react-router-dom' ; import { useAuth0 } from '../auth0-wrapper' ; //allow to use methods from the authentication wrapper import './NavMenu.css' ; //change class component to funcion component const NavMenu = () => { //define methods from the authentication wrapper const { isAuthenticated , loginWithRedirect , logout } = useAuth0 (); return ( < header > < Navbar className = \"navbar-expand-sm navbar-toggleable-sm ng-white border-bottom box-shadow mb-3\" light > < Container > < NavbarBrand tag = { Link } to = \"/\" > Trips < /NavbarBrand> { isAuthenticated ? ( < ul className = \"navbar-nav flex-grow\" > < NavItem > < NavLink tag = { Link } className = \"text-dark\" to = \"/create\" > Create < /NavLink> < /NavItem> < NavItem > < NavLink tag = { Link } className = \"text-dark\" to = \"/trips\" > Trips < /NavLink> < /NavItem> < NavItem > < button className = \"btn btn-danger\" onClick = {() => logout ()} > Log out < /button> < /NavItem> < /ul> ) : ( < ul className = \"navbar-nav flex-grow\" > < NavItem > < button className = \"btn btn-success\" onClick = {() => loginWithRedirect ()} > Log In < /button> < /NavItem> < /ul> )} < /Container> < /Navbar> < /header> ); } //allow to use the NavMenu export default NavMenu ; Update Index (.\\src\\index.js) ... import { Auth0Provider } from './auth0-wrapper' ; import config from './auth_config.json' //function that will route the user after the user logs in const onRedirectCallback = appState => { window . history . replaceState ( {}, document . title , appState && appState . targetUrl ? appState . targetUrl : window . location . pathname ); }; ReactDOM . render ( < Auth0Provider domain = { config . domain } client_id = { config . clientId } redirect_uri = { window . location . origin } onRedirectCallback = { onRedirectCallback } > < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter> < /Auth0Provider> , rootElement ); registerServiceWorker (); .\\src\\auth_config.json { \"domain\" : \"rnietoe.eu.auth0.com\" , \"clientId\" : \"0D2Ab46Xq5dBEueBDtX4Cg9vl8rsUyz5\" } .\\src\\components\\Layout.js ... import NavMenu from './components/NavMenu' ; //instead of { NavMenu } ...","title":"Building React and ASP.NET Core Applications"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#building-react-and-aspnet-core-applications","text":"","title":"Building React and ASP.NET Core Applications"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#create-new-application","text":"dotnet new react dotnet run Other Web Application Templates: dotnet new mvc webapp angular react reactredux webapi ...","title":"Create new Application"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#indexhtml","text":"All the React related files can be found inside the ClientApp folder. Inside the public folder, we can see the index.html file. This is the first page that is loaded when the application starts. < div id = \"root\" ></ div > This will be the only html file in the entire application since React is generally written in JavaScript","title":"index.html"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#indexjs","text":"Inside the src folder, you can see the index.js file. This is the JavaScript file that corresponds to the index.html . const rootElement = document . getElementById ( 'root' ); ReactDOM . render ( < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter>, rootElement );","title":"index.js"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#appjs","text":"The app component is the main component in React which acts as a container for all the other components. render () { return ( < Layout > < Route exact path = '/' component = { Home } /> < Route path = '/create' component = { Create } /> < Route path = '/trips' component = { Trips } /> < /Layout> ); }","title":"App.js"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#web-api","text":"We can create a folder as \"Data\" to put in all the data-related files such as models, services, etc. Then create: /Data/Models /Data/Services /Controllers","title":"Web API"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#getting-starting-with-react","text":"In React, the component is an individual item that can obtain other items. Extension file is .jsx Getting data into components using: props : html attribute context : data through many levels state : place to store data JSX lifecycle methods ( componentDidMount , etc.) You can only return one single parent module from a react component. This is why we always return a parents div in the render method: render () { return ( < div >< /div> ); }","title":"Getting starting with React"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#list-view","text":"axios is required: npm install axios --save componentDidMount lifecycle method is used to get the data from the API endpoint once the UI has been loaded: import React , { Component } from 'react' ; import axios from 'axios' ; export class Trips extends Component { constructor ( props ){ super ( props ); this . state = { trips : [], loading : true } } //Once the component is mounted, it sends a request to get all data componentDidMount (){ this . populateTripsData (); } populateTripsData (){ axios . get ( \"api/Trips/GetTrips\" ). then ( result => { const response = result . data ; this . setState ({ trips : response , loading : false }); }) } renderAllTripsTable ( trips ){ return ( < table className = \"table table-striped\" > < thead > < tr > < th > Name < /th> < th > Description < /th> < th > Date started < /th> < th > Date completed < /th> < th > Action < /th> < /tr> < /thead> < tbody > { trips . map ( trip => ( < tr key = { trip . id } > < td > { trip . name } < /td> < td > { trip . description } < /td> < td > { new Date ( trip . dateStarted ). toISOString (). slice ( 0 , 10 )} < /td> < td > { trip . dateCompleted ? new Date ( trip . dateCompleted ). toISOString (). slice ( 0 , 10 ) : '-' } < /td> < td > - < /td> < /tr> )) } < /tbody> < /table> ); } render (){ let content = this . state . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( this . renderAllTripsTable ( this . state . trips ) ) return ( < div > < h1 > All trips < /h1> < p > Here you can see all trips < /p> { content } < /div> ); } }","title":"List View"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#create-view","text":"import React , { Component } from 'react' ; import axios from 'axios' ; export class Create extends Component { constructor ( props ){ super ( props ); this . onChange = this . onChange . bind ( this ); this . onSubmit = this . onSubmit . bind ( this ); this . state = { name : '' , description : '' , dateStarted : null , dateCompleted : null } } onChange ( value , event ) { this . setState ({ [ event . target . name ] : value }); } onSubmit ( e ){ e . preventDefault (); let tripObject = { Id : Math . floor ( Math . random () * 1000 ), name : this . state . name , description : this . state . description , dateStarted : this . state . dateStarted , dateCompleted : this . state . dateCompleted } //post request and redirect to /trips component const { history } = this . props ; axios . post ( \"api/Trips/AddTrip\" , tripObject ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div className = \"trip-form\" > < h3 > Add new trip < /h3> < form onSubmit = { this . onSubmit } > < div className = \"form-group\" > < label > Trip name : < /label> < input type = \"text\" className = \"form-control\" value = { this . state . name } onChange = { this . onChange } /> < /div> < div className = \"form-group\" > < label > Trip description : < /label> < textarea type = \"text\" className = \"form-control\" value = { this . state . description } onChange = { this . onChange } /> < /div> < div className = \"row\" > < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of start : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateStarted } onChange = { this . onChange } /> < /div> < /div> < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of completion : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateCompleted } onChange = { this . onChange } /> < /div> < /div> < /div> < div className = \"form-group\" > < input type = \"submit\" value = \"Add trip\" className = \"btn btn-primary\" /> < /div> < /form> < /div> ) } }","title":"Create View"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#edit-view","text":"Trips.js onTripUpdate ( id ){ const { history } = this . props ; history . push ( '/update/' + id ); } ... < div className = \"form-group\" > < button onClick = {() => this . onTripUpdate ( trip . id )} className = \"btn btn-success\" > Update < /button> < /div> Apps.js render () { return ( < Layout > ... < Route path = '/update/:id' component = { Update } /> < /Layout> ); } Update.jsx import React , { Component } from 'react' ; import axios from 'axios' ; export class Update extends Component { constructor ( props ){ super ( props ); this . onChange = this . onChange . bind ( this ); this . onUpdateCancel = this . onUpdateCancel . bind ( this ); this . onSubmit = this . onSubmit . bind ( this ); this . state = { name : '' , description : '' , dateStarted : null , dateCompleted : null } } //Once the component is mounted, it sends a request to get data by id componentDidMount (){ const { id } = this . props . match . params ; axios . get ( \"api/Trips/SingleTrip/\" + id ). then ( trip => { const response = trip . data ; this . setState ({ name : response . name , description : response . description , dateStarted : new Date ( response . dateStarted ). toISOString (). slice ( 0 , 10 ), dateCompleted : response . dateCompleted ? new Date ( response . dateCompleted ). toISOString (). slice ( 0 , 10 ) : null }) }) } onChange ( value , event ) { this . setState ({ [ event . target . name ] : value }); } onUpdateCancel (){ //cancel and redirect to /trips component const { history } = this . props ; history . push ( '/trips' ); } onSubmit ( e ){ e . preventDefault (); let tripObject = { name : this . state . name , description : this . state . description , dateStarted : new Date ( this . state . dateStarted ). toISOString (), dateCompleted : this . state . dateCompleted ? new Date ( this . state . dateCompleted ). toISOString () : null } //PUT request and redirect to /trips component const { history } = this . props ; const { id } = this . props . match . params ; axios . put ( \"api/Trips/updateTrip/\" + id , tripObject ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div className = \"trip-form\" > < h3 > Add new trip < /h3> < form onSubmit = { this . onSubmit } > < div className = \"form-group\" > < label > Trip name : < /label> < input type = \"text\" className = \"form-control\" value = { this . state . name } onChange = { this . onChangeName } /> < /div> < div className = \"form-group\" > < label > Trip description : < /label> < textarea type = \"text\" className = \"form-control\" value = { this . state . description } onChange = { this . onChangeDescription } /> < /div> < div className = \"row\" > < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of start : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateStarted } onChange = { this . onChangeDateStarted } /> < /div> < /div> < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of completion : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateCompleted } onChange = { this . onChangeDateCompleted } /> < /div> < /div> < /div> < div className = \"form-group\" > < button onClick = { this . onUpdateCancel } className = \"btn btn-default\" > Cancel < /button> < button type = \"submit\" className = \"btn btn-success\" > Update < /button> < /div> < /form> < /div> ) } }","title":"Edit View"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#delete-view","text":"Trips.js onTripDelete ( id ){ const { history } = this . props ; history . push ( '/delete/' + id ); } ... < div className = \"form-group\" > < button onClick = {() => this . onTripDelete ( trip . id )} className = \"btn btn-danger\" > Delete < /button> < /div> Apps.js render () { return ( < Layout > ... < Route path = '/delete/:id' component = { Delete } /> < /Layout> ); } Delete.jsx import React , { Component } from 'react' ; import axios from 'axios' ; export class Delete extends Component { constructor ( props ){ super ( props ); this . onCancel = this . onCancel . bind ( this ); this . onConfirmation = this . onConfirmation . bind ( this ); this . state = { name : \"\" , description : \"\" } } //Once the component is mounted, it sends a request to get data by id componentDidMount (){ const { id } = this . props . match . params ; axios . get ( \"api/Trips/SingleTrip/\" + id ). then ( trip => { const response = trip . data ; this . setState ({ name : response . name , description : response . description }) }) } onCancel ( e ){ //cancel and redirect to /trips component const { history } = this . props ; history . push ( '/trips' ); } onConfirmation ( e ){ const { id } = this . props . match . params ; const { history } = this . props ; axios . delete ( \"api/Trips/DeleteTrip/\" + id ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div style = {{ marginTop : 10 }} > < h2 > Delete trip confirmation < /h2> < div class = \"card\" > < div class = \"card-body\" > < h4 class = \"card-title\" > { this . state . name } < /h4> < p class = \"card-text\" > { this . state . description } < /p> < button onClick = { this . onCancel } class = \"btn btn-default\" > Cancel < /button> < button onClick = { this . onConfirmation } class = \"btn btn-danger\" > Confirm < /button> < /div> < /div> < /div> ) } }","title":"Delete View"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#handling-errors","text":"Using axios.catch(error) import React , { Component } from 'react' ; import axios from 'axios' ; export class Trips extends Component { constructor ( props ){ super ( props ); this . onTripUpdate = this . onTripUpdate . bind ( this ); this . onTripDelete = this . onTripDelete . bind ( this ); this . state = { trips : [], loading : true , failed : false , error : '' //error message } } ... populateTripsData (){ axios . get ( \"api/Trips/GetTrips\" ). then ( result => { const response = result . data ; this . setState ({ trips : response , loading : false , failed : false , error : \"\" }); }). catch ( error => { this . setState ({ trips : [], loading : false , failed : true , error : \"Trips could not be loaded\" }); }); } ... render (){ let content = this . state . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( this . state . failed ? ( < div className = \"text-danger\" > < em > { this . state . error } < /em> < /div> ) : ( this . renderAllTripsTable ( this . state . trips )) ) return ( < div > < h1 > All trips < /h1> < p > Here you can see all trips < /p> { content } < /div> ); } }","title":"Handling errors"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#redux","text":"Redux is a state magement tool for JS applications. The entire state of an application is stored in one central location: The stores store the whole state. The reducers return parts of this state The actions are predefined, user-triggered events that define how a state should change. Require: npm install redux react-redux react-thunk npm install react-thunk --save","title":"Redux"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#actions","text":"Actions are payloads of information that send data from your application to your store .\\src\\actions\\tripsActions.js import axios from 'axios' ; export const GET_ALL_TRIPS_REQUEST = 'GET_ALL_TRIPS_REQUEST' ; export const GET_ALL_TRIPS_SUCCESS = 'GET_ALL_TRIPS_SUCCESS' ; export const GET_ALL_TRIPS_ERROR = 'GET_ALL_TRIPS_ERROR' ; const getTripsSuccess = payload => ({ type : GET_ALL_TRIPS_SUCCESS , payload }); const getTripsError = payload => ({ type : GET_ALL_TRIPS_ERROR , payload }); //export the action to be able to use it export const getAllTrips = () => dispatch => { dispatch ({ type : GET_ALL_TRIPS_REQUEST }); return axios . get ( 'api/Trips/GetTrips' ). then ( res => { const response = res . data ; dispatch ( getTripsSuccess ( response )); }). catch ( error => { dispatch ( getTripsError ( \"Something went wrong!\" )); return Promise . reject ({}); }) }","title":"Actions"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#reducers","text":"Reducers specify how the application's state change .\\src\\reducers\\tripsReducers.js import { GET_ALL_TRIPS_REQUEST , GET_ALL_TRIPS_SUCCESS , GET_ALL_TRIPS_ERROR } from '../actions/tripActions' ; const INITIAL_STATE = { loading : false , hasError : false , error : null , data : [] } export default ( state = INITIAL_STATE , action ) => { switch ( action . type ){ case GET_ALL_TRIPS_REQUEST : return { ... state , loading : true }; case GET_ALL_TRIPS_SUCCESS : return { ... state , loading : false , hasError : false , data : action . payload }; case GET_ALL_TRIPS_ERROR : return { ... state , loading : false , hasError : true , error : action . payload }; default : return state ; } } .\\src\\reducers\\index.js import { combineReducers } from 'redux' ; import tripReducers from './tripReducers' ; const rootReducer = combineReducers ({ trips : tripReducers }); //export the root reducer to be able to use it export default rootReducer ;","title":"Reducers"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#store","text":"getSate() - access application state dispatch(action) - udpate application state subscribe(listener) - register and unregister listeners Only have a single store in a Redux application: .\\src\\store\\store.js import { createStore , applyMiddleware } from 'redux' import thunk from 'redux-thunk' ; import rootReducer from '../reducers' ; const configureStore = () => applyMiddleware ( thunk )( createStore )( rootReducer ); //export the configure store to be able to use it export default configureStore ;","title":"Store"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#list-view-with-redux","text":"We want to load all the trips using the redux instead of just calling them directly from our component to web API: .\\src\\index.js import { Provider } from 'react-redux' ; //required for redux import configureStore from './store/store' ; //required for redux ... const store = configureStore ({}); ... ReactDOM . render ( < Provider store = { store } > //using the store < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter> < /Provider>, rootElement ); ... .\\src\\Apps.js ... import Trips from './components/Trip/Trips' ; //instead of { Trips } ... .\\src\\components\\Trip\\Trips.jsx import { connect } from 'react-redux' ; import { getAllTrips } from '../../actions/tripActions' ; ... componentDidMount (){ this . props . getAllTrips (); //get trips from tripActions } componentDidUpdate ( prevProps ){ if ( prevProps . trips . data != this . props . trips . data ){ this . setState ({ trips : this . props . trips . data }); } } ... render (){ let content = this . props . trips . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( //build the table if there is one or more trips this . state . trips . length && this . renderAllTripsTable ( this . state . trips ) ); ... const mapStateToProps = ({ trips }) => ({ trips }); //Allow the Trips component to access data from store. export default connect ( mapStateToProps , { getAllTrips })( Trips );","title":"List View with redux"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#authentication-using-auth0","text":"Log in into https://auth0.com . signed in as rnietoe.eu.auth0.com Create Single Page Web Application and configure Auth0 from Settings Set Allowed Callback Urls equals https://localhost:5001 Set Allowed Web Origins equals https://localhost:5001 Set Allowed Logout Urls equals https://localhost:5001 Install dependencies: npm install @auth0-spa-js --save Install the Auth0 React wrapper ( react-auth0-spa.js ) in our src directory. Copy paste from documenation . Create/Update the NavMenu component (./src/components/NavMenu.js) import React , { Component } from 'react' ; import { Collapse , Container , Navbar , NavbarBrand , NavbarToggler , NavItem , NavLink } from 'reactstrap' ; import { Link } from 'react-router-dom' ; import { useAuth0 } from '../auth0-wrapper' ; //allow to use methods from the authentication wrapper import './NavMenu.css' ; //change class component to funcion component const NavMenu = () => { //define methods from the authentication wrapper const { isAuthenticated , loginWithRedirect , logout } = useAuth0 (); return ( < header > < Navbar className = \"navbar-expand-sm navbar-toggleable-sm ng-white border-bottom box-shadow mb-3\" light > < Container > < NavbarBrand tag = { Link } to = \"/\" > Trips < /NavbarBrand> { isAuthenticated ? ( < ul className = \"navbar-nav flex-grow\" > < NavItem > < NavLink tag = { Link } className = \"text-dark\" to = \"/create\" > Create < /NavLink> < /NavItem> < NavItem > < NavLink tag = { Link } className = \"text-dark\" to = \"/trips\" > Trips < /NavLink> < /NavItem> < NavItem > < button className = \"btn btn-danger\" onClick = {() => logout ()} > Log out < /button> < /NavItem> < /ul> ) : ( < ul className = \"navbar-nav flex-grow\" > < NavItem > < button className = \"btn btn-success\" onClick = {() => loginWithRedirect ()} > Log In < /button> < /NavItem> < /ul> )} < /Container> < /Navbar> < /header> ); } //allow to use the NavMenu export default NavMenu ; Update Index (.\\src\\index.js) ... import { Auth0Provider } from './auth0-wrapper' ; import config from './auth_config.json' //function that will route the user after the user logs in const onRedirectCallback = appState => { window . history . replaceState ( {}, document . title , appState && appState . targetUrl ? appState . targetUrl : window . location . pathname ); }; ReactDOM . render ( < Auth0Provider domain = { config . domain } client_id = { config . clientId } redirect_uri = { window . location . origin } onRedirectCallback = { onRedirectCallback } > < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter> < /Auth0Provider> , rootElement ); registerServiceWorker (); .\\src\\auth_config.json { \"domain\" : \"rnietoe.eu.auth0.com\" , \"clientId\" : \"0D2Ab46Xq5dBEueBDtX4Cg9vl8rsUyz5\" } .\\src\\components\\Layout.js ... import NavMenu from './components/NavMenu' ; //instead of { NavMenu } ...","title":"Authentication using Auth0"},{"location":"React/Become-a-React-Developer/","text":"Index \u00b6 Learning React.js React.js Essential Training Learning Redux React.js: Building an Interface React: Creating and Hosting a Full-Stack Site React: Ecosystems","title":"Index"},{"location":"React/Become-a-React-Developer/#index","text":"Learning React.js React.js Essential Training Learning Redux React.js: Building an Interface React: Creating and Hosting a Full-Stack Site React: Ecosystems","title":"Index"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/","text":"Learning React.js \u00b6 Getting Started \u00b6 React is a JS library built at Facebook. Setup React Developer Tools using chrome: React Developer Tools extension . Edit extension setting with \"Allow access to file URLs\" enabled. React Element and JSX \u00b6 Create a react element using React.createElement : <!DOCTYPE html> < html > < head > < title > React Project </ title > < script src = \"https://unpkg.com/react@16.7.0/umd/react.development.js\" ></ script > < script src = \"https://unpkg.com/react-dom@16.7.0/umd/react-dom.development.js\" ></ script > </ head > < body > < div id = \"root\" ></ div > < script type = \"text/javascript\" > ReactDOM . render ( React . createElement ( \"div\" , //type null , //properties React . createElement ( //children \"h1\" , //type null , //properties \"Oh hello!\" //children ) ), document . getElementById ( \"root\" ) ); </ script > </ body > </ html > Using JSX (JS as XML). Require script type=\"text/babel\" and className instead of class: < script src = \"https://unpkg.com/@babel/standalone/babel.min.js\" ></ script > < script type = \"text/babel\" > let city = \"Madrid\" ; ReactDOM . render ( < h1 className = \"heading\" > Hello { city } < /h1>, document . getElementById ( \"root\" ) ); </ script > React Components \u00b6 All react components should be capitalized using Pascal Case Functions as const : const Hello = ( props ) => { return ( < div className = \"heading\" > < h1 > Welcome to { props . library } < /h1> < p > { props . message } < /p> < /div> ) }; ReactDOM . render ( < Hello library = \"React\" message = \"Enjoy!\" /> , document . getElementById ( \"root\" ) ); Properties: see below example using a functon with properties const Lake = ({ name , country }) => { return ( < div className = \"heading\" > < h1 > { name } < /h1> < p > { country } < /p> < /div> ) }; const App = () => ( < div > < Lake name = \"Lake Tahoe\" country = \"USA\" /> < Lake name = \"Angora Lake\" country = \"Angola\" /> < Lake name = \"Shirley Lake\" country = \"Australia\" /> < /div> ); ReactDOM . render ( < App /> , document . getElementById ( \"root\" ) ); Classes: always include the render method const Lake = ({ name }) => < h1 > { name } < /h1>; class App extends React . Component { render () { return ( < div > < Lake name = \"Lake Tahoe\" /> < Lake name = \"Angora Lake\" /> < Lake name = \"Shirley Lake\" /> < /div> ) } } See below example using a class with properties class Message extends React . Component { render () { return ( < div > < h1 style = {{ color : this . props . color }} > { this . props . msg } < /h1> < p > I will check back in { this . props . minutes } minutes < /p> < /div> ) } } ReactDOM . render ( < Message color = \"blue\" msg = \"how are you?\" minutes = { 5 } /> , document . getElementById ( 'root' ) ) State: When a component's State data changes, the render function will be called again to re-render the state change. class App extends React . Component { state = { logged : false } render () { return ( < div > The user is { this . state . logged ? \"logged in\" : \"logged out\" }. < /div> ) } } Render Components \u00b6 Render from a list: const lakeList = [ \"Echo Lake\" , \"Maud Lake\" , \"Cascade Lake\" ]; const App = ({ lakes }) => ( < ul > { lakes . map (( lake , i ) => < li key = { i } > { lake } < /li> )} < /ul> ) ReactDOM . render ( < App lakes = { lakeList } /> , document . getElementById ( \"root\" ) ); Render from a list of objects: const lakeList = [ { id : 1 , name : \"Echo Lake\" , trailhead : \"Echo Lake\" }, { id : 2 , name : \"Maud Lake\" , trailhead : \"Wright's Lake\" }, { id : 3 , name : \"Cascade Lake\" , trailhead : \"Bayview\" } ]; const App = ({ lakes }) => ( < ul > { lakes . map ( lake => < li key = { lake . id } > { lake . name } | Trailhead : { lake . trailhead } < /li> )} < /ul> ) ReactDOM . render ( < App lakes = { lakeList } /> , document . getElementById ( \"root\" ) ); React Events \u00b6 state = { loggedIn : false } logIn = () => this . setState ({ loggedIn : true }) logOut = () => this . setState ({ loggedIn : false }) render () { return ( < div > < button onClick = { this . logIn } > Log In < /button> < button onClick = { this . logOut } > Log Out < /button> < div > The user is { this . state . logged ? \"logged in\" : \"logged out\" }. < /div> < /div> ) } If Else \u00b6 const Lake = ({ name }) => < h1 > { name } < /h1>; const Resort = ({ name }) => < h1 > { name } < /h1>; const App = ({ summer }) => ( < div > // If summer then Lake Else Resort { summer ? < Lake name = \"Lake1\" /> : < Resort name = \"Resort2\" /> } < /div> ) ReactDOM . render ( < App summer = { false } /> , document . getElementById ( \"root\" ) ); Create React App \u00b6 Create React App requires nodejs npm -v node -v npm install -g create-react-app cd \"new_project_folder\" create-react-app \"project_name\" Next steps: Replace ReactDOM.render at /src/index.js with the one we have at index.html Copy const lakeList at /src/index.js from index.html Replace function App at /src/App.js with the one we have at index.html Install dependencies and start: cd lake-app npm install npm start Browse to http://localhost:3000/ to check the result. To create a production build: npm run build The build folder is ready to be deployed. You may serve it with a static server: npm install -g serve serve -s build","title":"Learning React.js"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#learning-reactjs","text":"","title":"Learning React.js"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#getting-started","text":"React is a JS library built at Facebook. Setup React Developer Tools using chrome: React Developer Tools extension . Edit extension setting with \"Allow access to file URLs\" enabled.","title":"Getting Started"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#react-element-and-jsx","text":"Create a react element using React.createElement : <!DOCTYPE html> < html > < head > < title > React Project </ title > < script src = \"https://unpkg.com/react@16.7.0/umd/react.development.js\" ></ script > < script src = \"https://unpkg.com/react-dom@16.7.0/umd/react-dom.development.js\" ></ script > </ head > < body > < div id = \"root\" ></ div > < script type = \"text/javascript\" > ReactDOM . render ( React . createElement ( \"div\" , //type null , //properties React . createElement ( //children \"h1\" , //type null , //properties \"Oh hello!\" //children ) ), document . getElementById ( \"root\" ) ); </ script > </ body > </ html > Using JSX (JS as XML). Require script type=\"text/babel\" and className instead of class: < script src = \"https://unpkg.com/@babel/standalone/babel.min.js\" ></ script > < script type = \"text/babel\" > let city = \"Madrid\" ; ReactDOM . render ( < h1 className = \"heading\" > Hello { city } < /h1>, document . getElementById ( \"root\" ) ); </ script >","title":"React Element and JSX"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#react-components","text":"All react components should be capitalized using Pascal Case Functions as const : const Hello = ( props ) => { return ( < div className = \"heading\" > < h1 > Welcome to { props . library } < /h1> < p > { props . message } < /p> < /div> ) }; ReactDOM . render ( < Hello library = \"React\" message = \"Enjoy!\" /> , document . getElementById ( \"root\" ) ); Properties: see below example using a functon with properties const Lake = ({ name , country }) => { return ( < div className = \"heading\" > < h1 > { name } < /h1> < p > { country } < /p> < /div> ) }; const App = () => ( < div > < Lake name = \"Lake Tahoe\" country = \"USA\" /> < Lake name = \"Angora Lake\" country = \"Angola\" /> < Lake name = \"Shirley Lake\" country = \"Australia\" /> < /div> ); ReactDOM . render ( < App /> , document . getElementById ( \"root\" ) ); Classes: always include the render method const Lake = ({ name }) => < h1 > { name } < /h1>; class App extends React . Component { render () { return ( < div > < Lake name = \"Lake Tahoe\" /> < Lake name = \"Angora Lake\" /> < Lake name = \"Shirley Lake\" /> < /div> ) } } See below example using a class with properties class Message extends React . Component { render () { return ( < div > < h1 style = {{ color : this . props . color }} > { this . props . msg } < /h1> < p > I will check back in { this . props . minutes } minutes < /p> < /div> ) } } ReactDOM . render ( < Message color = \"blue\" msg = \"how are you?\" minutes = { 5 } /> , document . getElementById ( 'root' ) ) State: When a component's State data changes, the render function will be called again to re-render the state change. class App extends React . Component { state = { logged : false } render () { return ( < div > The user is { this . state . logged ? \"logged in\" : \"logged out\" }. < /div> ) } }","title":"React Components"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#render-components","text":"Render from a list: const lakeList = [ \"Echo Lake\" , \"Maud Lake\" , \"Cascade Lake\" ]; const App = ({ lakes }) => ( < ul > { lakes . map (( lake , i ) => < li key = { i } > { lake } < /li> )} < /ul> ) ReactDOM . render ( < App lakes = { lakeList } /> , document . getElementById ( \"root\" ) ); Render from a list of objects: const lakeList = [ { id : 1 , name : \"Echo Lake\" , trailhead : \"Echo Lake\" }, { id : 2 , name : \"Maud Lake\" , trailhead : \"Wright's Lake\" }, { id : 3 , name : \"Cascade Lake\" , trailhead : \"Bayview\" } ]; const App = ({ lakes }) => ( < ul > { lakes . map ( lake => < li key = { lake . id } > { lake . name } | Trailhead : { lake . trailhead } < /li> )} < /ul> ) ReactDOM . render ( < App lakes = { lakeList } /> , document . getElementById ( \"root\" ) );","title":"Render Components"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#react-events","text":"state = { loggedIn : false } logIn = () => this . setState ({ loggedIn : true }) logOut = () => this . setState ({ loggedIn : false }) render () { return ( < div > < button onClick = { this . logIn } > Log In < /button> < button onClick = { this . logOut } > Log Out < /button> < div > The user is { this . state . logged ? \"logged in\" : \"logged out\" }. < /div> < /div> ) }","title":"React Events"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#if-else","text":"const Lake = ({ name }) => < h1 > { name } < /h1>; const Resort = ({ name }) => < h1 > { name } < /h1>; const App = ({ summer }) => ( < div > // If summer then Lake Else Resort { summer ? < Lake name = \"Lake1\" /> : < Resort name = \"Resort2\" /> } < /div> ) ReactDOM . render ( < App summer = { false } /> , document . getElementById ( \"root\" ) );","title":"If Else"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#create-react-app","text":"Create React App requires nodejs npm -v node -v npm install -g create-react-app cd \"new_project_folder\" create-react-app \"project_name\" Next steps: Replace ReactDOM.render at /src/index.js with the one we have at index.html Copy const lakeList at /src/index.js from index.html Replace function App at /src/App.js with the one we have at index.html Install dependencies and start: cd lake-app npm install npm start Browse to http://localhost:3000/ to check the result. To create a production build: npm run build The build folder is ready to be deployed. You may serve it with a static server: npm install -g serve serve -s build","title":"Create React App"},{"location":"React/Become-a-React-Developer/2-React.js-Essential-Training/","text":"React.js Essential Training \u00b6 Create react elements using JSX at /src/index.js import React from 'react' //import the react library import ReactDOM from 'react-dom' //needed to render React components var style = { backgroundColor : 'orange' , color : 'white' , fontFamily : 'Arial' } ReactDOM . render ( < div style = { style } > < h1 id = \"heading-element\" > Hello World !< /h1> < p > We 're glad you' re here !< /p> < /div>, document . getElementById ( 'root' ) //root div at /public/index.html ) Refactor React Components \u00b6 Create a class component with a custom method. See below example like SkiDayCounter: import React , { Component } from 'react' //replace React.Component by Component import { render } from 'react-dom' //replace ReactDOM.render by render let skiData = { total : 50 , powder : 20 , backcountry : 10 , goal : 100 } class SkiDayCounter extends Component { //using Component instead of React.Component calcGoalProgress = ( total , goal ) => { //custom method for this component return total / goal * 100 + '%' } render () { const { total , powder , backcountry , goal } = this . props return ( < section > < div >< p > Total Days : { total } < /p></div> < div >< p > Powder Days : { powder } < /p></div> < div >< p > Backcountry Days : { backcountry } < /p></div> < div >< p > Goal Progress : { this . calcGoalProgress ( total , goal )} < /p></div> < /section> ) } } render ( //using render instead of ReactDOM.render < SkiDayCounter total = { skiData . total } powder = { skiData . powder } backcountry = { skiData . backcountry } goal = { skiData . goal } /> , document . getElementById ( 'root' ) ) Create a function component with custom functions const calcGoalProgress = ( total , goal ) => { return total / goal * 100 + '%' } const SkiDayCounter = ({ total , powder , backcountry , goal }) => { return ( < section > < div > < p > Total Days : { total } < /p> < /div> < div > < p > Powder Days : { powder } < /p> < /div> < div > < p > Backcountry Days : { backcountry } < /p> < /div> < div > < p > Goal Progress : { calcGoalProgress ( total , goal )} < /p> < /div> < /section> ) } Props and State \u00b6 Compose components sample: import React from 'react' import { render } from 'react-dom' const Book = ({ title , author , pages }) => { return ( < section > < h2 > { title } < /h2> < p > by : { author } < /p> < p > Pages : { pages } pages < /p> < /section> ) } const Library = () => { return ( < div > < Book title = \"The Sun Also Rises\" author = \"Ernest Hemingway\" pages = { 260 } /> < Book title = \"White Teeth\" author = \"Zadie Smith\" pages = { 480 } /> < Book title = \"Cat's Cradle\" author = \"Kurt Vonnegut\" pages = { 304 } /> < /div> ) } render ( < Library /> , document . getElementById ( 'root' ) ) Refactor components sample let bookList = [ { \"title\" : \"Hunger\" , \"author\" : \"Roxane Gay\" , \"pages\" : 320 }, { \"title\" : \"The Sun Also Rises\" , \"author\" : \"Ernest Hemingway\" , \"pages\" : 260 }, { \"title\" : \"White Teeth\" , \"author\" : \"Zadie Smith\" , \"pages\" : 480 }, { \"title\" : \"Cat's Cradle\" , \"author\" : \"Kurt Vonnegut\" , \"pages\" : 304 } ] const Book = ({ title , author , pages }) => { return ( < section > < h2 > { title } < /h2> < p > by : { author } < /p> < p > Pages : { pages } pages < /p> < /section> ) } const Library = ({ books }) => { return ( < div > { books . map ( ( book , i ) => < Book key = { i } title = { book . title } author = { book . author } pages = { book . pages } /> )} < /div> ) } render ( < Library books = { bookList } /> , document . getElementById ( 'root' ) )","title":"React.js Essential Training"},{"location":"React/Become-a-React-Developer/2-React.js-Essential-Training/#reactjs-essential-training","text":"Create react elements using JSX at /src/index.js import React from 'react' //import the react library import ReactDOM from 'react-dom' //needed to render React components var style = { backgroundColor : 'orange' , color : 'white' , fontFamily : 'Arial' } ReactDOM . render ( < div style = { style } > < h1 id = \"heading-element\" > Hello World !< /h1> < p > We 're glad you' re here !< /p> < /div>, document . getElementById ( 'root' ) //root div at /public/index.html )","title":"React.js Essential Training"},{"location":"React/Become-a-React-Developer/2-React.js-Essential-Training/#refactor-react-components","text":"Create a class component with a custom method. See below example like SkiDayCounter: import React , { Component } from 'react' //replace React.Component by Component import { render } from 'react-dom' //replace ReactDOM.render by render let skiData = { total : 50 , powder : 20 , backcountry : 10 , goal : 100 } class SkiDayCounter extends Component { //using Component instead of React.Component calcGoalProgress = ( total , goal ) => { //custom method for this component return total / goal * 100 + '%' } render () { const { total , powder , backcountry , goal } = this . props return ( < section > < div >< p > Total Days : { total } < /p></div> < div >< p > Powder Days : { powder } < /p></div> < div >< p > Backcountry Days : { backcountry } < /p></div> < div >< p > Goal Progress : { this . calcGoalProgress ( total , goal )} < /p></div> < /section> ) } } render ( //using render instead of ReactDOM.render < SkiDayCounter total = { skiData . total } powder = { skiData . powder } backcountry = { skiData . backcountry } goal = { skiData . goal } /> , document . getElementById ( 'root' ) ) Create a function component with custom functions const calcGoalProgress = ( total , goal ) => { return total / goal * 100 + '%' } const SkiDayCounter = ({ total , powder , backcountry , goal }) => { return ( < section > < div > < p > Total Days : { total } < /p> < /div> < div > < p > Powder Days : { powder } < /p> < /div> < div > < p > Backcountry Days : { backcountry } < /p> < /div> < div > < p > Goal Progress : { calcGoalProgress ( total , goal )} < /p> < /div> < /section> ) }","title":"Refactor React Components"},{"location":"React/Become-a-React-Developer/2-React.js-Essential-Training/#props-and-state","text":"Compose components sample: import React from 'react' import { render } from 'react-dom' const Book = ({ title , author , pages }) => { return ( < section > < h2 > { title } < /h2> < p > by : { author } < /p> < p > Pages : { pages } pages < /p> < /section> ) } const Library = () => { return ( < div > < Book title = \"The Sun Also Rises\" author = \"Ernest Hemingway\" pages = { 260 } /> < Book title = \"White Teeth\" author = \"Zadie Smith\" pages = { 480 } /> < Book title = \"Cat's Cradle\" author = \"Kurt Vonnegut\" pages = { 304 } /> < /div> ) } render ( < Library /> , document . getElementById ( 'root' ) ) Refactor components sample let bookList = [ { \"title\" : \"Hunger\" , \"author\" : \"Roxane Gay\" , \"pages\" : 320 }, { \"title\" : \"The Sun Also Rises\" , \"author\" : \"Ernest Hemingway\" , \"pages\" : 260 }, { \"title\" : \"White Teeth\" , \"author\" : \"Zadie Smith\" , \"pages\" : 480 }, { \"title\" : \"Cat's Cradle\" , \"author\" : \"Kurt Vonnegut\" , \"pages\" : 304 } ] const Book = ({ title , author , pages }) => { return ( < section > < h2 > { title } < /h2> < p > by : { author } < /p> < p > Pages : { pages } pages < /p> < /section> ) } const Library = ({ books }) => { return ( < div > { books . map ( ( book , i ) => < Book key = { i } title = { book . title } author = { book . author } pages = { book . pages } /> )} < /div> ) } render ( < Library books = { bookList } /> , document . getElementById ( 'root' ) )","title":"Props and State"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/","text":"Web Security: OAuth and OpenID Connect \u00b6 https://www.linkedin.com/learning/web-security-oauth-and-openid-connect?u=1009514 OAuth 2.0 Servers Map of OAuth 2.0 Specs Aaron Parecki's book OAuth 2.0 Simplified What is OAuth? \u00b6 Some OAuth Servers: https://oauth2.thephpleague.com https://developers.google.com/oauthplayground https://openidconnect.net https://developer.okta.com A token introspection tool: https://www.jsonwebtoken.io flow = grant type scopes = permissions AuthZ with: API keys Id & Secret OAuth OpenID Connect (OIDC) is an OAuth 2.0 identity extension \u2013 only for users. Core Terminology \u00b6 Main OAuth extensions /authorize : grant permission to the resource. Could return an authZ code or an access token /token : trade an authZ code or refresh token for an access token /revoke : deactivate (invalidate) a token. Valid for access or refresh tokens /instrospect : learn more about the token /register : create new OAuth clients /userinfo : retrieve profile information about the authenticated user /.well-known/openid-configuration : which endpoints are support by our OAuth server Main OAuth tokens Access token: gives the client application access to the protected resource, usually the API Refresh token: used to request a new access token when this expires ID Token (included with OIDC): user profile information JWT (Json Web Token) iss : the issuer of the token, an entity we trust sub : the subject (user) of the token aud : the audience which should consume the token exp : expiration time of the token Claims: a key/value pair within the token that gives the client application information Client Credentials: AuthZ for microservices \u00b6 Private clients only where secrets are in backend code; not appropiate for web pages or mobiles apps Must use secure communications TLS no user relationship validate access token against the /instrospect endpoint expired token has to be rejected Implicit or Hybrid: AuthZ for mobile devices \u00b6 Attach the user to the process cookie/session storage AppAuth and Passport for Android or iOS Must use secure communications TLS use of redirect_uri Authorization Code \u00b6 get an Auth Code instead of an access token. The third-party application never sees our credentials. The end user never sees the access token. Resource Owner Password Flow \u00b6 recommended only for updating legacy systems danger! credentials are managed by the application itself Server side implementations \u00b6","title":"Web Security: OAuth and OpenID Connect"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#web-security-oauth-and-openid-connect","text":"https://www.linkedin.com/learning/web-security-oauth-and-openid-connect?u=1009514 OAuth 2.0 Servers Map of OAuth 2.0 Specs Aaron Parecki's book OAuth 2.0 Simplified","title":"Web Security: OAuth and OpenID Connect"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#what-is-oauth","text":"Some OAuth Servers: https://oauth2.thephpleague.com https://developers.google.com/oauthplayground https://openidconnect.net https://developer.okta.com A token introspection tool: https://www.jsonwebtoken.io flow = grant type scopes = permissions AuthZ with: API keys Id & Secret OAuth OpenID Connect (OIDC) is an OAuth 2.0 identity extension \u2013 only for users.","title":"What is OAuth?"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#core-terminology","text":"Main OAuth extensions /authorize : grant permission to the resource. Could return an authZ code or an access token /token : trade an authZ code or refresh token for an access token /revoke : deactivate (invalidate) a token. Valid for access or refresh tokens /instrospect : learn more about the token /register : create new OAuth clients /userinfo : retrieve profile information about the authenticated user /.well-known/openid-configuration : which endpoints are support by our OAuth server Main OAuth tokens Access token: gives the client application access to the protected resource, usually the API Refresh token: used to request a new access token when this expires ID Token (included with OIDC): user profile information JWT (Json Web Token) iss : the issuer of the token, an entity we trust sub : the subject (user) of the token aud : the audience which should consume the token exp : expiration time of the token Claims: a key/value pair within the token that gives the client application information","title":"Core Terminology"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#client-credentials-authz-for-microservices","text":"Private clients only where secrets are in backend code; not appropiate for web pages or mobiles apps Must use secure communications TLS no user relationship validate access token against the /instrospect endpoint expired token has to be rejected","title":"Client Credentials: AuthZ for microservices"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#implicit-or-hybrid-authz-for-mobile-devices","text":"Attach the user to the process cookie/session storage AppAuth and Passport for Android or iOS Must use secure communications TLS use of redirect_uri","title":"Implicit or Hybrid: AuthZ for mobile devices"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#authorization-code","text":"get an Auth Code instead of an access token. The third-party application never sees our credentials. The end user never sees the access token.","title":"Authorization Code"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#resource-owner-password-flow","text":"recommended only for updating legacy systems danger! credentials are managed by the application itself","title":"Resource Owner Password Flow"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#server-side-implementations","text":"","title":"Server side implementations"}]}