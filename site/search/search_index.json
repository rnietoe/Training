{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Training index \u00b6 Security \u00b6 Web Security: OAuth And OpenID Connect AWS \u00b6 AWS Solutions Architect Associate Git \u00b6 Git Essential Training: The Basics Git: Branches, Merges, and Remotes Git Intermediate Techniques React \u00b6 Building React and ASP.NET Core Applications Become a React Developer","title":"Training index"},{"location":"#training-index","text":"","title":"Training index"},{"location":"#security","text":"Web Security: OAuth And OpenID Connect","title":"Security"},{"location":"#aws","text":"AWS Solutions Architect Associate","title":"AWS"},{"location":"#git","text":"Git Essential Training: The Basics Git: Branches, Merges, and Remotes Git Intermediate Techniques","title":"Git"},{"location":"#react","text":"Building React and ASP.NET Core Applications Become a React Developer","title":"React"},{"location":"AWS/00-AWS-Solutions-Architect-Associate/","text":"AWS Architect \u00b6 AWS Well-Architected Framework : Reliability Backup and restore (RPO in hours, RTO in 24 hours or less): Back up your data and applications using point-in-time backups into the DR Region. Restore this data when necessary to recover from a disaster. Pilot light (RPO in minutes, RTO in hours): Replicate your data from one region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup such as databases and object storage are always on. Other elements such as application servers are loaded with application code and configurations, but are switched off and are only used during testing or when Disaster Recovery failover is invoked. Warm standby (RPO in seconds, RTO in minutes): Maintain a scaled-down but fully functional version of your workload always running in the DR Region. Business-critical systems are fully duplicated and are always on, but with a scaled down fleet. When the time comes for recovery, the system is scaled up quickly to handle the production load. The more scaled-up the Warm Standby is, the lower RTO and control plane reliance will be. When scaled up to full scale this is known as a Hot Standby . Multi-region (multi-site) active-active (RPO near zero, RTO potentially zero): Your workload is deployed to, and actively serving traffic from, multiple AWS Regions Performance Efficiency Security Cost Optimization Operational Excellence https://rnietoe.signin.aws.amazon.com/console rnietoe@gmail.com - AWS account (root account) rnietoe/training rnietoe/paying Google Authenticator for Android - MFA (Multi-Factor Authentication) Book your exam here Introduction Storage Network Compute Security Management CloudFormation HA Architecture Databases Applications Media ACloud.Guru Final Practice Exam \u00b6 https://aws.amazon.com/certification/certification-prep/ \u2022 Step 1: Take an AWS Training Class \u2022 Step 2: Review the Exam Guide and Sample Questions \u2022 Step 3: Practice with Self-Placed Labs and an Exam Prep Quest \u2022 Step 4: Study AWS Whitepapers \u2022 Step 5: Review AWS FAQs \u2022 Step 6: Take an Exam Prep Workshop \u2022 Step 7: Take a Practice Exam \u2022 Step 8: Schedule Your Exam and Get Certified https://www.qwiklabs.com/ AWS Documentation \u00b6 https://docs.aws.amazon.com/index.html?nc2=h_ql_doc_do https://aws.amazon.com/faqs/ https://aws.amazon.com/whitepapers/?whitepapers-main.sort-by=item.additionalFields.sortDate&whitepapers-main.sort-order=desc https://www.youtube.com/c/amazonwebservices/videos https://aws.amazon.com/architecture/ https://aws.amazon.com/new/ https://www.meetup.com/es/topics/amazon-web-services/ AWS Certified Solutions Architect Associate Practice Exams AWS Exam \u00b6 https://aws.amazon.com/certification/faqs/ https://aws.amazon.com/certification/certified-solutions-architect-associate/ AWS Cloud Practitioner Essentials (Second Edition) (Spanish) https://www.aws.training/Details/Curriculum?id=46152","title":"AWS Architect"},{"location":"AWS/00-AWS-Solutions-Architect-Associate/#aws-architect","text":"AWS Well-Architected Framework : Reliability Backup and restore (RPO in hours, RTO in 24 hours or less): Back up your data and applications using point-in-time backups into the DR Region. Restore this data when necessary to recover from a disaster. Pilot light (RPO in minutes, RTO in hours): Replicate your data from one region to another and provision a copy of your core workload infrastructure. Resources required to support data replication and backup such as databases and object storage are always on. Other elements such as application servers are loaded with application code and configurations, but are switched off and are only used during testing or when Disaster Recovery failover is invoked. Warm standby (RPO in seconds, RTO in minutes): Maintain a scaled-down but fully functional version of your workload always running in the DR Region. Business-critical systems are fully duplicated and are always on, but with a scaled down fleet. When the time comes for recovery, the system is scaled up quickly to handle the production load. The more scaled-up the Warm Standby is, the lower RTO and control plane reliance will be. When scaled up to full scale this is known as a Hot Standby . Multi-region (multi-site) active-active (RPO near zero, RTO potentially zero): Your workload is deployed to, and actively serving traffic from, multiple AWS Regions Performance Efficiency Security Cost Optimization Operational Excellence https://rnietoe.signin.aws.amazon.com/console rnietoe@gmail.com - AWS account (root account) rnietoe/training rnietoe/paying Google Authenticator for Android - MFA (Multi-Factor Authentication) Book your exam here Introduction Storage Network Compute Security Management CloudFormation HA Architecture Databases Applications Media","title":"AWS Architect"},{"location":"AWS/00-AWS-Solutions-Architect-Associate/#acloudguru-final-practice-exam","text":"https://aws.amazon.com/certification/certification-prep/ \u2022 Step 1: Take an AWS Training Class \u2022 Step 2: Review the Exam Guide and Sample Questions \u2022 Step 3: Practice with Self-Placed Labs and an Exam Prep Quest \u2022 Step 4: Study AWS Whitepapers \u2022 Step 5: Review AWS FAQs \u2022 Step 6: Take an Exam Prep Workshop \u2022 Step 7: Take a Practice Exam \u2022 Step 8: Schedule Your Exam and Get Certified https://www.qwiklabs.com/","title":"ACloud.Guru Final Practice Exam"},{"location":"AWS/00-AWS-Solutions-Architect-Associate/#aws-documentation","text":"https://docs.aws.amazon.com/index.html?nc2=h_ql_doc_do https://aws.amazon.com/faqs/ https://aws.amazon.com/whitepapers/?whitepapers-main.sort-by=item.additionalFields.sortDate&whitepapers-main.sort-order=desc https://www.youtube.com/c/amazonwebservices/videos https://aws.amazon.com/architecture/ https://aws.amazon.com/new/ https://www.meetup.com/es/topics/amazon-web-services/ AWS Certified Solutions Architect Associate Practice Exams","title":"AWS Documentation"},{"location":"AWS/00-AWS-Solutions-Architect-Associate/#aws-exam","text":"https://aws.amazon.com/certification/faqs/ https://aws.amazon.com/certification/certified-solutions-architect-associate/ AWS Cloud Practitioner Essentials (Second Edition) (Spanish) https://www.aws.training/Details/Curriculum?id=46152","title":"AWS Exam"},{"location":"AWS/01-Introduction/","text":"1. Introduction \u00b6 IaaS : Infrastructure as a Service: every server ( EC2 , VPC , RDS , S3 ) is in the cloud, but platform and software run on others' servers PaaS : Platform as a Service, applications, such as web hosting, Elastic Beanstalk or Lightsail SaaS : Software as a Service, you use the software developed by others from the cloud, such as Gmail FaaS : Function as a Service ( Lambda ) DaaS : Desktop as a Service such as Windows or Linux, provide by WorkSpaces . Three different ways to access AWS: Programmatic, using the aws CLI (Command Line Interface) Using the Console ( https://rnietoe.signin.aws.amazon.com/console ) Using SDK The three types of cloud deployments are: Public Hybrid On-premise Traditional Computing VS Cloud Computing Build your systems to be scalable, use disposable resources, reduce infrastructure to code, and assume EVERYTHING will fail sooner or later. resiliency: capacity to recover quickly from difficulties Scale Up / Vertical Scaling: increase RAM, CPU Scale Out / Horizontal Scaling: adding EC2 isntances behind a ELB Scale In : removing instances based on oldest instances or configuration or close to the next billing hour: Scale Down . decrease the class of EC2 or RDS to a lower power Global services (for every region): IAM Route53 CloudFront SNS (Simple Notify Service) SES (Simple Email Service) Regional Services with global view S3 Caching services: CloudFront API Gateway ElastiCache DynamoDB Accelerator (DAX) High-level AWS service used on premises: DMS (Database Migrations Service) is the best choice for conventional database migrations to AWS. homogenous: sample of Oracle to Oracle heterogenous with SCT (Schema Conversion Tool): sample of SQL Server to Aurora SMS (Server Migration Service) Application Discovery Service: collect information about their on premises data centers. Data is available in the AWS Migration Hub AWS DataSync is used to move large amounts of data from on-premise to AWS S3, EFS, FSx, etc. AWS Import/Export transfer large amounts of data from physical storage devices into AWS. You mail your portable storage devices to AWS, and AWS Import/Export transfers data directly off of your storage devices using Amazon's high-speed internal network. Download Amazon Linux 2 as an ISO AWS Service used on premise: AWS Snow Family: Snowball - to upload 50TB (200$) or 80TB (250$) to AWS in a week instead of three months. This allow S3 imports/exports Snowball Edge - Local compute and storage only till 100TB Snowmobile - a track with 100PB First 10 days are free. 15$ per day later Data transfer to S3 is free. Data transfer out is charged CodeDeploy - include applications Opsworks - include applications IoT greengrass Overview \u00b6 Availability Zone (AZ) is one or more data centers. AZs are randomized by AWS (us-east-1 can be different AZ for different accounts). Region is a distinct location within a geographical area with 2 or more AZ, designed to provide high availability to a specific geography. Choosen by law, latency and AWS Services US East (N. Virginia) us-east-1 was the first region and all new services are deployed here first Edge Locations (edge areas) are Amazon's CDN (Content Delivery Network) end points. Objects are cached for 48 hours by default ( TTL - Time To Live). This not just read only. Types: Web Distribution (websites) RTMD (media streaming) - Discontinued support by CloudFront on December 31, 2020\" Number of Edge Locations > Number of AZs > Number of Regions CloudFront \u00b6 AWS CloudFront distribution is a collection of Edge Locations. CloudFront content is cached in Edge Locations. This allows you to distribute content using a worldwide network of edge locations that provide low latency and high data transfer speeds. From AwS CloudFront select Create Distribution The CloudFront origin can be an S3 bucket, an EC2 instance, an ELB or Route53. We can restrict viewer access using signed URLs for individual files or signed cookies for multiple files. Netflix or CloudGuru samples. OAI - Origin Access Identity If the origin is EC2 then use CloudFront Signed url If the origin is S3 then use S3 signed url We can create invalidations to remove origin objets on the edge location To delete a CloudFront distribution, you have to disable it first. This process takes 15 minutes Pricing depends on: traffic distribution requests data transfer out We will be charged when deleting cached data from an edge location Resource Groups and Tag Editor \u00b6 Tags allow to find AWS resources in a selected region, but it can not directly managing those resources. This can make it easier to search for and filter resources by purpose, owner, environment, or other criteria. Resources groups allow to execute operations from AWS Systems Manager to different resources (such as a EC2 fleet) based on resources groups AWS Solutions \u00b6 AWS Solutions Implementations","title":"1. Introduction"},{"location":"AWS/01-Introduction/#1-introduction","text":"IaaS : Infrastructure as a Service: every server ( EC2 , VPC , RDS , S3 ) is in the cloud, but platform and software run on others' servers PaaS : Platform as a Service, applications, such as web hosting, Elastic Beanstalk or Lightsail SaaS : Software as a Service, you use the software developed by others from the cloud, such as Gmail FaaS : Function as a Service ( Lambda ) DaaS : Desktop as a Service such as Windows or Linux, provide by WorkSpaces . Three different ways to access AWS: Programmatic, using the aws CLI (Command Line Interface) Using the Console ( https://rnietoe.signin.aws.amazon.com/console ) Using SDK The three types of cloud deployments are: Public Hybrid On-premise Traditional Computing VS Cloud Computing Build your systems to be scalable, use disposable resources, reduce infrastructure to code, and assume EVERYTHING will fail sooner or later. resiliency: capacity to recover quickly from difficulties Scale Up / Vertical Scaling: increase RAM, CPU Scale Out / Horizontal Scaling: adding EC2 isntances behind a ELB Scale In : removing instances based on oldest instances or configuration or close to the next billing hour: Scale Down . decrease the class of EC2 or RDS to a lower power Global services (for every region): IAM Route53 CloudFront SNS (Simple Notify Service) SES (Simple Email Service) Regional Services with global view S3 Caching services: CloudFront API Gateway ElastiCache DynamoDB Accelerator (DAX) High-level AWS service used on premises: DMS (Database Migrations Service) is the best choice for conventional database migrations to AWS. homogenous: sample of Oracle to Oracle heterogenous with SCT (Schema Conversion Tool): sample of SQL Server to Aurora SMS (Server Migration Service) Application Discovery Service: collect information about their on premises data centers. Data is available in the AWS Migration Hub AWS DataSync is used to move large amounts of data from on-premise to AWS S3, EFS, FSx, etc. AWS Import/Export transfer large amounts of data from physical storage devices into AWS. You mail your portable storage devices to AWS, and AWS Import/Export transfers data directly off of your storage devices using Amazon's high-speed internal network. Download Amazon Linux 2 as an ISO AWS Service used on premise: AWS Snow Family: Snowball - to upload 50TB (200$) or 80TB (250$) to AWS in a week instead of three months. This allow S3 imports/exports Snowball Edge - Local compute and storage only till 100TB Snowmobile - a track with 100PB First 10 days are free. 15$ per day later Data transfer to S3 is free. Data transfer out is charged CodeDeploy - include applications Opsworks - include applications IoT greengrass","title":"1. Introduction"},{"location":"AWS/01-Introduction/#overview","text":"Availability Zone (AZ) is one or more data centers. AZs are randomized by AWS (us-east-1 can be different AZ for different accounts). Region is a distinct location within a geographical area with 2 or more AZ, designed to provide high availability to a specific geography. Choosen by law, latency and AWS Services US East (N. Virginia) us-east-1 was the first region and all new services are deployed here first Edge Locations (edge areas) are Amazon's CDN (Content Delivery Network) end points. Objects are cached for 48 hours by default ( TTL - Time To Live). This not just read only. Types: Web Distribution (websites) RTMD (media streaming) - Discontinued support by CloudFront on December 31, 2020\" Number of Edge Locations > Number of AZs > Number of Regions","title":"Overview"},{"location":"AWS/01-Introduction/#cloudfront","text":"AWS CloudFront distribution is a collection of Edge Locations. CloudFront content is cached in Edge Locations. This allows you to distribute content using a worldwide network of edge locations that provide low latency and high data transfer speeds. From AwS CloudFront select Create Distribution The CloudFront origin can be an S3 bucket, an EC2 instance, an ELB or Route53. We can restrict viewer access using signed URLs for individual files or signed cookies for multiple files. Netflix or CloudGuru samples. OAI - Origin Access Identity If the origin is EC2 then use CloudFront Signed url If the origin is S3 then use S3 signed url We can create invalidations to remove origin objets on the edge location To delete a CloudFront distribution, you have to disable it first. This process takes 15 minutes Pricing depends on: traffic distribution requests data transfer out We will be charged when deleting cached data from an edge location","title":"CloudFront"},{"location":"AWS/01-Introduction/#resource-groups-and-tag-editor","text":"Tags allow to find AWS resources in a selected region, but it can not directly managing those resources. This can make it easier to search for and filter resources by purpose, owner, environment, or other criteria. Resources groups allow to execute operations from AWS Systems Manager to different resources (such as a EC2 fleet) based on resources groups","title":"Resource Groups and Tag Editor"},{"location":"AWS/01-Introduction/#aws-solutions","text":"AWS Solutions Implementations","title":"AWS Solutions"},{"location":"AWS/02-Storage/","text":"2. Storage \u00b6 S3 \u00b6 AWS S3 (Simple Storage Service) is object-based for the safe storage of flat files such as text files, videos, pictures and any other flat file from 0 to 5 tb. The object has a key (filename), value (data), versionID, metadata, encryption, and security by ACL (Access Control Lists), torrent and Bucket Policies Objects stored in S3 (no One Zone) are stored across at least three AZs Buckets are folders/containers for everything that you store in S3. S3 bucket names are global , and must be unique, but they are create in a region. Universal namespaces are like https://s3-region_name.amazonaws.com/bucket_name. The response code is http 200 when file upload succeeds. Contact AWS to create more than 100 buckets minimum object size is 0 kb an object can have up to 10 tags Both Virtual-host-Style (such as: https://bucket-name.s3.Region.amazonaws.com/key name) and Path-Style (such as https://s3.Region.amazonaws.com/bucket-name/key name) URLs are supported, but path-style URLs (soon to be retired) will be eventually depreciated in favor of virtual hosted-style URLs for S3 bucket access. DNS compliant names are also recommended. Objetcs have eventual consistency: Create new files are read inmediately. Updates and deletes takes a little bit of time to propagate. Many other services can store data in S3, including Kinesis Firehose, Storage Gateway and the AWS API The S3 API is based in REST (Representational State Transfer) which uses CRUD methods S3 classes order by pricing: S3 Standard : Frequently accessed data. Availability of 99.99% S3 Standard - IA (Infrequently Accessed). Availability of 99.9% S3 One Zone - IA (when multiple AZ not required) . costs is 20% less than S3 Standard-IA S3 RRS (Reduce Redundancy Storage). Availability of 99.5%. RRS is the only S3 Class that does not offer 99.999999999% durability . S3 Glacier (low cost storage. Retrieval times from 3 minutes to 12 hours). archives stored in vaults(as buckets) with AES 256-bit encryption Expedited access within 3-5 min Standard access within 3-5 hours Bulk access within 5-12 hours Up to 5% retrieved at no change S3 Glacier Vault Lock Policy : compliance controls for S3 Glacier with a Vault Lock policy. The policy can no longer be changed Only empty vaults can be deleted S3 Glacier Deep Archive (lowest-cost. Retrieval time of 12 hours is acceptable) Enhanced features: S3 - Intelligent Tiering , using machine learning to move files to the most cost-effective access tier. It does not make sense to use S3 standard. Instead, use S3 - Intelligent Tiering to save money S3 Batch Operations create jobs to enable automatic actions S3 on Outposts store customer data generated on-premises before moving it to an AWS Region. S3 Objetc lock use WORM model (objects are Written Once and Read Many) during bucket creation. Objects (the whole bucket or individual files) became unmodificable and undeletable: Governance mode: some users are grant with permissions to alter settings or delete the object version Compliance mode: objects version cannot be modified or deleted, even by the root user More features: Unlimited storage We can use the bucket to host a static website using S3 with option static website hosting enabled, with index.html and error.html. Dynamic website can not be hosted on S3. Encryption: Encryption in Transit, using https (SSL/TLS) Encryption at Rest: In the server side - SSE (Server Side Encryption - 256-bit AES encryption - storage encryption, no transit encryption): SSE-S3 : An encryption key that Amazon S3 creates, manages, and uses for you. SSE-KMS : An encryption key protected by AWS Key Managament Service SSE-C with customer provided keys In the client side - encrypted before uploading with a client library such as Amazon S3 Encryption Client. Versioning (disabled by default): usefull as backup tool MFA Delete setting: an additional layer of security that requires MFA for changing Bucket Versioning settings and permanently deleting object versions. To modify MFA delete settings, use the AWS CLI, AWS SDK, or the Amazon S3 REST API. Versioning cannot be disabled once enabled Uploaded new verions are private by default Deleting a file create a new version ( deleted marker ), then restoring the file recovers all previous versions. Integrated with lifecycle rules Use S3 Lifecycle rules to define actions you want AWS S3 to take during an object's lifetime such as transitioning objects to another storage class, archiving them, or deleting them after a specified period of time. S3 Transfer Acceleration takes advantage of Amazon CloudFront (edge location distribution) using Amazon internal network (no internet). It enables fast, easy and secure transfers of files to and from your bucket. Speed Comparison Tool Cross Region Replication : replicate a bucket from a region to another: From S3 Management tab, select Create replication rule Create a new role and select the source bucket and destination bucket. Replicated buckets can be in different AWS accounts Replication requires versioning to be enable on the source and destionation buckets. we can change the storage class for the replicated objetcs replication starts when files are added and updated later. Deletes, permissions and previous versions are not replicated Multipart uploads use multithreading to upload large files to S3 buckets in parallel (the parts of the file are uploaded in parallel). Recommended for > 100mb and required for > 5Gb begin an upload before you know the final object size quick recovery from network issues improved throughput allow pause and resume object uploads. General S3 FAQs To upload a file larger than 160 GB, use the AWS CLI, AWS SDK, or Amazon S3 REST API. S3 Event notifications: Object Created Object Removed Object Restored RRS Object Lost Replication S3 Pricing depends on: Storage class Storage Requests Data transfer Transfer acceleration Cross region replication S3 Security: managed in the AWS console and the AWS CLI Uploaded files are private by default When editing S3 bucket permissions (policies and ACLs), the concept of the resource owner refers to he AWS account that creates Amazon S3 buckets and objects. Bucket ACLs (legacy): Object policies applies to an individual file. Bucket policies applies to the entire bucket - like one hosting a static S3 website public sample: { \"Version\" : \"2020-09-08\" , // identify the document structure \"Statement\" : [ { \"Sid\" : \"PublicReadGetObject\" , // statement id \"Effect\" : \"Allow\" , // allow | deny \"Principal\" : \"*\" , \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" ] } ] } S3 Performance: Use prefixes and delimiters to improve performance: path between the bucket and file Use SSE-KMS have limitations Use Multipart uploads Use downloads with S3 Byte-Range Fetches Use S3 Select / Glacier Select using SQL to download only the subset of data we need from ZIP or CSV files EFS \u00b6 AWS EFS (Elastic File System) is a mountable file storage service for EC2 ( linux ), like a NAS (Network Attached Storage - shared folder ), based on NFSv4 (Network File System), but has no connection to S3 which is an object storage service. EFS is shareable . there is folder hierarchy no supported on windows instances Data is stored across multiple AZ's within a region How to create an EFS shared by two EC2 instances \u00b6 Create file system from AWS EFS Create EC2 instances with following user data: #!/bin/bash yum update -y yum install httpd -y service httpd start chkconfig httpd on yum install amazon-efs-utils -y default SG require inbound rule of type NFSv4 (port 2049) with source SG WebDMZ connect to the EC2 instances, and then mount EFS using the command from EFS - Amazon EC2 mount instructions from local VPC : cd /var/www/html cd .. mount -t efs -o tls fs-9816b269:/ /var/www/html cd html echo \"<html><body><h1>using EFS</h1></body></html>\" > index.html EBS \u00b6 AWS EBS (Elastic Block Storage) is like a virtual hard disk in the cloud. This is a block service used for durable storage in EC2 instances and again has no connection to S3. EBS cannot be shared by two EC2 instances EBS volume can be attached to any EC2 instance in the same AZ EBS volume attached as an additional disk (not the root volume) can be detached without stopping the instance EBS Types: SSD (Solid State Drive) GP2 - General Purpose IO1 - Provisioned IOPS - from 10.000 to 32.000 Input Output per second - high performance HDD (Hard Disk Drive) ST1 - Throughtput optimised - Low cost for frequently access SC1 - Cold HDD - Lowest cost for less frequently access Magnetic - Previous generation HDD based volumes will always be less expensive than SSD types. an EBS-optimized EC2 instance should be used for high performance. No availabled in EC2 free-tier EBS Pricing depends on: Volumes Snapshots Data transfer volume recovery (attaching volumes from own intances to another) Although there is no direct way to encrypt an existing unencrypted volume or snapshot, you can encrypt them by creating either a volume or a snapshot. FSx (File Systems) \u00b6 AWS FSx for windows file server Based on SMB (Windows Server Message Block) AWS FSx for Lustre Optimised file system can store data on S3 can be deploy in single or multi AZ windows authentication using Active Directory self manage or by AWS Storage gateway \u00b6 connect on-premise software with cloud-based storage (S3) using AWS storage as local storage. File Gateway . Store files as objects in Amazon S3, with a local cache for low-latency access to your most recently used data. Volume Gateway : objects are hard disk drives. It looks like EBS snapshots. there are storage volumes (entire dataset) and cached volumes Gateway-Cached volumes allow you to store your data in S3 and retain a copy of frequently accessed data subsets locally. Cached volumes offer a substantial cost savings on primary storage and minimize the need to scale your storage on-premises. You also retain low-latency access to your frequently accessed data. Tape gateway : Back up your data to Amazon S3 using your existing tape-based processes as public or VPC (private) VTL (Virtual Tape Library) is a library/collection of backup tapes that can be backed up AWS Backup \u00b6 centrally manage and automate backups across AWS services, as well as on-premises servers","title":"2. Storage"},{"location":"AWS/02-Storage/#2-storage","text":"","title":"2. Storage"},{"location":"AWS/02-Storage/#s3","text":"AWS S3 (Simple Storage Service) is object-based for the safe storage of flat files such as text files, videos, pictures and any other flat file from 0 to 5 tb. The object has a key (filename), value (data), versionID, metadata, encryption, and security by ACL (Access Control Lists), torrent and Bucket Policies Objects stored in S3 (no One Zone) are stored across at least three AZs Buckets are folders/containers for everything that you store in S3. S3 bucket names are global , and must be unique, but they are create in a region. Universal namespaces are like https://s3-region_name.amazonaws.com/bucket_name. The response code is http 200 when file upload succeeds. Contact AWS to create more than 100 buckets minimum object size is 0 kb an object can have up to 10 tags Both Virtual-host-Style (such as: https://bucket-name.s3.Region.amazonaws.com/key name) and Path-Style (such as https://s3.Region.amazonaws.com/bucket-name/key name) URLs are supported, but path-style URLs (soon to be retired) will be eventually depreciated in favor of virtual hosted-style URLs for S3 bucket access. DNS compliant names are also recommended. Objetcs have eventual consistency: Create new files are read inmediately. Updates and deletes takes a little bit of time to propagate. Many other services can store data in S3, including Kinesis Firehose, Storage Gateway and the AWS API The S3 API is based in REST (Representational State Transfer) which uses CRUD methods S3 classes order by pricing: S3 Standard : Frequently accessed data. Availability of 99.99% S3 Standard - IA (Infrequently Accessed). Availability of 99.9% S3 One Zone - IA (when multiple AZ not required) . costs is 20% less than S3 Standard-IA S3 RRS (Reduce Redundancy Storage). Availability of 99.5%. RRS is the only S3 Class that does not offer 99.999999999% durability . S3 Glacier (low cost storage. Retrieval times from 3 minutes to 12 hours). archives stored in vaults(as buckets) with AES 256-bit encryption Expedited access within 3-5 min Standard access within 3-5 hours Bulk access within 5-12 hours Up to 5% retrieved at no change S3 Glacier Vault Lock Policy : compliance controls for S3 Glacier with a Vault Lock policy. The policy can no longer be changed Only empty vaults can be deleted S3 Glacier Deep Archive (lowest-cost. Retrieval time of 12 hours is acceptable) Enhanced features: S3 - Intelligent Tiering , using machine learning to move files to the most cost-effective access tier. It does not make sense to use S3 standard. Instead, use S3 - Intelligent Tiering to save money S3 Batch Operations create jobs to enable automatic actions S3 on Outposts store customer data generated on-premises before moving it to an AWS Region. S3 Objetc lock use WORM model (objects are Written Once and Read Many) during bucket creation. Objects (the whole bucket or individual files) became unmodificable and undeletable: Governance mode: some users are grant with permissions to alter settings or delete the object version Compliance mode: objects version cannot be modified or deleted, even by the root user More features: Unlimited storage We can use the bucket to host a static website using S3 with option static website hosting enabled, with index.html and error.html. Dynamic website can not be hosted on S3. Encryption: Encryption in Transit, using https (SSL/TLS) Encryption at Rest: In the server side - SSE (Server Side Encryption - 256-bit AES encryption - storage encryption, no transit encryption): SSE-S3 : An encryption key that Amazon S3 creates, manages, and uses for you. SSE-KMS : An encryption key protected by AWS Key Managament Service SSE-C with customer provided keys In the client side - encrypted before uploading with a client library such as Amazon S3 Encryption Client. Versioning (disabled by default): usefull as backup tool MFA Delete setting: an additional layer of security that requires MFA for changing Bucket Versioning settings and permanently deleting object versions. To modify MFA delete settings, use the AWS CLI, AWS SDK, or the Amazon S3 REST API. Versioning cannot be disabled once enabled Uploaded new verions are private by default Deleting a file create a new version ( deleted marker ), then restoring the file recovers all previous versions. Integrated with lifecycle rules Use S3 Lifecycle rules to define actions you want AWS S3 to take during an object's lifetime such as transitioning objects to another storage class, archiving them, or deleting them after a specified period of time. S3 Transfer Acceleration takes advantage of Amazon CloudFront (edge location distribution) using Amazon internal network (no internet). It enables fast, easy and secure transfers of files to and from your bucket. Speed Comparison Tool Cross Region Replication : replicate a bucket from a region to another: From S3 Management tab, select Create replication rule Create a new role and select the source bucket and destination bucket. Replicated buckets can be in different AWS accounts Replication requires versioning to be enable on the source and destionation buckets. we can change the storage class for the replicated objetcs replication starts when files are added and updated later. Deletes, permissions and previous versions are not replicated Multipart uploads use multithreading to upload large files to S3 buckets in parallel (the parts of the file are uploaded in parallel). Recommended for > 100mb and required for > 5Gb begin an upload before you know the final object size quick recovery from network issues improved throughput allow pause and resume object uploads. General S3 FAQs To upload a file larger than 160 GB, use the AWS CLI, AWS SDK, or Amazon S3 REST API. S3 Event notifications: Object Created Object Removed Object Restored RRS Object Lost Replication S3 Pricing depends on: Storage class Storage Requests Data transfer Transfer acceleration Cross region replication S3 Security: managed in the AWS console and the AWS CLI Uploaded files are private by default When editing S3 bucket permissions (policies and ACLs), the concept of the resource owner refers to he AWS account that creates Amazon S3 buckets and objects. Bucket ACLs (legacy): Object policies applies to an individual file. Bucket policies applies to the entire bucket - like one hosting a static S3 website public sample: { \"Version\" : \"2020-09-08\" , // identify the document structure \"Statement\" : [ { \"Sid\" : \"PublicReadGetObject\" , // statement id \"Effect\" : \"Allow\" , // allow | deny \"Principal\" : \"*\" , \"Action\" : [ \"s3:GetObject\" ], \"Resource\" : [ \"arn:aws:s3:::BUCKET_NAME/*\" ] } ] } S3 Performance: Use prefixes and delimiters to improve performance: path between the bucket and file Use SSE-KMS have limitations Use Multipart uploads Use downloads with S3 Byte-Range Fetches Use S3 Select / Glacier Select using SQL to download only the subset of data we need from ZIP or CSV files","title":"S3"},{"location":"AWS/02-Storage/#efs","text":"AWS EFS (Elastic File System) is a mountable file storage service for EC2 ( linux ), like a NAS (Network Attached Storage - shared folder ), based on NFSv4 (Network File System), but has no connection to S3 which is an object storage service. EFS is shareable . there is folder hierarchy no supported on windows instances Data is stored across multiple AZ's within a region","title":"EFS"},{"location":"AWS/02-Storage/#how-to-create-an-efs-shared-by-two-ec2-instances","text":"Create file system from AWS EFS Create EC2 instances with following user data: #!/bin/bash yum update -y yum install httpd -y service httpd start chkconfig httpd on yum install amazon-efs-utils -y default SG require inbound rule of type NFSv4 (port 2049) with source SG WebDMZ connect to the EC2 instances, and then mount EFS using the command from EFS - Amazon EC2 mount instructions from local VPC : cd /var/www/html cd .. mount -t efs -o tls fs-9816b269:/ /var/www/html cd html echo \"<html><body><h1>using EFS</h1></body></html>\" > index.html","title":"How to create an EFS shared by two EC2 instances"},{"location":"AWS/02-Storage/#ebs","text":"AWS EBS (Elastic Block Storage) is like a virtual hard disk in the cloud. This is a block service used for durable storage in EC2 instances and again has no connection to S3. EBS cannot be shared by two EC2 instances EBS volume can be attached to any EC2 instance in the same AZ EBS volume attached as an additional disk (not the root volume) can be detached without stopping the instance EBS Types: SSD (Solid State Drive) GP2 - General Purpose IO1 - Provisioned IOPS - from 10.000 to 32.000 Input Output per second - high performance HDD (Hard Disk Drive) ST1 - Throughtput optimised - Low cost for frequently access SC1 - Cold HDD - Lowest cost for less frequently access Magnetic - Previous generation HDD based volumes will always be less expensive than SSD types. an EBS-optimized EC2 instance should be used for high performance. No availabled in EC2 free-tier EBS Pricing depends on: Volumes Snapshots Data transfer volume recovery (attaching volumes from own intances to another) Although there is no direct way to encrypt an existing unencrypted volume or snapshot, you can encrypt them by creating either a volume or a snapshot.","title":"EBS"},{"location":"AWS/02-Storage/#fsx-file-systems","text":"AWS FSx for windows file server Based on SMB (Windows Server Message Block) AWS FSx for Lustre Optimised file system can store data on S3 can be deploy in single or multi AZ windows authentication using Active Directory self manage or by AWS","title":"FSx (File Systems)"},{"location":"AWS/02-Storage/#storage-gateway","text":"connect on-premise software with cloud-based storage (S3) using AWS storage as local storage. File Gateway . Store files as objects in Amazon S3, with a local cache for low-latency access to your most recently used data. Volume Gateway : objects are hard disk drives. It looks like EBS snapshots. there are storage volumes (entire dataset) and cached volumes Gateway-Cached volumes allow you to store your data in S3 and retain a copy of frequently accessed data subsets locally. Cached volumes offer a substantial cost savings on primary storage and minimize the need to scale your storage on-premises. You also retain low-latency access to your frequently accessed data. Tape gateway : Back up your data to Amazon S3 using your existing tape-based processes as public or VPC (private) VTL (Virtual Tape Library) is a library/collection of backup tapes that can be backed up","title":"Storage gateway"},{"location":"AWS/02-Storage/#aws-backup","text":"centrally manage and automate backups across AWS services, as well as on-premises servers","title":"AWS Backup"},{"location":"AWS/03-Network/","text":"3. Network \u00b6 ENI - Elastic Network Interface - virtual network card or adapter attached to an AWS EC2 instance for basic networking. It can include multiple attributes, such as security groups, IPv6 and IPv4 addresses, MAC addresses, and more. ENI can be attached to an instance: when it\u2019s running ( hot attach ) when it\u2019s stopped ( warm attach ) when the instance is being launched ( cold attach ) Multiple ENIs connected to a single instance allows dual-homing ENIs are assocated with a subnet ENA - Enhanced Networking Adapter - use SR-IOV (Single Root I/O Virtualization) to allow speeds between 10 and 100 Gbps requirement EFA - Elastic Fabric Adapter - machine learning or HPC (High Performance Computing) requirement VPC \u00b6 An interactive IP address and CIDR range visualizer here 192.168 is the network and 0.1 is the host DHCP (Dynamic Host Configuration Protocol) will be used to provide dynamic addresses where required within the VPC Traffic Mirroring copies network traffic from an ENI and sends it wherever you want it to go AWS VPC (Virtual Private Cloud) is like a logical datacenter in AWS. A VPC is an isolated portion of the AWS cloud dedicated to a single AWS account where you can launch AWS resources. You define a VPC\u2019s IP address space from ranges you select (10.0.0.0/16). Subnets are segments of a VPC\u2019s IP address range where you can place groups of isolated resources (10.0.1.0/24). 1 subnet = 1 AZ each default subnet is a public subnet (DMZ). Each instance that you launch into a default subnet has a public IPv4 address and a private IPv4 address each nondefault subnet has a private IPv4 address, but no public IPv4 address a public subnet within a VPC is one that has at least one route in its routing table that uses an Internet Gateway (IGW). You can enable internet access for an instance launched into a nondefault subnet by attaching an internet gateway to its nondefault VPC and associating an Elastic IP address with the instance. EIP ( Elastic IP ) are public IP addresses from the VPC EIP are permnently allocated to you account untill released EIP has a price, so the account is charged untill release ENIs consume EIPs EIPs can be moved between instances in the same region. Creating an Elastic IP address and associate it with your EC2 instance would be the simplest way to make your instance reachable from the outside world. Route tables are a set of rules, called routes, that are used to determine where network traffic is directed. Internet Gateway allow communication between your VPC and the internet. An IG serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses 1 VPC = 1 IG egress-only internet gateway allows IPv6 based traffic within a VPC to access the internet, whilst denying any internet based resources to connection back into the VPC. VPN reuses existing VPN equipment and processes, and reuse existing internet connections. VPG (Virtual Private Gateway) is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection. CWG (customer GateWay) is a resource that is installed on the customer side of the Site-to-Site VPN connection. VPC peering creates a connection between two VPCs using same or different accounts and regions. No transitive: VPC peering only routes traffic between source and destination VPCs. no transitive peering VPC-A <=> VPC-B <=> VPC-C ... VPC-A <> VPC-C owner role required RTs must be configured with the destination VPC and the origin (target) VPC peering. VPC Endpoints : connections that enables private connectivity to services hosted in AWS, based on region and service name, from within your VPC without using an Internet Gateway, VPN, Network Address Translation (NAT) devices, or firewall proxies. Interface endpoints Gateway Load Balancer endpoints Gateway endpoints VPC pricing: traffic in the VPC is free using same AZ with private IP is free using different AZs and public IP has a cost Some scans can be performed without alerting AWS, some require you to alert, such as Penetration Testing every aws account has a default VPC in each region. AWS recomends not deleting them You can have up to 5 non-default VPCs per account and region, but you can place a support request to increase the number. Once a VPC is set to Dedicated hosting, it can be changed back to default hosting via the CLI, SDK or API. Note that this will not change hosting settings for existing instances, only future ones. Existing instances can be changed via CLI, SDK or API modifying the Instance Placement attribute but need to be in a stopped state to do so. How to create a VPC \u00b6 Creating a Basic VPC and Associated Components in AWS Create VPC with IPv4 CIDR block equals 10.0.0.0/16 and IPv6 CIDR block provided by Amazon. Note that only a RT, a NACL and a SG are created Create subnet with IPv4 CIDR block equals 10.0.1.0/24 (public) and 10.0.2.0/24 (private) with different AZs. AWS reserves both the first four and the last IP address (5) in each subnet's CIDR block, so there are 251 available IPv4 addresses instead of 256 Select the public subnet and from Actions, clic on Modify auto-assign IP settings to automatically request a public IP for a new network interface in this subnet. Create internet gateway as the virtual router that connects the VPC to the internet. Select the IG and from Actions, clic on attach to VPC aws ec2 attach-internet-gateway --vpc-id \"vpc-05fa9d6e7085db9bf\" --internet-gateway-id \"igw-0e2c1cfa68b4dfc76\" --region us-east-2 Create route table to specify how packets are forwarded between the subnets within your VPC, the internet, and your VPN connection. New subnets will be associated to the main RT instead of this public RT. Allow the internet access selecting the public RT and Edit routes button and add route with destination 0.0.0.0/0 (IPv4) and ::/0 (IPv6) and our IG as the target (origin) open the connection from the IG to any IP address Edit subnet associations and select 10.0.1.0/24 to associate the public subnet with the public RT Create first EC2 instances as WebServer with network equals our VPC select public subnet and auto-assign public ip equals use subnet settings (enabled) for the first instance Create new SG as WebDMZ with rules SSH and HTTP Create a new key-pair as rnietoe-ohio chmod 400 rnietoe-ohio.pem # set read permissions to the user ssh ec2-user@3.16.203.67 -i rnietoe-ohio.pem Create second EC2 instances as DBServer with network equals our VPC select private subnet and auto-assign public ip equals use subnet settings (disabled) for the second instance Select default SG instead of WebDMZ there is no public IP so we can't connect to this instance Create new SG as DBSG with our VPC Add inbound rules of type All ICMP - IPv2 with source 10.0.1.0/24 to allow WebDMZ to ping EC2 instance inside this SG, HTTP, HTTPS, SSH and MySQL/Aurora Set DBSG to DBServer from actions : security : change security group . Remove default SG too Ping the DBServer private IP from WebServer ping 10 .0.2.200 Copy private key (rnietoe-ohio.pem) to WebServer and check the connection from WebServer to DBServer nano rnietoe-ohio.pem chmod 400 rnietoe-ohio.pem ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # this fail because private instance does not have access to internet By default, instances in new subnets in a custom VPC can communicate with each other across AZs. NAT instances \u00b6 NAT instances (Network Address Transaction) are single EC2 instances. a NAT instance allows you to get your private subnets communicate out to the internet without becaming public source/destination checks on the NAT instance must be disabled to allow the sending and receiving traffic for the private instances Launch EC2 instance choosing Amazon Linux 2 AMI 2.0.20201126.0 x86_64 HVM gp2 from Community AMIs Select rnietoeVPC and public subnet Select WebDMZ as SG and the same key pair Select EC2 instance and clic on actions : networking : change source/destination check and disable all the traffic it sends and receives, as NAT instance requirement aws ec2 modify-instance-attribute --instance-id = i-0cdece2dd619e009b --no-source-dest-check Edit private route table and create a route (from edit routes ) to allow connections (0.0.0.0/0) to the NAT instance. Test it: ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # should work now, but it does not We have created a small VM that will not work for thouthands of EC2 instances. NAT Gateway \u00b6 NAT gateway enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. NAT gateway are redundant inside the AZ 1 NAT gateway per AZ not need to patch not associated with SGs no need to disable source/destination check To use a NAT gateway, create one in a public subnet and assign it an Elastic IP address. Then, update the route tables for your private subnets to point internet traffic to the NAT gateway. Terminate previous EC2 NAT instance From VPC, create NAT gateway Select public subnet and allocate a new Elastic IP Edit private route table and create a route (from edit routes ) to allow connections (0.0.0.0/0) to the NAT gateway. Test it: ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # this work successfully yum install mysql -y Network ACLs \u00b6 network ACLs are stateless (outbound traffic must be specified) Block IP addresses using NACL instead of SG NACLs act on the subnet level, while SGs act on the instance level. NACL rule number defined precedence Default NACL allow all traffic????????????????? Create Network ACL: All inbound rules are denied by default??????????????? Create a Web page in the EC2 WebServer and check the valid connection: service httpd status # Unit httpd.service could not be found sudo su yum install httpd -y chkconfig httpd on service httpd start cd /var/www/html nano index.html <html><body><h1>This is server 1 </h1></body></html> Edit subnet associations for our NACL named WebNACL and select the public subnet public subnet is disassociate from the default NACL because a subnet can be associated to a NACL only 1 NACL - 1 subnet web page is not accessible now Edit inbound rules adding new rules (100, 200, 300) allowing ports 80, 443 and 22 Rule Number increase in 100 units, like 100, 200, 300... inbound rules work order by rule number, so first allow some ports and then deny everything else Edit outbound rules adding new rules allowing ports 80, 443 and 1024-65535 NACL are stateless: outbound rules have to be defined explicitly Ephemeral ports Requests originating from Elastic Load Balancing use ports 1024-65535. Edit inbound rules adding new rule 400 denying port 80 to my public Ip 92.189.102.194/32 Web page still accessible since rule 100 allow the traffic Edit inbound rules editing rule 400 as 99 Web page no accessible since rule 99 deny the traffic before rule 100 allow all traffic Edit inbound rules removing rule 99 and add new rule 400 to allow traffic on ports 1024-65535 yum update -y # it should works again``` Each network ACL also includes a rule whose rule number is an asterisk. This rule ensures that if a packet doesn't match any of the other numbered rules, it's denied. You can't modify or remove this rule: 100 All Traffic Allow 200 All Traffic Deny * All Traffic Deny VPC FlowLogs \u00b6 VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. VPC Flow Logs can be created at the VPC, subnet, and ENI levels. you can enable flow logs if VPC is in the same aws account you can not edit flow logs not all ip traffic is monitored such as: Amazon DNS Server Windows license activation 169.254.169.254 (metadata) DHCP traffic VPC reserved ports How to configure VPC FlowLogs: Create VPC with ipv4 CIDR block as 10.0.0.0/16 Create subnet with ipv4 CIDR block as 10.0.1.0/24 and enable auto-assign public IPv4 address Launch EC2 instance with VPC, subnet and a new SG with HTTP and SSH rules Create IG and associate it to the default VPC RT Create flow log in the ENI (Elastic Network interface) specify the filter: accepted traffic only rejected traffic only capture all traffic set Maximum aggregation interval to 1 min flow log data destination can be CloudWatch Logs or S3 bucket. Select S3 bucket Create bucket as arn:aws:s3:::rnietoeflowlogs Specify AWS default format Create log group from CloudWatch as VPCFlowLogs Create flow log again: specify the filter with all the traffic set Maximum aggregation interval to 1 min set flow log data destination as CloudWatch Logs select the destination log group as VPCFlowLogs set up permissions to define the IAM role that has permission to publish to the Amazon CloudWatch log group. Create new role as flowlogsRole with the following policy: { \"Statement\" : [ { \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:CreateLogStream\" , \"logs:DescribeLogGroups\" , \"logs:DescribeLogStreams\" , \"logs:PutLogEvents\" ], \"Effect\" : \"Allow\" , \"Resource\" : \"*\" } ] } Go to CloudWatch : Log groups : VPCFlowLogs : Log stream and check results Create filter pattern as (protocol 6 is TCP): [version, account, eni, source, destination, srcport, destport=\"22\", protocol=\"6\", packets, bytes, windowstart, windowend, action=\"ACCEPT\", flowlogstatus] test pattern 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 22 6 1 40 1563108188 1563108227 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 182.68.238.8 172.31.83.158 42227 22 6 1 44 1563109030 1563109067 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 42.171.23.181 172.31.83.158 52417 22 6 24 4065 1563191069 1563191121 ACCEPT OK 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 80 6 1 40 1563108188 1563108227 REJECT OK set metric details Create alarm for above metric filter set period to 1 min Whenever SSHAccept is greater/equal than 1 Run query from CloudWatch Logs Insights using VPC Flow Logs sample queries (right panel) Go back to S3 and check a new folder named AWSLogs has been created in our rnietoeflowlogs bucket Go to AWS Athena and set up a query result location in AWS S3 with the arn: s3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/2020/12/12/ Run query to create Athena Table CREATE EXTERNAL TABLE IF NOT EXISTS default . vpc_flow_logs ( version int , account string , interfaceid string , sourceaddress string , destinationaddress string , sourceport int , destinationport int , protocol int , numpackets int , numbytes bigint , starttime int , endtime int , action string , logstatus string ) PARTITIONED BY ( dt string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' LOCATION 's3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/' TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); Create Partitions ALTER TABLE default . vpc_flow_logs ADD PARTITION ( dt = '2020-12-12' ) location 's3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/2020/12/12' ; Analyze Data SELECT day_of_week ( from_iso8601_timestamp ( dt )) AS day , dt , interfaceid , sourceaddress , destinationport , action , protocol FROM vpc_flow_logs WHERE action = 'ACCEPT' AND protocol = 6 order by sourceaddress LIMIT 100 ; Bastion host \u00b6 a bastion (jump box) is used to securely administer/manage EC2 instances in the private subnet using SSH or RDP NAT gateway cannot be used as a bastion host For high availability: create two public subnets in two different AZs with a bastion host each, and a Network Load Balancer with static IP address or Auto scaling group (cheaper) cannot use an Application Load Balancer as it is layer 7 and you need to use layer 4. Direct Connect \u00b6 Dedicated line from on premise to AWS to improve the (VPN) network connection (security and performance) from 1 Gbps to 10 Gbps Create virtual interface from AWS Direct Connect : Virtual interfaces as Public Create Customer Gateway from VPC : Customer Gateways Create Virtual Private Gateway from VPC : Virtual Private Gateways attach the VPG to the VPC Create VPN Connection from VPC : Site-to-Site VPN Connections using the VPG and Customer Gateway Set up the VPN on the customer gateway How do I configure a VPN over AWS Direct Connect? Global Accelerator \u00b6 improves the availability and performance of your applications for local or global users 1234567890abcdef.awsglobalaccelerator.com Create accelerator from AWS Global Accelerator Add a listener to checks for connection requests that arrive to an assigned set of static IP addresses on a port or port range that you specify. (80, 443) Leave client affinity setting as none Add endpoint groups where the accelerator direct traffic to from one or more listeners. An endpoint group includes endpoints, such as load balancers. Add endpoints to each endpoint group Endpoints can be Network Load Balancers, Application Load Balancers, EC2 instances, or Elastic IP addresses. two static IP addresses are assigned. Disable Global Accelerator before removing is required Network zones are simliar to AZs. They are isolated units with their own set of physical infrastructure and service IP addresses from a unique IP subnet. If one IP address from a network zone becomes unavailable, due to network disruptions or IP address blocking by certain client networks, your client applications can retry using the healthy static IP address from the other isolated network zone. VPC endpoints \u00b6 VPC endpoint is a service that replace NAT gateway and allow connections from the private subnet to other AWS services, such as S3 Interface endpoints: ENI (Elastic Network Interface) with private IP as entry point Gateway endpoints support S3 and DynamoDB Create Endpoint from VPC : Endpoints select service com.amazonaws.us-east-2.s3 , our VPC, our main route table and the full access policy the update in the route table could take some time AWS Private Link \u00b6 To open up our apps to other VPCs, we can try: if open up the VPC to the internet. everything will be public you can use VPC peering . However, many relationships will be required AWS Private Link peers many VPCs. They only require a NLB on the AWS VPC and a ENI on the customer VPC Transit Gateway \u00b6 TGW (Transit GateWay) is a network transit hub that interconnects attachments (VPCs and VPNs) within the same account or across accounts. Cross region is allowed support IP multicast VPN CloudHub \u00b6 AWS VPN CloudHub manage multiple sites with own VPN connections Route 53 \u00b6 AWS Route 53 service name comes from port 53, where DNS (Domain Name System) work on we can register a DNS using Route53 - Register domain . You can purchase and manage domain names such as example.com, and Route 53 will automatically configure DNS settings for your domains Ensure there is a free bucket with the same domain name Failover Routing policy routes data to a second resource if the first is unhealthy. Route 53 can be used for Disaster Recovery by simply shifting traffic to the new region. Latency-based Routing policy routes data to resources that have better performance Route 53 Traffic Flow makes it easy for you to manage traffic globally through a variety of routing types. Using Route 53 Traffic Flow\u2019s simple visual editor, you can easily manage how your end-users are routed to your application\u2019s endpoints\u2014whether in a single AWS region or distributed around the globe. .com => NS (Name Server) Records => SOA (Start Of Authority) DNS changes can take 48 hours to take effect due to the cache CName (Canonical Name) maps to the host name: https://mobile.acloud.guru = https://m.acloud.guru Alias Record provide a Route 53\u2013specific extension to DNS functionality. An alias could be created for the ELB. Alias Records can also point to AWS Resources that are hosted in other accounts by manually entering the ARN ELB resolve DNS names instead of IPv4 addresses Routing Policies: Simple Routing: one dns record with multiple IP addresses Weighted Routing: traffic based on weighting (20%-30%-50%) Latency-based Routing: traffic based on the lowest latency Failover Routing: route the traffic to the primary or secondary site defined based on health checks Geolocation Routing: traffic based on the user's location Geoproximity Routing: traffic based on the users' and resources' location. Available in traffic flow-only mode using bias Multivalue Answer Routing, similar to simple routing, but using health checks on each record sets to serve traffic to random web servers Using Route53: Register domain from Route53 takes between 2 hours and 3 days Create Record Set of type IPv4 address with the three EC2 public IPs. Set TTL (Time to Live) to 1 min to clear from cache Create healthcheck and associate it to each record set, so it will be removed from Route53 until it passes the health check Create traffic policy to configure Geoproximity Routing ipconfig /flushdns # to remove saved ip from cache from the client side With Route 53, there is a default limit of 50 domain names. However, this limit can be increased by contacting AWS support API Gateway \u00b6 API Gateway vs Application Load Balancer API Gateway + Lambda + DynamoDB (serverless) instead of ELB + EC2 + RDS API (Application Programming Interface) Gateway is like a door for your AWS environment. Targets are: Lamda EC2 DynamoDB scaling is automatic (as aws lambda) enable api gateway caching to cache API gateway endpoint's responses for a TTL period in seconds. If a cache is configured, then Amazon API Gateway will return a cached response for duplicate requests for a customizable time, but only if under configured throttling limits. same origin policy to prevent cross site scripting (XSS) attacks. CORS (Cross Origin Resource Sharing) allow restricted resources in a web page to be requested from a different domain. Enable CORS in Apigateway when the error message is \"Origin policy cannot be read at the remote resource\" .","title":"3. Network"},{"location":"AWS/03-Network/#3-network","text":"ENI - Elastic Network Interface - virtual network card or adapter attached to an AWS EC2 instance for basic networking. It can include multiple attributes, such as security groups, IPv6 and IPv4 addresses, MAC addresses, and more. ENI can be attached to an instance: when it\u2019s running ( hot attach ) when it\u2019s stopped ( warm attach ) when the instance is being launched ( cold attach ) Multiple ENIs connected to a single instance allows dual-homing ENIs are assocated with a subnet ENA - Enhanced Networking Adapter - use SR-IOV (Single Root I/O Virtualization) to allow speeds between 10 and 100 Gbps requirement EFA - Elastic Fabric Adapter - machine learning or HPC (High Performance Computing) requirement","title":"3. Network"},{"location":"AWS/03-Network/#vpc","text":"An interactive IP address and CIDR range visualizer here 192.168 is the network and 0.1 is the host DHCP (Dynamic Host Configuration Protocol) will be used to provide dynamic addresses where required within the VPC Traffic Mirroring copies network traffic from an ENI and sends it wherever you want it to go AWS VPC (Virtual Private Cloud) is like a logical datacenter in AWS. A VPC is an isolated portion of the AWS cloud dedicated to a single AWS account where you can launch AWS resources. You define a VPC\u2019s IP address space from ranges you select (10.0.0.0/16). Subnets are segments of a VPC\u2019s IP address range where you can place groups of isolated resources (10.0.1.0/24). 1 subnet = 1 AZ each default subnet is a public subnet (DMZ). Each instance that you launch into a default subnet has a public IPv4 address and a private IPv4 address each nondefault subnet has a private IPv4 address, but no public IPv4 address a public subnet within a VPC is one that has at least one route in its routing table that uses an Internet Gateway (IGW). You can enable internet access for an instance launched into a nondefault subnet by attaching an internet gateway to its nondefault VPC and associating an Elastic IP address with the instance. EIP ( Elastic IP ) are public IP addresses from the VPC EIP are permnently allocated to you account untill released EIP has a price, so the account is charged untill release ENIs consume EIPs EIPs can be moved between instances in the same region. Creating an Elastic IP address and associate it with your EC2 instance would be the simplest way to make your instance reachable from the outside world. Route tables are a set of rules, called routes, that are used to determine where network traffic is directed. Internet Gateway allow communication between your VPC and the internet. An IG serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses 1 VPC = 1 IG egress-only internet gateway allows IPv6 based traffic within a VPC to access the internet, whilst denying any internet based resources to connection back into the VPC. VPN reuses existing VPN equipment and processes, and reuse existing internet connections. VPG (Virtual Private Gateway) is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection. CWG (customer GateWay) is a resource that is installed on the customer side of the Site-to-Site VPN connection. VPC peering creates a connection between two VPCs using same or different accounts and regions. No transitive: VPC peering only routes traffic between source and destination VPCs. no transitive peering VPC-A <=> VPC-B <=> VPC-C ... VPC-A <> VPC-C owner role required RTs must be configured with the destination VPC and the origin (target) VPC peering. VPC Endpoints : connections that enables private connectivity to services hosted in AWS, based on region and service name, from within your VPC without using an Internet Gateway, VPN, Network Address Translation (NAT) devices, or firewall proxies. Interface endpoints Gateway Load Balancer endpoints Gateway endpoints VPC pricing: traffic in the VPC is free using same AZ with private IP is free using different AZs and public IP has a cost Some scans can be performed without alerting AWS, some require you to alert, such as Penetration Testing every aws account has a default VPC in each region. AWS recomends not deleting them You can have up to 5 non-default VPCs per account and region, but you can place a support request to increase the number. Once a VPC is set to Dedicated hosting, it can be changed back to default hosting via the CLI, SDK or API. Note that this will not change hosting settings for existing instances, only future ones. Existing instances can be changed via CLI, SDK or API modifying the Instance Placement attribute but need to be in a stopped state to do so.","title":"VPC"},{"location":"AWS/03-Network/#how-to-create-a-vpc","text":"Creating a Basic VPC and Associated Components in AWS Create VPC with IPv4 CIDR block equals 10.0.0.0/16 and IPv6 CIDR block provided by Amazon. Note that only a RT, a NACL and a SG are created Create subnet with IPv4 CIDR block equals 10.0.1.0/24 (public) and 10.0.2.0/24 (private) with different AZs. AWS reserves both the first four and the last IP address (5) in each subnet's CIDR block, so there are 251 available IPv4 addresses instead of 256 Select the public subnet and from Actions, clic on Modify auto-assign IP settings to automatically request a public IP for a new network interface in this subnet. Create internet gateway as the virtual router that connects the VPC to the internet. Select the IG and from Actions, clic on attach to VPC aws ec2 attach-internet-gateway --vpc-id \"vpc-05fa9d6e7085db9bf\" --internet-gateway-id \"igw-0e2c1cfa68b4dfc76\" --region us-east-2 Create route table to specify how packets are forwarded between the subnets within your VPC, the internet, and your VPN connection. New subnets will be associated to the main RT instead of this public RT. Allow the internet access selecting the public RT and Edit routes button and add route with destination 0.0.0.0/0 (IPv4) and ::/0 (IPv6) and our IG as the target (origin) open the connection from the IG to any IP address Edit subnet associations and select 10.0.1.0/24 to associate the public subnet with the public RT Create first EC2 instances as WebServer with network equals our VPC select public subnet and auto-assign public ip equals use subnet settings (enabled) for the first instance Create new SG as WebDMZ with rules SSH and HTTP Create a new key-pair as rnietoe-ohio chmod 400 rnietoe-ohio.pem # set read permissions to the user ssh ec2-user@3.16.203.67 -i rnietoe-ohio.pem Create second EC2 instances as DBServer with network equals our VPC select private subnet and auto-assign public ip equals use subnet settings (disabled) for the second instance Select default SG instead of WebDMZ there is no public IP so we can't connect to this instance Create new SG as DBSG with our VPC Add inbound rules of type All ICMP - IPv2 with source 10.0.1.0/24 to allow WebDMZ to ping EC2 instance inside this SG, HTTP, HTTPS, SSH and MySQL/Aurora Set DBSG to DBServer from actions : security : change security group . Remove default SG too Ping the DBServer private IP from WebServer ping 10 .0.2.200 Copy private key (rnietoe-ohio.pem) to WebServer and check the connection from WebServer to DBServer nano rnietoe-ohio.pem chmod 400 rnietoe-ohio.pem ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # this fail because private instance does not have access to internet By default, instances in new subnets in a custom VPC can communicate with each other across AZs.","title":"How to create a VPC"},{"location":"AWS/03-Network/#nat-instances","text":"NAT instances (Network Address Transaction) are single EC2 instances. a NAT instance allows you to get your private subnets communicate out to the internet without becaming public source/destination checks on the NAT instance must be disabled to allow the sending and receiving traffic for the private instances Launch EC2 instance choosing Amazon Linux 2 AMI 2.0.20201126.0 x86_64 HVM gp2 from Community AMIs Select rnietoeVPC and public subnet Select WebDMZ as SG and the same key pair Select EC2 instance and clic on actions : networking : change source/destination check and disable all the traffic it sends and receives, as NAT instance requirement aws ec2 modify-instance-attribute --instance-id = i-0cdece2dd619e009b --no-source-dest-check Edit private route table and create a route (from edit routes ) to allow connections (0.0.0.0/0) to the NAT instance. Test it: ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # should work now, but it does not We have created a small VM that will not work for thouthands of EC2 instances.","title":"NAT instances"},{"location":"AWS/03-Network/#nat-gateway","text":"NAT gateway enable instances in a private subnet to connect to the internet or other AWS services, but prevent the internet from initiating a connection with those instances. NAT gateway are redundant inside the AZ 1 NAT gateway per AZ not need to patch not associated with SGs no need to disable source/destination check To use a NAT gateway, create one in a public subnet and assign it an Elastic IP address. Then, update the route tables for your private subnets to point internet traffic to the NAT gateway. Terminate previous EC2 NAT instance From VPC, create NAT gateway Select public subnet and allocate a new Elastic IP Edit private route table and create a route (from edit routes ) to allow connections (0.0.0.0/0) to the NAT gateway. Test it: ssh ec2-user@10.0.2.200 -i rnietoe-ohio.pem yum update -y # this work successfully yum install mysql -y","title":"NAT Gateway"},{"location":"AWS/03-Network/#network-acls","text":"network ACLs are stateless (outbound traffic must be specified) Block IP addresses using NACL instead of SG NACLs act on the subnet level, while SGs act on the instance level. NACL rule number defined precedence Default NACL allow all traffic????????????????? Create Network ACL: All inbound rules are denied by default??????????????? Create a Web page in the EC2 WebServer and check the valid connection: service httpd status # Unit httpd.service could not be found sudo su yum install httpd -y chkconfig httpd on service httpd start cd /var/www/html nano index.html <html><body><h1>This is server 1 </h1></body></html> Edit subnet associations for our NACL named WebNACL and select the public subnet public subnet is disassociate from the default NACL because a subnet can be associated to a NACL only 1 NACL - 1 subnet web page is not accessible now Edit inbound rules adding new rules (100, 200, 300) allowing ports 80, 443 and 22 Rule Number increase in 100 units, like 100, 200, 300... inbound rules work order by rule number, so first allow some ports and then deny everything else Edit outbound rules adding new rules allowing ports 80, 443 and 1024-65535 NACL are stateless: outbound rules have to be defined explicitly Ephemeral ports Requests originating from Elastic Load Balancing use ports 1024-65535. Edit inbound rules adding new rule 400 denying port 80 to my public Ip 92.189.102.194/32 Web page still accessible since rule 100 allow the traffic Edit inbound rules editing rule 400 as 99 Web page no accessible since rule 99 deny the traffic before rule 100 allow all traffic Edit inbound rules removing rule 99 and add new rule 400 to allow traffic on ports 1024-65535 yum update -y # it should works again``` Each network ACL also includes a rule whose rule number is an asterisk. This rule ensures that if a packet doesn't match any of the other numbered rules, it's denied. You can't modify or remove this rule: 100 All Traffic Allow 200 All Traffic Deny * All Traffic Deny","title":"Network ACLs"},{"location":"AWS/03-Network/#vpc-flowlogs","text":"VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. VPC Flow Logs can be created at the VPC, subnet, and ENI levels. you can enable flow logs if VPC is in the same aws account you can not edit flow logs not all ip traffic is monitored such as: Amazon DNS Server Windows license activation 169.254.169.254 (metadata) DHCP traffic VPC reserved ports How to configure VPC FlowLogs: Create VPC with ipv4 CIDR block as 10.0.0.0/16 Create subnet with ipv4 CIDR block as 10.0.1.0/24 and enable auto-assign public IPv4 address Launch EC2 instance with VPC, subnet and a new SG with HTTP and SSH rules Create IG and associate it to the default VPC RT Create flow log in the ENI (Elastic Network interface) specify the filter: accepted traffic only rejected traffic only capture all traffic set Maximum aggregation interval to 1 min flow log data destination can be CloudWatch Logs or S3 bucket. Select S3 bucket Create bucket as arn:aws:s3:::rnietoeflowlogs Specify AWS default format Create log group from CloudWatch as VPCFlowLogs Create flow log again: specify the filter with all the traffic set Maximum aggregation interval to 1 min set flow log data destination as CloudWatch Logs select the destination log group as VPCFlowLogs set up permissions to define the IAM role that has permission to publish to the Amazon CloudWatch log group. Create new role as flowlogsRole with the following policy: { \"Statement\" : [ { \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:CreateLogStream\" , \"logs:DescribeLogGroups\" , \"logs:DescribeLogStreams\" , \"logs:PutLogEvents\" ], \"Effect\" : \"Allow\" , \"Resource\" : \"*\" } ] } Go to CloudWatch : Log groups : VPCFlowLogs : Log stream and check results Create filter pattern as (protocol 6 is TCP): [version, account, eni, source, destination, srcport, destport=\"22\", protocol=\"6\", packets, bytes, windowstart, windowend, action=\"ACCEPT\", flowlogstatus] test pattern 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 22 6 1 40 1563108188 1563108227 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 182.68.238.8 172.31.83.158 42227 22 6 1 44 1563109030 1563109067 REJECT OK 2 086112738802 eni-0d5d75b41f9befe9e 42.171.23.181 172.31.83.158 52417 22 6 24 4065 1563191069 1563191121 ACCEPT OK 2 086112738802 eni-0d5d75b41f9befe9e 61.177.172.128 172.31.83.158 39611 80 6 1 40 1563108188 1563108227 REJECT OK set metric details Create alarm for above metric filter set period to 1 min Whenever SSHAccept is greater/equal than 1 Run query from CloudWatch Logs Insights using VPC Flow Logs sample queries (right panel) Go back to S3 and check a new folder named AWSLogs has been created in our rnietoeflowlogs bucket Go to AWS Athena and set up a query result location in AWS S3 with the arn: s3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/2020/12/12/ Run query to create Athena Table CREATE EXTERNAL TABLE IF NOT EXISTS default . vpc_flow_logs ( version int , account string , interfaceid string , sourceaddress string , destinationaddress string , sourceport int , destinationport int , protocol int , numpackets int , numbytes bigint , starttime int , endtime int , action string , logstatus string ) PARTITIONED BY ( dt string ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' LOCATION 's3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/' TBLPROPERTIES ( \"skip.header.line.count\" = \"1\" ); Create Partitions ALTER TABLE default . vpc_flow_logs ADD PARTITION ( dt = '2020-12-12' ) location 's3://rnietoeflowlogs/AWSLogs/065275835852/vpcflowlogs/us-east-1/2020/12/12' ; Analyze Data SELECT day_of_week ( from_iso8601_timestamp ( dt )) AS day , dt , interfaceid , sourceaddress , destinationport , action , protocol FROM vpc_flow_logs WHERE action = 'ACCEPT' AND protocol = 6 order by sourceaddress LIMIT 100 ;","title":"VPC FlowLogs"},{"location":"AWS/03-Network/#bastion-host","text":"a bastion (jump box) is used to securely administer/manage EC2 instances in the private subnet using SSH or RDP NAT gateway cannot be used as a bastion host For high availability: create two public subnets in two different AZs with a bastion host each, and a Network Load Balancer with static IP address or Auto scaling group (cheaper) cannot use an Application Load Balancer as it is layer 7 and you need to use layer 4.","title":"Bastion host"},{"location":"AWS/03-Network/#direct-connect","text":"Dedicated line from on premise to AWS to improve the (VPN) network connection (security and performance) from 1 Gbps to 10 Gbps Create virtual interface from AWS Direct Connect : Virtual interfaces as Public Create Customer Gateway from VPC : Customer Gateways Create Virtual Private Gateway from VPC : Virtual Private Gateways attach the VPG to the VPC Create VPN Connection from VPC : Site-to-Site VPN Connections using the VPG and Customer Gateway Set up the VPN on the customer gateway How do I configure a VPN over AWS Direct Connect?","title":"Direct Connect"},{"location":"AWS/03-Network/#global-accelerator","text":"improves the availability and performance of your applications for local or global users 1234567890abcdef.awsglobalaccelerator.com Create accelerator from AWS Global Accelerator Add a listener to checks for connection requests that arrive to an assigned set of static IP addresses on a port or port range that you specify. (80, 443) Leave client affinity setting as none Add endpoint groups where the accelerator direct traffic to from one or more listeners. An endpoint group includes endpoints, such as load balancers. Add endpoints to each endpoint group Endpoints can be Network Load Balancers, Application Load Balancers, EC2 instances, or Elastic IP addresses. two static IP addresses are assigned. Disable Global Accelerator before removing is required Network zones are simliar to AZs. They are isolated units with their own set of physical infrastructure and service IP addresses from a unique IP subnet. If one IP address from a network zone becomes unavailable, due to network disruptions or IP address blocking by certain client networks, your client applications can retry using the healthy static IP address from the other isolated network zone.","title":"Global Accelerator"},{"location":"AWS/03-Network/#vpc-endpoints","text":"VPC endpoint is a service that replace NAT gateway and allow connections from the private subnet to other AWS services, such as S3 Interface endpoints: ENI (Elastic Network Interface) with private IP as entry point Gateway endpoints support S3 and DynamoDB Create Endpoint from VPC : Endpoints select service com.amazonaws.us-east-2.s3 , our VPC, our main route table and the full access policy the update in the route table could take some time","title":"VPC endpoints"},{"location":"AWS/03-Network/#aws-private-link","text":"To open up our apps to other VPCs, we can try: if open up the VPC to the internet. everything will be public you can use VPC peering . However, many relationships will be required AWS Private Link peers many VPCs. They only require a NLB on the AWS VPC and a ENI on the customer VPC","title":"AWS Private Link"},{"location":"AWS/03-Network/#transit-gateway","text":"TGW (Transit GateWay) is a network transit hub that interconnects attachments (VPCs and VPNs) within the same account or across accounts. Cross region is allowed support IP multicast","title":"Transit Gateway"},{"location":"AWS/03-Network/#vpn-cloudhub","text":"AWS VPN CloudHub manage multiple sites with own VPN connections","title":"VPN CloudHub"},{"location":"AWS/03-Network/#route-53","text":"AWS Route 53 service name comes from port 53, where DNS (Domain Name System) work on we can register a DNS using Route53 - Register domain . You can purchase and manage domain names such as example.com, and Route 53 will automatically configure DNS settings for your domains Ensure there is a free bucket with the same domain name Failover Routing policy routes data to a second resource if the first is unhealthy. Route 53 can be used for Disaster Recovery by simply shifting traffic to the new region. Latency-based Routing policy routes data to resources that have better performance Route 53 Traffic Flow makes it easy for you to manage traffic globally through a variety of routing types. Using Route 53 Traffic Flow\u2019s simple visual editor, you can easily manage how your end-users are routed to your application\u2019s endpoints\u2014whether in a single AWS region or distributed around the globe. .com => NS (Name Server) Records => SOA (Start Of Authority) DNS changes can take 48 hours to take effect due to the cache CName (Canonical Name) maps to the host name: https://mobile.acloud.guru = https://m.acloud.guru Alias Record provide a Route 53\u2013specific extension to DNS functionality. An alias could be created for the ELB. Alias Records can also point to AWS Resources that are hosted in other accounts by manually entering the ARN ELB resolve DNS names instead of IPv4 addresses Routing Policies: Simple Routing: one dns record with multiple IP addresses Weighted Routing: traffic based on weighting (20%-30%-50%) Latency-based Routing: traffic based on the lowest latency Failover Routing: route the traffic to the primary or secondary site defined based on health checks Geolocation Routing: traffic based on the user's location Geoproximity Routing: traffic based on the users' and resources' location. Available in traffic flow-only mode using bias Multivalue Answer Routing, similar to simple routing, but using health checks on each record sets to serve traffic to random web servers Using Route53: Register domain from Route53 takes between 2 hours and 3 days Create Record Set of type IPv4 address with the three EC2 public IPs. Set TTL (Time to Live) to 1 min to clear from cache Create healthcheck and associate it to each record set, so it will be removed from Route53 until it passes the health check Create traffic policy to configure Geoproximity Routing ipconfig /flushdns # to remove saved ip from cache from the client side With Route 53, there is a default limit of 50 domain names. However, this limit can be increased by contacting AWS support","title":"Route 53"},{"location":"AWS/03-Network/#api-gateway","text":"API Gateway vs Application Load Balancer API Gateway + Lambda + DynamoDB (serverless) instead of ELB + EC2 + RDS API (Application Programming Interface) Gateway is like a door for your AWS environment. Targets are: Lamda EC2 DynamoDB scaling is automatic (as aws lambda) enable api gateway caching to cache API gateway endpoint's responses for a TTL period in seconds. If a cache is configured, then Amazon API Gateway will return a cached response for duplicate requests for a customizable time, but only if under configured throttling limits. same origin policy to prevent cross site scripting (XSS) attacks. CORS (Cross Origin Resource Sharing) allow restricted resources in a web page to be requested from a different domain. Enable CORS in Apigateway when the error message is \"Origin policy cannot be read at the remote resource\" .","title":"API Gateway"},{"location":"AWS/04-Compute/","text":"4. Compute \u00b6 EC2 \u00b6 AWS EC2 (Elastic Compute Cloud) is a VM in the cloud. It is a web service that provides resizeable compute capacity in the cloud. AWS originally used a modified version of the Xen Hypervisor to host EC2. In 2017, AWS began rolling out their own Hypervisor called Nitro . Resource Optimization gives recommendations to help saving money Host recovery restarts EC2 instances when a problem is detected or when a new host is available Placement groups \u00b6 Clustered : Group homogenous EC2 instances within a single AZ for network performance Spread : Individial EC2 instances are placed on distinct rack within one region for HW errors you can only have a maximum of 7 running instances per AZ Partitioned : Multiple EC2 instances in the same rack EC2 fleet - multiple EC2 instances managed by AWS System Manager Spot fleet - multiple Spot (and on demand) instances The name of your placement group must be unique within your AWS Account There is no charge for creating a placement group Instance types \u00b6 General Purpose por basic apps: T2 (burst performance), M5 , M4 and M3 . They provie balance of memory and CPU Compute Optimized for CPU intensive apps: C5 , C5n C4 and C3 Memory Optimized for eXtreme memory requirements: X1e , X1 , R4 and R3 Storage Optimized for high Input/Output access: H1 , I3 , I3en and D2 Advance Computing for hardware compute requirements: P3 , P2 , G3 and F1 Change the instance type required stopping the instance previously. Pricing models: \u00b6 On Demand : low cost, paying by hour or second. You have full control over its lifecycle\u2014you decide when to launch, stop, hibernate, start, reboot, or terminate it. Sample: when task run uninterrupted from start to finish Reserved : the most economical option for long-term workloads with predictable usage patterns. Contract terms are 1 to 3 years. It includes different discounts Standard Reserved instances (75% off on demand instances). Cannot be moved between regions. You can choose if a Reserved Instance applies to either a specific AZ, or an entire Region, but you cannot change the region. Convertible Reserved instances (54%) Schedule Reserved instances, based on times Spot : taket advantage of unused EC2 capacity. It can accept interruptions. Used for various stateless, fault-tolerant, or flexible applications such as big data, containerized workloads, CI/CD, web servers, HPC (high-performance computing), and other test & development workloads. Extra charge when you terminate the instance Spot Instances are available at up to a 90% discount compared to On-Demand prices. It is possible that your Spot Instance is terminated before the warning can be made available. In rare situations, Spot blocks may be interrupted due to Amazon EC2 capacity needs. In these cases, AWS provides a two-minute warning before the instance is terminated, and customers are not charged for the terminated instances even if they have used them. Capacity Rebalancing helps you maintain workload availability by proactively augmenting your fleet with a new Spot Instance before a running Spot Instance receives the two-minute Spot Instance interruption notice. When Capacity Rebalancing is enabled, Auto Scaling or Spot Fleet attempts to proactively replace Spot Instances that have received a rebalance recommendation, providing the opportunity to rebalance your workload to new Spot Instances that are not at elevated risk of interruption. Capacity Rebalancing complements the capacity optimized allocation strategy (which is designed to help find the most optimal spare capacity) and the mixed instances policy (which is designed to enhance availability by deploying instances across multiple instance types running in multiple AZs). Dedicated : physical EC2 server. It reduces cost using your SW licenses (importing your licensed VM). Also when multitenant not supported by law (compliance) Tenancy: Shared : (default) multiple customers share the hardware instance. Cost reduced Dedicated instance: (not free) launch the instance in a dedicated host for one customer, but it may be moved to other of my hosts on restart Dedicated host : launch the instance in a dedicated host for one customer EC2 Pricing depends on: Clock hours of server time instance type pricing model (on demand, reserved, spot, dedicated host) number of instances load balancing detailed monitoring auto scaling Elastic IP Addresses Operative Systems and sw packages Windows instances are billed by the full hour, and partial hours are billed as full hours AMI \u00b6 AMI (Amazon Machine Image) are instance image snapshots of different Operative System public for everyone explicit for specified accounts implicit for the owner only (default) AMI are based on region, OS, architecture (32 or 64 bits), launch permissions and storage for the root volume (EBS or Instance store - ephemeral storage) EC2 instance with Instance Store can't be stopped Reboot does not cause the data to be deleted on an instance store volume Instance Store does not appear in the AWS EC2 Volume list To use hibernation , the root volume must be an encrypted EBS volume and RAM must be less than 150gb AWS does not copy launch permissions, tags, or SG rules from the source AMI to the new AMI. You must first deregister the AMI before you can delete the snapshot Snapshots are incremental You can use AWS APIs, CLI or the AWS Console to copy snapshots, share snapshots, and create volumes from snapshots. Volumes exist on EBS. Snapshots and instance store exist on S3. aws ec2 create-snapshot Use snapshots and AMI to change EC2 volumes (AZ and encryption). How to create EC2: \u00b6 From AWS EC2, Launch instance Select AMI, for example: Amazon Linux 2 AMI Choose an Instance Type (t2 micro - free tier) Configure Instance Details (number of instances) Enable CloudWatch detailed monitoring is not free Add Storage. Root and EBS volume types allow encryption and delete on termination (turn off by default on EBS) EC2 instance and volume are in the same AZ Add tags like Name, Department or Employee_Id Configure Security Groups : virtual firewalls to enable traffic - disabled by default SG support \"allow\" rules only. Use Network ACL to block specific IPs instead of SG All inbound traffic ( ingress ) is blocked by default and all outbound traffic ( egrees ) is allowed SG are stateful (no inbound traffic is allowed without request while all outbound traffic is allowed) network ACLs are stateless (outbound traffic must be specified) Security groups operate at the instance level, not to subnet as NACL Linux=SSH port 22. Microsoft Windows=RDP (Remote Desktop Protocol) port 3389. http/https ports 80/443 default SG for linux instances brings already inbound with SSH. Default SG for windows instances bring RDP SG changes are take effect immediately one or more SG can be assigned to EC2 instance. EC2 and SG relation is many to many a maximum of 5 SGs per instance Launch using a key pair (public and private key) Always design for failure. Have one EC2 instance in each AZ, using clustering and backups implement elasticity with auto scaling, ELB, decoupling applications and running tasks in parallel How to connect to EC2 \u00b6 Using AWS Console, select the EC2 instance and clic on the Connect button. Password required Using putty: Download putty and load PK file created at EC2/Key Pairs (rnietoe.ppk) using putty gen. Key pair can created or imported in AWS. You can create you key pair with the following command, keep the PK and import the public key in AWS: ssh-keygen -C rnietoe@gmail.com -f ~/.ssh/rnietoe Configure SSH Auth with PK Copy IP address to the session host name field Open connection, login as ec2-user (default user) and type sudo su command Using gitbash and install a web server: cd \"C:\\Users\\rniet\\OneDrive\\AWS\" CHMOD 400 rnietoe.pem # change permissions to unlock my key down ssh ec2-user@3.80.39.184 -i rnietoe.pem sudo su uname -a # software details cat ~/.ssh/authorized_keys # the public key while true ; do echo ; done # to monitor the CPU usage Using CLI Download the windows installer from AWS Command Line Interface and installe it Now we have the aws command in our prompt Configure IAM User with Programmatic access Download the access key and the secret access key: aws configure aws configure --profile profile_name # when we want to work with cli profiles # once typep the access key, the secret access key, the default region and the output format (json/text) aws ec2 describe-regions # use :q to exit from command output aws <command> --profile <profile_name> # when we want to execute commands with a specific profile set AWS_PROFILE = <profile_name> # set/unset default profile echo $AWS_PROFILE aws sts get-caller-identity # print out account and user info cat ~/.aws/config # file containing profile configuration cat ~/.aws/credentials # file containing profile credentials rm -rf ~/.aws We must use roles for security reasons instead of saving credentials (anyone could access to the .aws directory). Roles are global, they are not specified any region. Create a role to allows EC2 to use S3 as an admin: Go to IAM/Roles and create a new role Select EC2 as the type of trusted entity Attach AmazonS3FullAccess permissions policies Go to EC2 , select the instance and actions/instance settings/attach/replace iam role Then we can delete .aws directory with credential and still running aws s3 ls How to create a static website on S3 \u00b6 aws configure # enter user\u00b4s accesskeys and us-east-1 as default region aws s3 mb s3://rnietoe2 # make bucket command aws s3 ls # list all s3 instances echo \"hello world\" > hello_world.txt aws s3 cp hello_world.txt s3://rnietoe2 # upload: ./hello_world.txt to s3://rnietoe2/hello_world.txt cd ~ # go to home directory cd .aws # go the hidden directory nano credentials # display access keys More details to create Bootstrap actions to install additional software are here bootstrapping: providing code to be run on an instance at launch How to build a web server \u00b6 sudo su yum update -y # check for updates yum install httpd -y # install apache cd /var/www/html # create index.html in this path nano index.html <html><body><h1>This is server 1 </h1></body></html> service httpd start # sudo systemctl start httpd # start apache service chkconfig on # start apache on restarts Elastic Beanstalk \u00b6 AWS Elastic Beanstalk deploy and manage applications in the AWS cloud without worrying about the infrastructure that runs those applications. Some platforms are: docker multi-container docker tomcat (php and apache) .net (windows and iis) python ... you cannot change the environment tier after creating an environment Create Application from Elastic Beanstalk Select PHP as platform and the sample application code Created environment includes: S3 bucket LB / Target group Security group (virtual firewall) EIP EC2 instance Launch Configuration / Auto Scaling CloudWatch alarm ... mkdir helloworld cd hellowworld eb init -p PHP echo \"Hello World\" > index.html eb create dev-env eb open eb deploy # to deploy updates to the applications Retrieving instance metadata \u00b6 Connect to the EC2 instance and get instance metadata to get information about an instance using ip address 169.254.169.254 : curl http://169.254.169.254/latest/user-data curl http://169.254.169.254/latest/user-data > bootstrap.txt cat bootstrap.txt curl http://169.254.169.254/latest/meta-data curl http://169.254.169.254/latest/meta-data/local-ipv4 curl http://169.254.169.254/latest/meta-data/public-ipv4 Lambda \u00b6 AWS Lambda is the FaaS (Function-as-a-Service) to run your code globally without provisioning or managing servers (Serverless). Lambda can be used for Infrastructure as Code. Lambda, EC2 and ECS supports hyper-threading on one or more virtual CPUs. You can use JSON or YAML for Lambda templates. The resources section is the only required field in Lambda templates. Scales out automatically . (for example, 5 lambda replications running at the same time). Each time your function is triggered, a new and separate instance of that function is started. There are limits, but these can be adjusted on request. when creating a lambda function, a role is required to provide credentials with rights to other services. This is exactly the same as needing a Role on an EC2 instance to access S3 or DDB. Create a new role selecting the Simple microservice permissions policy template. different services can trigger your function, such as Api Gateway a lambda function can trigger other lambda functions ALB, Cognito, Lex, Alexa, API Gateway, CloudFront, and Kinesis Data Firehose are all valid direct (synchronous) triggers for Lambda functions. S3 is one of the valid asynchronous triggers. Pricing: number of request 1 million request per month free 0,20$ next million requests duration (how long lambda functions are executing for) 4000.000 gb-seconds per month free, up to 3,2 million seconds of compute time 0,00001667 for every GB second used thereafter The amount of memory assigned. additional charges when using other AWS services languages supported: node.js java c# Go Python lambda python sample: def lambda_handler ( event , context ): print ( \"In lambda handler\" ) resp = { \"statusCode\" : 200 , \"headers\" : { \"Access-Control-Allow-Origin\" : \"*\" , }, \"body\" : \"Hello world\" } return resp triggered in a html: < html > < script > function myFunction () { var xhttp = new XMLHttpRequest (); xhttp . onreadystatechange = function () { if ( this . readyState == 4 && this . status == 200 ) { document . getElementById ( \"my-demo\" ). innerHTML = this . responseText ; } }; xhttp . open ( \"GET\" , \"YOUR-API-GATEWAY-LINK-HERE\" , true ); xhttp . send (); } </ script > < body > < div align = \"center\" > < br >< br >< br >< br > < h1 > Hello < span id = \"my-demo\" > Cloud Gurus! </ span ></ h1 > < button onclick = \"myFunction()\" > Click me </ button >< br > < img src = \"https://s3.amazonaws.com/acloudguru-opsworkslab-donotdelete/ACG_Austin.JPG\" > </ div > </ body > </ html > Lambda Troubleshooting Batch \u00b6 AWS Batch enables you to easily and efficiently run batch computing jobs of any scale on AWS using on-demand and Spot EC2. like hangfire??","title":"4. Compute"},{"location":"AWS/04-Compute/#4-compute","text":"","title":"4. Compute"},{"location":"AWS/04-Compute/#ec2","text":"AWS EC2 (Elastic Compute Cloud) is a VM in the cloud. It is a web service that provides resizeable compute capacity in the cloud. AWS originally used a modified version of the Xen Hypervisor to host EC2. In 2017, AWS began rolling out their own Hypervisor called Nitro . Resource Optimization gives recommendations to help saving money Host recovery restarts EC2 instances when a problem is detected or when a new host is available","title":"EC2"},{"location":"AWS/04-Compute/#placement-groups","text":"Clustered : Group homogenous EC2 instances within a single AZ for network performance Spread : Individial EC2 instances are placed on distinct rack within one region for HW errors you can only have a maximum of 7 running instances per AZ Partitioned : Multiple EC2 instances in the same rack EC2 fleet - multiple EC2 instances managed by AWS System Manager Spot fleet - multiple Spot (and on demand) instances The name of your placement group must be unique within your AWS Account There is no charge for creating a placement group","title":"Placement groups"},{"location":"AWS/04-Compute/#instance-types","text":"General Purpose por basic apps: T2 (burst performance), M5 , M4 and M3 . They provie balance of memory and CPU Compute Optimized for CPU intensive apps: C5 , C5n C4 and C3 Memory Optimized for eXtreme memory requirements: X1e , X1 , R4 and R3 Storage Optimized for high Input/Output access: H1 , I3 , I3en and D2 Advance Computing for hardware compute requirements: P3 , P2 , G3 and F1 Change the instance type required stopping the instance previously.","title":"Instance types"},{"location":"AWS/04-Compute/#pricing-models","text":"On Demand : low cost, paying by hour or second. You have full control over its lifecycle\u2014you decide when to launch, stop, hibernate, start, reboot, or terminate it. Sample: when task run uninterrupted from start to finish Reserved : the most economical option for long-term workloads with predictable usage patterns. Contract terms are 1 to 3 years. It includes different discounts Standard Reserved instances (75% off on demand instances). Cannot be moved between regions. You can choose if a Reserved Instance applies to either a specific AZ, or an entire Region, but you cannot change the region. Convertible Reserved instances (54%) Schedule Reserved instances, based on times Spot : taket advantage of unused EC2 capacity. It can accept interruptions. Used for various stateless, fault-tolerant, or flexible applications such as big data, containerized workloads, CI/CD, web servers, HPC (high-performance computing), and other test & development workloads. Extra charge when you terminate the instance Spot Instances are available at up to a 90% discount compared to On-Demand prices. It is possible that your Spot Instance is terminated before the warning can be made available. In rare situations, Spot blocks may be interrupted due to Amazon EC2 capacity needs. In these cases, AWS provides a two-minute warning before the instance is terminated, and customers are not charged for the terminated instances even if they have used them. Capacity Rebalancing helps you maintain workload availability by proactively augmenting your fleet with a new Spot Instance before a running Spot Instance receives the two-minute Spot Instance interruption notice. When Capacity Rebalancing is enabled, Auto Scaling or Spot Fleet attempts to proactively replace Spot Instances that have received a rebalance recommendation, providing the opportunity to rebalance your workload to new Spot Instances that are not at elevated risk of interruption. Capacity Rebalancing complements the capacity optimized allocation strategy (which is designed to help find the most optimal spare capacity) and the mixed instances policy (which is designed to enhance availability by deploying instances across multiple instance types running in multiple AZs). Dedicated : physical EC2 server. It reduces cost using your SW licenses (importing your licensed VM). Also when multitenant not supported by law (compliance) Tenancy: Shared : (default) multiple customers share the hardware instance. Cost reduced Dedicated instance: (not free) launch the instance in a dedicated host for one customer, but it may be moved to other of my hosts on restart Dedicated host : launch the instance in a dedicated host for one customer EC2 Pricing depends on: Clock hours of server time instance type pricing model (on demand, reserved, spot, dedicated host) number of instances load balancing detailed monitoring auto scaling Elastic IP Addresses Operative Systems and sw packages Windows instances are billed by the full hour, and partial hours are billed as full hours","title":"Pricing models:"},{"location":"AWS/04-Compute/#ami","text":"AMI (Amazon Machine Image) are instance image snapshots of different Operative System public for everyone explicit for specified accounts implicit for the owner only (default) AMI are based on region, OS, architecture (32 or 64 bits), launch permissions and storage for the root volume (EBS or Instance store - ephemeral storage) EC2 instance with Instance Store can't be stopped Reboot does not cause the data to be deleted on an instance store volume Instance Store does not appear in the AWS EC2 Volume list To use hibernation , the root volume must be an encrypted EBS volume and RAM must be less than 150gb AWS does not copy launch permissions, tags, or SG rules from the source AMI to the new AMI. You must first deregister the AMI before you can delete the snapshot Snapshots are incremental You can use AWS APIs, CLI or the AWS Console to copy snapshots, share snapshots, and create volumes from snapshots. Volumes exist on EBS. Snapshots and instance store exist on S3. aws ec2 create-snapshot Use snapshots and AMI to change EC2 volumes (AZ and encryption).","title":"AMI"},{"location":"AWS/04-Compute/#how-to-create-ec2","text":"From AWS EC2, Launch instance Select AMI, for example: Amazon Linux 2 AMI Choose an Instance Type (t2 micro - free tier) Configure Instance Details (number of instances) Enable CloudWatch detailed monitoring is not free Add Storage. Root and EBS volume types allow encryption and delete on termination (turn off by default on EBS) EC2 instance and volume are in the same AZ Add tags like Name, Department or Employee_Id Configure Security Groups : virtual firewalls to enable traffic - disabled by default SG support \"allow\" rules only. Use Network ACL to block specific IPs instead of SG All inbound traffic ( ingress ) is blocked by default and all outbound traffic ( egrees ) is allowed SG are stateful (no inbound traffic is allowed without request while all outbound traffic is allowed) network ACLs are stateless (outbound traffic must be specified) Security groups operate at the instance level, not to subnet as NACL Linux=SSH port 22. Microsoft Windows=RDP (Remote Desktop Protocol) port 3389. http/https ports 80/443 default SG for linux instances brings already inbound with SSH. Default SG for windows instances bring RDP SG changes are take effect immediately one or more SG can be assigned to EC2 instance. EC2 and SG relation is many to many a maximum of 5 SGs per instance Launch using a key pair (public and private key) Always design for failure. Have one EC2 instance in each AZ, using clustering and backups implement elasticity with auto scaling, ELB, decoupling applications and running tasks in parallel","title":"How to create EC2:"},{"location":"AWS/04-Compute/#how-to-connect-to-ec2","text":"Using AWS Console, select the EC2 instance and clic on the Connect button. Password required Using putty: Download putty and load PK file created at EC2/Key Pairs (rnietoe.ppk) using putty gen. Key pair can created or imported in AWS. You can create you key pair with the following command, keep the PK and import the public key in AWS: ssh-keygen -C rnietoe@gmail.com -f ~/.ssh/rnietoe Configure SSH Auth with PK Copy IP address to the session host name field Open connection, login as ec2-user (default user) and type sudo su command Using gitbash and install a web server: cd \"C:\\Users\\rniet\\OneDrive\\AWS\" CHMOD 400 rnietoe.pem # change permissions to unlock my key down ssh ec2-user@3.80.39.184 -i rnietoe.pem sudo su uname -a # software details cat ~/.ssh/authorized_keys # the public key while true ; do echo ; done # to monitor the CPU usage Using CLI Download the windows installer from AWS Command Line Interface and installe it Now we have the aws command in our prompt Configure IAM User with Programmatic access Download the access key and the secret access key: aws configure aws configure --profile profile_name # when we want to work with cli profiles # once typep the access key, the secret access key, the default region and the output format (json/text) aws ec2 describe-regions # use :q to exit from command output aws <command> --profile <profile_name> # when we want to execute commands with a specific profile set AWS_PROFILE = <profile_name> # set/unset default profile echo $AWS_PROFILE aws sts get-caller-identity # print out account and user info cat ~/.aws/config # file containing profile configuration cat ~/.aws/credentials # file containing profile credentials rm -rf ~/.aws We must use roles for security reasons instead of saving credentials (anyone could access to the .aws directory). Roles are global, they are not specified any region. Create a role to allows EC2 to use S3 as an admin: Go to IAM/Roles and create a new role Select EC2 as the type of trusted entity Attach AmazonS3FullAccess permissions policies Go to EC2 , select the instance and actions/instance settings/attach/replace iam role Then we can delete .aws directory with credential and still running aws s3 ls","title":"How to connect to EC2"},{"location":"AWS/04-Compute/#how-to-create-a-static-website-on-s3","text":"aws configure # enter user\u00b4s accesskeys and us-east-1 as default region aws s3 mb s3://rnietoe2 # make bucket command aws s3 ls # list all s3 instances echo \"hello world\" > hello_world.txt aws s3 cp hello_world.txt s3://rnietoe2 # upload: ./hello_world.txt to s3://rnietoe2/hello_world.txt cd ~ # go to home directory cd .aws # go the hidden directory nano credentials # display access keys More details to create Bootstrap actions to install additional software are here bootstrapping: providing code to be run on an instance at launch","title":"How to create a static website on S3"},{"location":"AWS/04-Compute/#how-to-build-a-web-server","text":"sudo su yum update -y # check for updates yum install httpd -y # install apache cd /var/www/html # create index.html in this path nano index.html <html><body><h1>This is server 1 </h1></body></html> service httpd start # sudo systemctl start httpd # start apache service chkconfig on # start apache on restarts","title":"How to build a web server"},{"location":"AWS/04-Compute/#elastic-beanstalk","text":"AWS Elastic Beanstalk deploy and manage applications in the AWS cloud without worrying about the infrastructure that runs those applications. Some platforms are: docker multi-container docker tomcat (php and apache) .net (windows and iis) python ... you cannot change the environment tier after creating an environment Create Application from Elastic Beanstalk Select PHP as platform and the sample application code Created environment includes: S3 bucket LB / Target group Security group (virtual firewall) EIP EC2 instance Launch Configuration / Auto Scaling CloudWatch alarm ... mkdir helloworld cd hellowworld eb init -p PHP echo \"Hello World\" > index.html eb create dev-env eb open eb deploy # to deploy updates to the applications","title":"Elastic Beanstalk"},{"location":"AWS/04-Compute/#retrieving-instance-metadata","text":"Connect to the EC2 instance and get instance metadata to get information about an instance using ip address 169.254.169.254 : curl http://169.254.169.254/latest/user-data curl http://169.254.169.254/latest/user-data > bootstrap.txt cat bootstrap.txt curl http://169.254.169.254/latest/meta-data curl http://169.254.169.254/latest/meta-data/local-ipv4 curl http://169.254.169.254/latest/meta-data/public-ipv4","title":"Retrieving instance metadata"},{"location":"AWS/04-Compute/#lambda","text":"AWS Lambda is the FaaS (Function-as-a-Service) to run your code globally without provisioning or managing servers (Serverless). Lambda can be used for Infrastructure as Code. Lambda, EC2 and ECS supports hyper-threading on one or more virtual CPUs. You can use JSON or YAML for Lambda templates. The resources section is the only required field in Lambda templates. Scales out automatically . (for example, 5 lambda replications running at the same time). Each time your function is triggered, a new and separate instance of that function is started. There are limits, but these can be adjusted on request. when creating a lambda function, a role is required to provide credentials with rights to other services. This is exactly the same as needing a Role on an EC2 instance to access S3 or DDB. Create a new role selecting the Simple microservice permissions policy template. different services can trigger your function, such as Api Gateway a lambda function can trigger other lambda functions ALB, Cognito, Lex, Alexa, API Gateway, CloudFront, and Kinesis Data Firehose are all valid direct (synchronous) triggers for Lambda functions. S3 is one of the valid asynchronous triggers. Pricing: number of request 1 million request per month free 0,20$ next million requests duration (how long lambda functions are executing for) 4000.000 gb-seconds per month free, up to 3,2 million seconds of compute time 0,00001667 for every GB second used thereafter The amount of memory assigned. additional charges when using other AWS services languages supported: node.js java c# Go Python lambda python sample: def lambda_handler ( event , context ): print ( \"In lambda handler\" ) resp = { \"statusCode\" : 200 , \"headers\" : { \"Access-Control-Allow-Origin\" : \"*\" , }, \"body\" : \"Hello world\" } return resp triggered in a html: < html > < script > function myFunction () { var xhttp = new XMLHttpRequest (); xhttp . onreadystatechange = function () { if ( this . readyState == 4 && this . status == 200 ) { document . getElementById ( \"my-demo\" ). innerHTML = this . responseText ; } }; xhttp . open ( \"GET\" , \"YOUR-API-GATEWAY-LINK-HERE\" , true ); xhttp . send (); } </ script > < body > < div align = \"center\" > < br >< br >< br >< br > < h1 > Hello < span id = \"my-demo\" > Cloud Gurus! </ span ></ h1 > < button onclick = \"myFunction()\" > Click me </ button >< br > < img src = \"https://s3.amazonaws.com/acloudguru-opsworkslab-donotdelete/ACG_Austin.JPG\" > </ div > </ body > </ html > Lambda Troubleshooting","title":"Lambda"},{"location":"AWS/04-Compute/#batch","text":"AWS Batch enables you to easily and efficiently run batch computing jobs of any scale on AWS using on-demand and Spot EC2. like hangfire??","title":"Batch"},{"location":"AWS/05-Security/","text":"5. Security \u00b6 IAM \u00b6 AWS IAM (Identity and Access Management) enables you to manage access to AWS services and resources securely. Users, groups, roles and policies are managed globally (not for a specific a region). Identities include users, groups, and roles. These are the IAM resource objects that are used to identify and group. You can attach a policy to an IAM identity. A Principal is a person or application that uses the AWS account root user, an IAM user or an IAM role to sign in and make requests to AWS. You may have a 3rd party device that uses BioMetrics to initiate and exchange of the password or secret key with AWS, but that is not an AWS IAM service You can use permissions to allow and deny users and groups access to AWS resources. Managed policies. Attach readonly policies already defined in AWS Inline policies: Select a policy template, generate a policy, or create a custom policy. Not explicitly allowed = implicitly denied explicit deny > everything else Groups are a collection of users with specific permissions/policies Roles are a secure way to grant permissions to entities that you trust. Create role from IAM. Select EC2 as trusted entity to call AWS services on your behalf. Attach permission policy AmazonS3FullAccess Named as S3_Admin_Access aws iam create-role --role-name DEV_ROLE --assume-role-policy-document file://mypolicy.json To attach an IAM role to an instance that has no role, the instance can be in the stopped or running state. To replace the IAM role on an instance that already has an attached IAM role, the instance must be in the running state. Permissions boundary define the maximum permissions an identity can have, but it does apply any permission New users are assigned with Access Key Id and secret when first created to access AWS via the APIs and CLI. It's safer to use IAM roles than it is to use Access Keys. Power User Access allows access to all AWS services except the management of groups and users within IAM. New users have NO permissions when first created. When you manage IAM policies, follow the standard security advice of granting the least privilege , or granting only the permissions required to perform a task. Determine what users (and roles) need to do, and then craft policies that allow them to perform only those tasks. A Policy is the document used to grant permissions to users, groups, and roles, but it can not be attached directly to an application. Policies are written using JSON. AdministratorAccess Policy json: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } IAM policies applies to users and groups. IAM roles by another AWS account provide the aws link to other account. Switch role for cross account with Console access Changes to IAM Policies take effect almost immediately (with maybe a few seconds delay). policy conditions determine when a policy applies With the IAM Policy Simulator , you can test and troubleshoot identity-based policies, IAM permissions boundaries, Organizations service control policies, and resource-based policies. root user tasks: modify the root user change the AWS support plan close an AWS account Create a CloudFront key pair Enable MFA on an S3 bucket Restore permissions for other iAM users use user and pwd in the AWS console. Use key pair in the AWS API and CLI aws iam create-access-key --user-name Alice aws iam list-access-key --user-name Alice ARN \u00b6 Amazon Resource Name arn : partition : service : region : account_id : resouce_type : qualifier : resource : qualifier partition: aws|aws-cn service: s3|ec2|rds region: us-east-1|eu-central-1 (omitted when service is global such as iam) account_id: twelve digits (ommitted with service is globally unique, such as s3) resource_type: resource: qualifier: Security in the cloud \u00b6 AWS Artifact : Compliance and security reports in the AWS Cloud A PCI DSS Level 1 certification attests to the security of the AWS platform regarding credit card transactions . A HIPAA certification attests to the fact that the AWS Platform has met the standard required for the secure storage of medical records in the US A ISO certification for quality -controlled IT systems in the AWS cloud Shared responsability model talks about who is responsible for what in cloud AWS is responsible for the security OF the cloud. Customer is responsible for security IN the cloud Customer is responsible of patching EC2 instances. The customer would be responsible for patching the Operating System for IaaS solutions Encryption is a shared responsability If you can do in the aws console, you are the responsible Amazon Inspector to anayze and report security issues on EC2, but it can not examine individual policies Trusted Advisor for recomendations and advices (not only EC2 instances). It helps you optimize cost, fault-tolerance, and more. CloudTrail track user activity and API usage CloudWatch monitoring performance AWS Config monitor configuration settings Athena serverless service for querying data in S3 using SQL. Commonly used to analyse logs. It will work with a number of data formats including JSON, Apache Parquet, Apache ORC amongst others, but XML is not a format that is supported. Macie uses Machine learning to protect sensitive data (Personally Identifiable Information) stored in S3 Personal Health Dashboard helps you to inspect account alerts and find remediation guidance for your account Service Health Dashboard displays the general status of AWS services WAF (Web Application Firewall) \u00b6 AWS WAF block requests to stop hackers requests from: specific IP address (CloudFront, ALB or API Gateway) origin country request size request header values SQL code injection Cross scripting It works with CloudFront or ELB It operates down to Layer 7 (http/https). allow all requests except specified block all requests except specified Block traffic response is forbbiden (403) Shield \u00b6 AWS Shield protect a lot of traffic ( DDOS attacks). AWS Shield Standard: free protection in layers 3 and 4 AWS Shield Advanced: advance protection costs $3000/month. It offers automated application layer monitoring. Directory Service \u00b6 There are three options that help you migrate Active Directory -dependent applications to the AWS Cloud: Managed Microsoft AD - Directory Service for Microsoft Active Diretory Simple AD using linux samba AD Connector AWS Cognito These solutions also enable users to sign into AWS applications such as Amazon WorkSpaces and Amazon QuickSight with their AD credentials. Developers who don\u2019t need AD can use Amazon Cloud Directory to create cloud-scale directories that organize and manage hierarchical information such as organizational charts, course catalogs, and device registries. RAM (Resource Access Management) \u00b6 AWS RAM shares AWS resources with other AWS accounts. Create a resource share Choose the resources type to add to the resource share Select the resource id Add principals to the resource share. Principals can be AWS accounts, organizational units, or your organization. Add tags to the resource share. from the other account, browse to AWS RAM : Shared with me : Resource shares and accept the shared resource SSO (Single Sign-On) \u00b6 AWS SSO is a cloud service that makes it easy to manage SSO access to multiple AWS accounts and business applications. this require Active Directory and SAML (Security Assertion Markup Language) integration Cognito \u00b6 AWS Cognito is the SSO Solution in AWS providing Web Identity Federation (auth using Facebook , Google and Amazon) recomended for mobile apps log in store user profiles based on open standards as OAuth2.0 , SAML2.0, OpenID Connect users pool: JWT (Json Web Token) for registration, authentication and account recovery identity pool: temporary AWS credentials to access AWS recources Cognito uses Push synchronization to push updates and synchronize user data across multiple devices KMS (Key Management Service) \u00b6 AWS KMS manages encryption keys to encrypt data You can configure your application to use the KMS API to encrypt all data before saving it to disk. When to use AWS CMK (Customer Master Key) per region AWS Managed CMK: used by default; free AWS Owned CMK: used by AWS Customer Managed CMK: created by you. it allows key rotation : replace old access key Symmetric CMK default option same key for encryption and decryption AES-256 algoritm data never leaves AWS unencrypted KMS must be called for using Asymmetric CMK Public and Private key pair SSL RSA and ECC algoritm private key never leaves AWS unencrypted AWS services integrated with KMS does not support asymmetric CMKs aws kms create-key --description \"MyCMK\" # allow every action to the root user by default aws kms create-alias --target-key-id KEYID --alias-name \"alias/MyCMK\" echo \"hello world\" > test.txt aws kms encrypt --key-id KEYID \"alias/MyCMK\" --plaintext file://test.txt --output text --query CiphertextBlob aws kms encrypt --key-id KEYID \"alias/MyCMK\" --plaintext file://test.txt --output text --query CiphertextBlob | base64 --decode > test.txt.encrypted aws kms decrypt --ciphertext-blob fileb://test.txt.encrypted --output text --query Plaintext aws kms decrypt --ciphertext-blob fileb://test.txt.encrypted --output text --query Plaintext | base64 --decode FIPS 140-2 Level 2 KMS is multitenant CloudHSM (Hardware Security Model) \u00b6 manage your own encryption keys using FIPS 140-2 Level 3 single tenant (dedicated hardware) run within a VPC in your account When to use AWS CloudHSM Secrets Manager \u00b6 AWS Secrets Manager helps you protect access to your applications, services, and IT resources. You can easily rotate , manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. generate random secrets Security Hub \u00b6 AWS Security Hub is a not free service that provides a consolidated view of your security status in AWS: this runs automatic checks to scan for compliance with regulations and laws AWS GuardDuty is a IDS (intrusion detection system) AWS Inspector performs vulnerability analysis AWS Macie provides S3 bucket policy compliance scans for sensitive data","title":"5. Security"},{"location":"AWS/05-Security/#5-security","text":"","title":"5. Security"},{"location":"AWS/05-Security/#iam","text":"AWS IAM (Identity and Access Management) enables you to manage access to AWS services and resources securely. Users, groups, roles and policies are managed globally (not for a specific a region). Identities include users, groups, and roles. These are the IAM resource objects that are used to identify and group. You can attach a policy to an IAM identity. A Principal is a person or application that uses the AWS account root user, an IAM user or an IAM role to sign in and make requests to AWS. You may have a 3rd party device that uses BioMetrics to initiate and exchange of the password or secret key with AWS, but that is not an AWS IAM service You can use permissions to allow and deny users and groups access to AWS resources. Managed policies. Attach readonly policies already defined in AWS Inline policies: Select a policy template, generate a policy, or create a custom policy. Not explicitly allowed = implicitly denied explicit deny > everything else Groups are a collection of users with specific permissions/policies Roles are a secure way to grant permissions to entities that you trust. Create role from IAM. Select EC2 as trusted entity to call AWS services on your behalf. Attach permission policy AmazonS3FullAccess Named as S3_Admin_Access aws iam create-role --role-name DEV_ROLE --assume-role-policy-document file://mypolicy.json To attach an IAM role to an instance that has no role, the instance can be in the stopped or running state. To replace the IAM role on an instance that already has an attached IAM role, the instance must be in the running state. Permissions boundary define the maximum permissions an identity can have, but it does apply any permission New users are assigned with Access Key Id and secret when first created to access AWS via the APIs and CLI. It's safer to use IAM roles than it is to use Access Keys. Power User Access allows access to all AWS services except the management of groups and users within IAM. New users have NO permissions when first created. When you manage IAM policies, follow the standard security advice of granting the least privilege , or granting only the permissions required to perform a task. Determine what users (and roles) need to do, and then craft policies that allow them to perform only those tasks. A Policy is the document used to grant permissions to users, groups, and roles, but it can not be attached directly to an application. Policies are written using JSON. AdministratorAccess Policy json: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"*\" , \"Resource\" : \"*\" } ] } IAM policies applies to users and groups. IAM roles by another AWS account provide the aws link to other account. Switch role for cross account with Console access Changes to IAM Policies take effect almost immediately (with maybe a few seconds delay). policy conditions determine when a policy applies With the IAM Policy Simulator , you can test and troubleshoot identity-based policies, IAM permissions boundaries, Organizations service control policies, and resource-based policies. root user tasks: modify the root user change the AWS support plan close an AWS account Create a CloudFront key pair Enable MFA on an S3 bucket Restore permissions for other iAM users use user and pwd in the AWS console. Use key pair in the AWS API and CLI aws iam create-access-key --user-name Alice aws iam list-access-key --user-name Alice","title":"IAM"},{"location":"AWS/05-Security/#arn","text":"Amazon Resource Name arn : partition : service : region : account_id : resouce_type : qualifier : resource : qualifier partition: aws|aws-cn service: s3|ec2|rds region: us-east-1|eu-central-1 (omitted when service is global such as iam) account_id: twelve digits (ommitted with service is globally unique, such as s3) resource_type: resource: qualifier:","title":"ARN"},{"location":"AWS/05-Security/#security-in-the-cloud","text":"AWS Artifact : Compliance and security reports in the AWS Cloud A PCI DSS Level 1 certification attests to the security of the AWS platform regarding credit card transactions . A HIPAA certification attests to the fact that the AWS Platform has met the standard required for the secure storage of medical records in the US A ISO certification for quality -controlled IT systems in the AWS cloud Shared responsability model talks about who is responsible for what in cloud AWS is responsible for the security OF the cloud. Customer is responsible for security IN the cloud Customer is responsible of patching EC2 instances. The customer would be responsible for patching the Operating System for IaaS solutions Encryption is a shared responsability If you can do in the aws console, you are the responsible Amazon Inspector to anayze and report security issues on EC2, but it can not examine individual policies Trusted Advisor for recomendations and advices (not only EC2 instances). It helps you optimize cost, fault-tolerance, and more. CloudTrail track user activity and API usage CloudWatch monitoring performance AWS Config monitor configuration settings Athena serverless service for querying data in S3 using SQL. Commonly used to analyse logs. It will work with a number of data formats including JSON, Apache Parquet, Apache ORC amongst others, but XML is not a format that is supported. Macie uses Machine learning to protect sensitive data (Personally Identifiable Information) stored in S3 Personal Health Dashboard helps you to inspect account alerts and find remediation guidance for your account Service Health Dashboard displays the general status of AWS services","title":"Security in the cloud"},{"location":"AWS/05-Security/#waf-web-application-firewall","text":"AWS WAF block requests to stop hackers requests from: specific IP address (CloudFront, ALB or API Gateway) origin country request size request header values SQL code injection Cross scripting It works with CloudFront or ELB It operates down to Layer 7 (http/https). allow all requests except specified block all requests except specified Block traffic response is forbbiden (403)","title":"WAF (Web Application Firewall)"},{"location":"AWS/05-Security/#shield","text":"AWS Shield protect a lot of traffic ( DDOS attacks). AWS Shield Standard: free protection in layers 3 and 4 AWS Shield Advanced: advance protection costs $3000/month. It offers automated application layer monitoring.","title":"Shield"},{"location":"AWS/05-Security/#directory-service","text":"There are three options that help you migrate Active Directory -dependent applications to the AWS Cloud: Managed Microsoft AD - Directory Service for Microsoft Active Diretory Simple AD using linux samba AD Connector AWS Cognito These solutions also enable users to sign into AWS applications such as Amazon WorkSpaces and Amazon QuickSight with their AD credentials. Developers who don\u2019t need AD can use Amazon Cloud Directory to create cloud-scale directories that organize and manage hierarchical information such as organizational charts, course catalogs, and device registries.","title":"Directory Service"},{"location":"AWS/05-Security/#ram-resource-access-management","text":"AWS RAM shares AWS resources with other AWS accounts. Create a resource share Choose the resources type to add to the resource share Select the resource id Add principals to the resource share. Principals can be AWS accounts, organizational units, or your organization. Add tags to the resource share. from the other account, browse to AWS RAM : Shared with me : Resource shares and accept the shared resource","title":"RAM (Resource Access Management)"},{"location":"AWS/05-Security/#sso-single-sign-on","text":"AWS SSO is a cloud service that makes it easy to manage SSO access to multiple AWS accounts and business applications. this require Active Directory and SAML (Security Assertion Markup Language) integration","title":"SSO (Single Sign-On)"},{"location":"AWS/05-Security/#cognito","text":"AWS Cognito is the SSO Solution in AWS providing Web Identity Federation (auth using Facebook , Google and Amazon) recomended for mobile apps log in store user profiles based on open standards as OAuth2.0 , SAML2.0, OpenID Connect users pool: JWT (Json Web Token) for registration, authentication and account recovery identity pool: temporary AWS credentials to access AWS recources Cognito uses Push synchronization to push updates and synchronize user data across multiple devices","title":"Cognito"},{"location":"AWS/05-Security/#kms-key-management-service","text":"AWS KMS manages encryption keys to encrypt data You can configure your application to use the KMS API to encrypt all data before saving it to disk. When to use AWS CMK (Customer Master Key) per region AWS Managed CMK: used by default; free AWS Owned CMK: used by AWS Customer Managed CMK: created by you. it allows key rotation : replace old access key Symmetric CMK default option same key for encryption and decryption AES-256 algoritm data never leaves AWS unencrypted KMS must be called for using Asymmetric CMK Public and Private key pair SSL RSA and ECC algoritm private key never leaves AWS unencrypted AWS services integrated with KMS does not support asymmetric CMKs aws kms create-key --description \"MyCMK\" # allow every action to the root user by default aws kms create-alias --target-key-id KEYID --alias-name \"alias/MyCMK\" echo \"hello world\" > test.txt aws kms encrypt --key-id KEYID \"alias/MyCMK\" --plaintext file://test.txt --output text --query CiphertextBlob aws kms encrypt --key-id KEYID \"alias/MyCMK\" --plaintext file://test.txt --output text --query CiphertextBlob | base64 --decode > test.txt.encrypted aws kms decrypt --ciphertext-blob fileb://test.txt.encrypted --output text --query Plaintext aws kms decrypt --ciphertext-blob fileb://test.txt.encrypted --output text --query Plaintext | base64 --decode FIPS 140-2 Level 2 KMS is multitenant","title":"KMS (Key Management Service)"},{"location":"AWS/05-Security/#cloudhsm-hardware-security-model","text":"manage your own encryption keys using FIPS 140-2 Level 3 single tenant (dedicated hardware) run within a VPC in your account When to use AWS CloudHSM","title":"CloudHSM (Hardware Security Model)"},{"location":"AWS/05-Security/#secrets-manager","text":"AWS Secrets Manager helps you protect access to your applications, services, and IT resources. You can easily rotate , manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. generate random secrets","title":"Secrets Manager"},{"location":"AWS/05-Security/#security-hub","text":"AWS Security Hub is a not free service that provides a consolidated view of your security status in AWS: this runs automatic checks to scan for compliance with regulations and laws AWS GuardDuty is a IDS (intrusion detection system) AWS Inspector performs vulnerability analysis AWS Macie provides S3 bucket policy compliance scans for sensitive data","title":"Security Hub"},{"location":"AWS/06-Management/","text":"6. Management and Governance \u00b6 OpsWorks \u00b6 AWS OpsWorks implemet a configuration management system for automated deployment of instances, services and applications OpsWorks Stacks : (default) collection of layers on-premises OpsWorks for Chef Automate: cookbooks contain recipes /layers in the cloud OpsWorks for Puppet : master servers with preconfigured modules /layers in the cloud pre-built layers Ruby PHP Node.js Java Amazon RDS HA Proxy MySql EMR (Elastic MapReduce) \u00b6 web service that makes it easy to process large amounts of data efficiently, mapping the process to multiple processors to reduce computing . Sample of big data clusters nodes: master : coordinates job distribution and store logs by default core : run tasks assigned by the master node and store data in the cluster task (optional): runs only task that do not store data logs must be defined on cluster creation Export, Import, Query, and Join Tables in DynamoDB Using Amazon EMR Systems Manager \u00b6 View and manage AWS resources in the cloud (EC2 fleets) or on premise (virtual machines) SSM (Systems Manager) Parameter Store \u00b6 Centralized storage and management of your secrets and configuration data such as passwords, database strings, and license codes. You can encrypt values using KMS, or store as plain text, and secure access at every level. organize parameters into hierarchies Create Parameter from SSM Parameter Store { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:CreateLogStream\" , \"logs:PutLogEvents\" , \"ssm:GetParameter*\" , \"ssm:GetParametersByPath\" ], \"Resource\" : \"*\" } ] } import json import os import boto3 client = boto3 . client ( \"ssm\" ) env = os . environ [ \"ENV\" ] # env variable value = \"prod\" app_config_path = os . environ [ \"APP_CONFIG_PATH\" ] # env variable value = \"acg\" full_config_path = \"/\" + env + \"/\" + app_config_path # /prod/acg def lambda_handler ( event , context ): print ( \"Config Path: \" + full_config_path ) param_details = client . get_parameters_by_path ( Path = full_config_path , Recursive = True , WithDecryption = True ) print ( json . dumps ( param_details , default = str )) CloudWatch \u00b6 Create Alarms to Stop, Terminate, Reboot, or Recover an Instance Monitor resources and applications performances from the cloud and on-premises systems based on logs and events: compute ec2 instances - every 5 minutes by default autoscaling groups ELB Route53 health checks storage and content delivery EBS - virtual hard disk Storage gateways CloudFront network AWS can see that you have Memory, but a custom metric is required to see how much of the memory is being used AWS can see how much of CPU you are using in a EC2, but cannot see what you are using if for. CloudWatch logs insights adds cross log group querying CloudTrail \u00b6 Continuously log your AWS account activity monitoring API calls per account and region data is retained for 90 days logs can be stored in a single S3 bucket read activity in the S3 bucket will be easier than using the API Trusted Advisor \u00b6 Recomendations and advices (not only EC2 instances). It helps you optimize cost, fault-tolerance, perfoemance, security and more. Organisations \u00b6 AWS Organizations enables you to centrally manage billing, control access, compliance, and security, and share resources across multiple accounts in the AWS Cloud. You can consolidate all your AWS accounts into an organization, and arrange all AWS accounts into distinct OUs (Organizational Units). Provides single payer and centralized cost tracking Lets you create and invite accounts Allows you to apply policy-based controls Helps you simplify organization-wide management of AWS services Or you can create an organization with only consolidated billing features. After you create an organization, you cannot join this account to another organization until you delete its current organization. From AWS Organisations, create organisation Enable AWS Single Sign On to centrally manage access to multiple AWS accounts and business applications. Create an organization trail in AWS CloudTrail to log all events for all AWS accounts in your organization. From Organize Accounts Tab, we create a new organisational units From Policies tab, enable service control policies and create a sample policy to block EC2 usage. Select Amazon EC2 statement and deny effect Apply the new policy to organisational units or to AWS Accounts Maximum of 20 Link accounts. Contact AWS for more Assuming all instances are in the same AWS Organization, the reserved instance pricing for the unused on demand instances will be applied. Landing Zone helps to quickly setup a secure, multi-account AWS environment based on AWS best practices. services control policies for central control over all of the permissions AWS Config \u00b6 AWS Config provides an inventory of your AWS resources and a history of configuration changes to these resources. You can use AWS Config to define rules that evaluate these configurations for compliance. How to get started: Specify the types of AWS resources you want AWS Config to record Define the Amazon S3 bucket to which it sends files Set the Amazon SNS topic to which it sends notifications Define config rules Restart EC2 instances and check AWS Config results You are charged based on the number of configuration items recorded, the number of active AWS Config rule evaluations and the number of conformance pack evaluations in your account Pricing \u00b6 Capex: Capital Expenditure: you pay up front. It's a fixed cost Opex: Operational Expenditure: you pay for what you use, like electricity, gas or water Budgets predict costs before they are incurred. Alarms can be set to monitor spending on your AWS account from AWS Billing service - Budgets Receive Billing Alerts must be enabled at Billing Preferences . Cost explorer is use to explore costs after they have been incurred. See Billing & Cost Management Dashboard . Creating a billing alarm at ClouldWatch/Alarms/Billing using SNS (Simple Notification Service) topic to monitor estimated AWS charges. Application Integration/SNS is a messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Using AWS SNS topics, your publisher systems can fan-out messages to a large number of subscriber endpoints for parallel processing, including Amazon SQS queues, AWS Lambda functions, and HTTP/S webhooks. Additionally, SNS can be used to fan out notifications to end users using mobile push, SMS, and email. Application Integration/SQS (Simple Queue Service) offers a queue that lets you integrate and decouple distributed software systems and components. Support Plans: All accounts receive billing support Pricing policies pay as you go pay less when you reserve pay even less per unit by using more pay even less as AWS grows custom pricing cost drivers: compute storage data outbound (data leavingn, not data entering) free services VPC Elastic Beanstalk Cloud Formation IAM Autoscaling Opsworks Consolidated billing Create a paying account for billing purposes only. Do not deploy resources into the paying account. Consolidated billing allows you to get volume discounts on all your accounts Cost Calculators \u00b6 AWS Simple Monthly Calculator ( DEPRECATED ) AWS Total Cost of Ownership Calculator ( DEPRECATED ): comparing AWS VS on premise AWS Pricing Calculator Cost Explorer \u00b6 AWS Cost explorer is use to explore costs after they have been incurred. disable by default, it takes 24 hours to be enabled details are given before the end of the month bill...","title":"6. Management and Governance"},{"location":"AWS/06-Management/#6-management-and-governance","text":"","title":"6. Management and Governance"},{"location":"AWS/06-Management/#opsworks","text":"AWS OpsWorks implemet a configuration management system for automated deployment of instances, services and applications OpsWorks Stacks : (default) collection of layers on-premises OpsWorks for Chef Automate: cookbooks contain recipes /layers in the cloud OpsWorks for Puppet : master servers with preconfigured modules /layers in the cloud pre-built layers Ruby PHP Node.js Java Amazon RDS HA Proxy MySql","title":"OpsWorks"},{"location":"AWS/06-Management/#emr-elastic-mapreduce","text":"web service that makes it easy to process large amounts of data efficiently, mapping the process to multiple processors to reduce computing . Sample of big data clusters nodes: master : coordinates job distribution and store logs by default core : run tasks assigned by the master node and store data in the cluster task (optional): runs only task that do not store data logs must be defined on cluster creation Export, Import, Query, and Join Tables in DynamoDB Using Amazon EMR","title":"EMR (Elastic MapReduce)"},{"location":"AWS/06-Management/#systems-manager","text":"View and manage AWS resources in the cloud (EC2 fleets) or on premise (virtual machines)","title":"Systems Manager"},{"location":"AWS/06-Management/#ssm-systems-manager-parameter-store","text":"Centralized storage and management of your secrets and configuration data such as passwords, database strings, and license codes. You can encrypt values using KMS, or store as plain text, and secure access at every level. organize parameters into hierarchies Create Parameter from SSM Parameter Store { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:CreateLogStream\" , \"logs:PutLogEvents\" , \"ssm:GetParameter*\" , \"ssm:GetParametersByPath\" ], \"Resource\" : \"*\" } ] } import json import os import boto3 client = boto3 . client ( \"ssm\" ) env = os . environ [ \"ENV\" ] # env variable value = \"prod\" app_config_path = os . environ [ \"APP_CONFIG_PATH\" ] # env variable value = \"acg\" full_config_path = \"/\" + env + \"/\" + app_config_path # /prod/acg def lambda_handler ( event , context ): print ( \"Config Path: \" + full_config_path ) param_details = client . get_parameters_by_path ( Path = full_config_path , Recursive = True , WithDecryption = True ) print ( json . dumps ( param_details , default = str ))","title":"SSM (Systems Manager) Parameter Store"},{"location":"AWS/06-Management/#cloudwatch","text":"Create Alarms to Stop, Terminate, Reboot, or Recover an Instance Monitor resources and applications performances from the cloud and on-premises systems based on logs and events: compute ec2 instances - every 5 minutes by default autoscaling groups ELB Route53 health checks storage and content delivery EBS - virtual hard disk Storage gateways CloudFront network AWS can see that you have Memory, but a custom metric is required to see how much of the memory is being used AWS can see how much of CPU you are using in a EC2, but cannot see what you are using if for. CloudWatch logs insights adds cross log group querying","title":"CloudWatch"},{"location":"AWS/06-Management/#cloudtrail","text":"Continuously log your AWS account activity monitoring API calls per account and region data is retained for 90 days logs can be stored in a single S3 bucket read activity in the S3 bucket will be easier than using the API","title":"CloudTrail"},{"location":"AWS/06-Management/#trusted-advisor","text":"Recomendations and advices (not only EC2 instances). It helps you optimize cost, fault-tolerance, perfoemance, security and more.","title":"Trusted Advisor"},{"location":"AWS/06-Management/#organisations","text":"AWS Organizations enables you to centrally manage billing, control access, compliance, and security, and share resources across multiple accounts in the AWS Cloud. You can consolidate all your AWS accounts into an organization, and arrange all AWS accounts into distinct OUs (Organizational Units). Provides single payer and centralized cost tracking Lets you create and invite accounts Allows you to apply policy-based controls Helps you simplify organization-wide management of AWS services Or you can create an organization with only consolidated billing features. After you create an organization, you cannot join this account to another organization until you delete its current organization. From AWS Organisations, create organisation Enable AWS Single Sign On to centrally manage access to multiple AWS accounts and business applications. Create an organization trail in AWS CloudTrail to log all events for all AWS accounts in your organization. From Organize Accounts Tab, we create a new organisational units From Policies tab, enable service control policies and create a sample policy to block EC2 usage. Select Amazon EC2 statement and deny effect Apply the new policy to organisational units or to AWS Accounts Maximum of 20 Link accounts. Contact AWS for more Assuming all instances are in the same AWS Organization, the reserved instance pricing for the unused on demand instances will be applied. Landing Zone helps to quickly setup a secure, multi-account AWS environment based on AWS best practices. services control policies for central control over all of the permissions","title":"Organisations"},{"location":"AWS/06-Management/#aws-config","text":"AWS Config provides an inventory of your AWS resources and a history of configuration changes to these resources. You can use AWS Config to define rules that evaluate these configurations for compliance. How to get started: Specify the types of AWS resources you want AWS Config to record Define the Amazon S3 bucket to which it sends files Set the Amazon SNS topic to which it sends notifications Define config rules Restart EC2 instances and check AWS Config results You are charged based on the number of configuration items recorded, the number of active AWS Config rule evaluations and the number of conformance pack evaluations in your account","title":"AWS Config"},{"location":"AWS/06-Management/#pricing","text":"Capex: Capital Expenditure: you pay up front. It's a fixed cost Opex: Operational Expenditure: you pay for what you use, like electricity, gas or water Budgets predict costs before they are incurred. Alarms can be set to monitor spending on your AWS account from AWS Billing service - Budgets Receive Billing Alerts must be enabled at Billing Preferences . Cost explorer is use to explore costs after they have been incurred. See Billing & Cost Management Dashboard . Creating a billing alarm at ClouldWatch/Alarms/Billing using SNS (Simple Notification Service) topic to monitor estimated AWS charges. Application Integration/SNS is a messaging service that enables you to decouple microservices, distributed systems, and serverless applications. Using AWS SNS topics, your publisher systems can fan-out messages to a large number of subscriber endpoints for parallel processing, including Amazon SQS queues, AWS Lambda functions, and HTTP/S webhooks. Additionally, SNS can be used to fan out notifications to end users using mobile push, SMS, and email. Application Integration/SQS (Simple Queue Service) offers a queue that lets you integrate and decouple distributed software systems and components. Support Plans: All accounts receive billing support Pricing policies pay as you go pay less when you reserve pay even less per unit by using more pay even less as AWS grows custom pricing cost drivers: compute storage data outbound (data leavingn, not data entering) free services VPC Elastic Beanstalk Cloud Formation IAM Autoscaling Opsworks Consolidated billing Create a paying account for billing purposes only. Do not deploy resources into the paying account. Consolidated billing allows you to get volume discounts on all your accounts","title":"Pricing"},{"location":"AWS/06-Management/#cost-calculators","text":"AWS Simple Monthly Calculator ( DEPRECATED ) AWS Total Cost of Ownership Calculator ( DEPRECATED ): comparing AWS VS on premise AWS Pricing Calculator","title":"Cost Calculators"},{"location":"AWS/06-Management/#cost-explorer","text":"AWS Cost explorer is use to explore costs after they have been incurred. disable by default, it takes 24 hours to be enabled details are given before the end of the month bill...","title":"Cost Explorer"},{"location":"AWS/07-Architecture/","text":"7. Hight Availability Architecture \u00b6 When a web site requires a minimum of 6 EC2 and tolerate the failaure of 1 AZ, the most cost effective environment is 3 AZ with 3 EC2s in each AZ. if 1 AZ fails, there are still 6 EC2 instances. Elastic Load Balancers \u00b6 Elastic Load Balancer implements Dynamic Load Balancer Load Balancing algorithms: Round Robin: first request to first server, second request to second server... Randomized Centrally managed, based on defined conditions threshold-based: all request to a server till a defined limit (threshold) ELB types : ALB ( Application Load Balancers) for intelligent routing - HTTP/HTTPS (layer 7) NLB ( Network Load Balancers) for extreme performance and static IPs - TCP/TLS (layer 4) CLB ( Classic Load Balancers) for classic/ old/legacy EC2 instantes or test/dev environments - low cost - HTTP/HTTPS/TCP (layers 4 and 7) GLB ( Gateway Load Balancers) to deploy, scale, and manage your third-party virtual appliances supported services: EC2 ECS Auto Scaling CloudWatch Route53 some features: X-Forwarded-For header is used to get the IPv4 address of end users sticky sessions bind a user's session to a specific EC2 instance. All user requests during the session are sent to the same intance (CLB) or target group (ALB) cross zone load balancing allow ELB to send traffic to another AZ ELB can spread (propagar) load across AZs not regions. path patterns are listener with rules to foward requests based on the URL path Error 504 means gateway timeout How to use classic load balancer Create load balancer from EC2 : Load Balancers of type CLB Register EC2 instances Select our Security Group (virtual firewall) Browse to the ELB (Elastic Load Balance) DNS name and see the result, instead of browsing to the EC2 IP address How to use application load balancer Create target group to group EC2 instances, IP addresses or Lambda functions for load balancing Create load balancer of type ALB Configure Load Balancer with name and select every AZ At least two public subnets are required to enable the LB Configure Routing with a Target Group name and the following health check settings: healthy threshold: 3 times unhealthy threshold: 3 times timeout: 3 seconds interval: 5 seconds success code: 200 Register target adding our EC2 instances (to registered) Review and create Auto Scaling \u00b6 (required) Create launch configuration to create a saved instance configuration for the new EC2 instances Create Auto Scaling group to create a collection of similar EC2 instances that are treated as a logical unit, based on its AMI select all VPC subnets for multiAZ metrics are: CPU utilization DiskReadOps network-in network-out new instances are launched Creating an Auto Scaling group using: a launch template: needed to use Dedicated Hosts and more features that can't be achieved with launch configurations. a launch configuration the Amazon EC2 launch wizard using an EC2 instance Scaling Options To mantain current instance levels at all times (for example, 10 EC2 instances) Scale manually scake based on healthchecks Scale based on a schedule: Scaling actions are performed automatically as a function of time and date Scale based on demand using scaling polices Use predictive scaling based on performance EC2 Autoscaling works in conjunction with the AWS Autoscaling service to provide a predictive ability to your autoscaling groups. A scaling cooldown helps you prevent your EC2 Auto Scaling group from launching or terminating additional instances before the effects of previous scaling activities are visible. the error \"you must use a fully formed launch template\" means the launch template is missing some information a second error can be not having enough permssions to launch EC2 instances...","title":"7. Hight Availability Architecture"},{"location":"AWS/07-Architecture/#7-hight-availability-architecture","text":"When a web site requires a minimum of 6 EC2 and tolerate the failaure of 1 AZ, the most cost effective environment is 3 AZ with 3 EC2s in each AZ. if 1 AZ fails, there are still 6 EC2 instances.","title":"7. Hight Availability Architecture"},{"location":"AWS/07-Architecture/#elastic-load-balancers","text":"Elastic Load Balancer implements Dynamic Load Balancer Load Balancing algorithms: Round Robin: first request to first server, second request to second server... Randomized Centrally managed, based on defined conditions threshold-based: all request to a server till a defined limit (threshold) ELB types : ALB ( Application Load Balancers) for intelligent routing - HTTP/HTTPS (layer 7) NLB ( Network Load Balancers) for extreme performance and static IPs - TCP/TLS (layer 4) CLB ( Classic Load Balancers) for classic/ old/legacy EC2 instantes or test/dev environments - low cost - HTTP/HTTPS/TCP (layers 4 and 7) GLB ( Gateway Load Balancers) to deploy, scale, and manage your third-party virtual appliances supported services: EC2 ECS Auto Scaling CloudWatch Route53 some features: X-Forwarded-For header is used to get the IPv4 address of end users sticky sessions bind a user's session to a specific EC2 instance. All user requests during the session are sent to the same intance (CLB) or target group (ALB) cross zone load balancing allow ELB to send traffic to another AZ ELB can spread (propagar) load across AZs not regions. path patterns are listener with rules to foward requests based on the URL path Error 504 means gateway timeout How to use classic load balancer Create load balancer from EC2 : Load Balancers of type CLB Register EC2 instances Select our Security Group (virtual firewall) Browse to the ELB (Elastic Load Balance) DNS name and see the result, instead of browsing to the EC2 IP address How to use application load balancer Create target group to group EC2 instances, IP addresses or Lambda functions for load balancing Create load balancer of type ALB Configure Load Balancer with name and select every AZ At least two public subnets are required to enable the LB Configure Routing with a Target Group name and the following health check settings: healthy threshold: 3 times unhealthy threshold: 3 times timeout: 3 seconds interval: 5 seconds success code: 200 Register target adding our EC2 instances (to registered) Review and create","title":"Elastic Load Balancers"},{"location":"AWS/07-Architecture/#auto-scaling","text":"(required) Create launch configuration to create a saved instance configuration for the new EC2 instances Create Auto Scaling group to create a collection of similar EC2 instances that are treated as a logical unit, based on its AMI select all VPC subnets for multiAZ metrics are: CPU utilization DiskReadOps network-in network-out new instances are launched Creating an Auto Scaling group using: a launch template: needed to use Dedicated Hosts and more features that can't be achieved with launch configurations. a launch configuration the Amazon EC2 launch wizard using an EC2 instance Scaling Options To mantain current instance levels at all times (for example, 10 EC2 instances) Scale manually scake based on healthchecks Scale based on a schedule: Scaling actions are performed automatically as a function of time and date Scale based on demand using scaling polices Use predictive scaling based on performance EC2 Autoscaling works in conjunction with the AWS Autoscaling service to provide a predictive ability to your autoscaling groups. A scaling cooldown helps you prevent your EC2 Auto Scaling group from launching or terminating additional instances before the effects of previous scaling activities are visible. the error \"you must use a fully formed launch template\" means the launch template is missing some information a second error can be not having enough permssions to launch EC2 instances...","title":"Auto Scaling"},{"location":"AWS/08-Databases/","text":"8. Databases \u00b6 RDS \u00b6 AWS RDS (Relational Database Service): MS SQL Server (port 1433) (up to 16TB of storage when using the Provisioned IOPS and General Purpose SSD storage) Oracle . Includes license model (BYOL - Bring your own license) MariaDB MySQL (port 3306) PostgreSQL (port 5432) Amazon Aurora . MySQL and PostgreSQL compatibility. No free tier. This is up to 5X faster than a traditional MySQL database Some features: Enable deletion protection: delete require this feature disabled. if enable, modify to disable first Multi-AZ for disaster recovery and high availability . By default in production RDS. Primary host replicates to a secondary host when failover. Enable multiAZ impact to the RDS instance if running host -t NS database_endpoint # Query DNS Records on Linux nslookup database_endpoint # Query DNS Records on Windows There is no mulit-region RDS . AWS does have multi-AZ RDS. Backup retention from 0 days (disable) to 35 days Read Replicas for performance improvement. Quering read replica can have a delay of less than a minute. They are usefull when: scaling due to excess read traffic source db unavailable reporting and data warehousing disaster recovery hosting a cross region with lower latency MS SQL Server cannot be read replica Create read replica from the RDS instance (must have backups turned on and check the option Public accesible) A read replica can be promoted as a standalone instance. Note that the promotion process is irreversible. Use of AWS EBS volumes for database and log storage. Only Aurora uses its storage system . We can manage RDS instances (CPU and Memory) using AWS CLI, AWS RDS API or management console. RDS run on VM, but login is not allowed. Patching RDS is Amazon's responsability Scaling vertical scaling (storage, CPU, memory or network) with a new RDS instance of DB instance class. Instance may shutdown multiAZ takes more time for vertical scaling, but RDS is shutted down less time than working on a single AZ horizontal scaling with read replicas scaling with AWS EBS storage Backup options backups in S3 are stored in an RDS own bucket. backup is for the db host, not only the databases. only the differences are stored in the new snapshot. in multiAZ there should not be impact. In single AZ, I/O is suspend from few seconds to few minutes. restoring a backup create a new RDS instance. you cannot restore to an existing db instance. retain DB Parameter Group and SG. Inbound rules should allow the connection using the DB port Backup types: automated backup: in a multiAZ scenario, data is backed up dialy taking a snapshot from the EBS volume of the secondary RDS instance scheduled window retention data can be restored to point in time on intervals of 5 minutes based on transactions logs . manual snapshots. Take DB Snapshot from RDS instance kept till deleted only recommended before large changes we can copy snapshot to different regions (crossregion copy) we can share snapshots as public or private with other aws accounts To mitigate the slow restore process: Restore a RDS instance with high I/O capacity Maximize the I/O during the restore process Pricing DB Engine and version License model DB Engine class Multi-AZ: the price is twice Storage type and allocation Instance hours Database Storage: EBS VS Aurora Size of bakcup storage Data transfer out Data transferred between AZs for replication of Multi-AZ deployments is free Reserve instance when long terms for both Multi-AZ and Single-AZ configurations Security Network isolation, using VPC: Private subnet Security group (firewall) Public accessibility (turn off) ClassicLink Direct Connect : dedicated line from on premise to AWS to improve the network connection VPC Peering IAM for access control: Do not use AWS root credentials IAM for RDS management MFA for extra level of protection Integrated security Active directory for SQL Server IAM Authentication MySQL, PostgreSQL, Aurora Encryption at Rest: must be defined at creation or restoration time it is free to encrypt data using KMS (Key Management System) and AES-256 encryption is replicated to all nodes, backups and snapshots cannot decrypt once encrypted two tier encryption: enable key rotation SSL for db conectivity Monitoring RDS sends metrics to CloudWatch 15-18 metrics based on the instance class (CPU, free storage space, network traffic, database connections and IOPS) Performance Insights is a free database performance dashboard, but it is not available for db.t2 instances. Only Aurora, MySql and PostgreSql support. AWS RDS Events to be notified when events occurs Script to test monitoring: CREATE TABLE scale_data ( section NUMERIC NOT NULL , id1 NUMERIC NOT NULL , id2 NUMERIC NOT NULL ); INSERT INTO scale_data SELECT sections . * , gen . * , CEIL ( RANDOM () * 100 ) FROM GENERATE_SERIES ( 1 , 300 ) sections , GENERATE_SERIES ( 1 , 900000 ) gen WHERE gen <= sections * 3000 ; How to create an RDS instance (PostgreSQL) \u00b6 Download pgAdmin and install -> http://127.0.0.1:54689/browser Create a VPC using the wizard: Select VPC with Public and Private Subnets Select all default settings but Elastic IP Allocation ID. If not found, copy and paste Configure RDS subnet groups: Create DB Subnet groups (public and private) DB Subnet Group doesn't meet AZ coverage requirement. Please add subnets to cover at least 2 AZs. Current coverage: 1 Create a new public (10.0.2.0/24) and private (10.0.3.0/24) subnets in a different AZ. Then try to create the subnet group again Edit route table association for the two new subnets. private subnets with NAT and public subnets with IGW (internet gateway) From RDS , click Create Database button Select PostgreSQL engine type, previous to last version, free tier template, db.t2.micro as db instance class, no multi-AZ Select the VPC created above and the public subnet group. Public access Security groups are required for publicly accessible databases . Create new VPC security group, so port 5432 will be opened to allow the PostgreSQL access. Run pgAdmin and create a new server Lab1 using the RDS endpoint as hostname ( mypostgresql.cmr9irlg1qe3.us-east-1.rds.amazonaws.com ), and the user and pwd CREATE TABLE test_data ( section NUMERIC NOT NULL , id1 NUMERIC NOT NULL , id2 NUMERIC NOT NULL ); INSERT INTO test_data SELECT sections . * , gen . * , CEIL ( RANDOM () * 100 ) FROM GENERATE_SERIES ( 1 , 30 ) sections , GENERATE_SERIES ( 1 , 90 ) gen WHERE gen <= sections * 3000 ; How to create a secured RDS instance \u00b6 Create a *parameter group* from RDS with family postgres10 named PostgresSSL. Edit PostgresSSL parameter group to enable and enforce ssl (ssl and rds.force_ssl = 1) Encryption at rest is not enabled for a db.t2.micro, so a dev/test RDS is required Set No Public Accessible and enable encryption Edit SG rules filling our private subnet (10.0.1.0/24 and 10.0.3.0/24) as source for PostgreSQL type From IAM, create a new Lambda Role using policy AWSLambdaVPCAccessExecutionRole named LambdaVPC From Lambda, create function called ConnectSecureRDS with runtime as node.js 10.x and LambdaVPC role Select our VPC with private subnets and the SG already edited with inbound/outbound rules filling our private subnet (10.0.1.0/24 and 10.0.3.0/24) as source for PostgreSQL type Upload the .zip file for the function code: const { Client } = require ( 'pg' ) // import node.js Postgres library exports . handler = function ( event , context , callback ) { const client = new Client () client . connect () client . query ( 'SELECT $1::text as message' , [ 'Hello world!' ], ( err , res ) => { console . log ( err ? err . stack : res . rows [ 0 ]. message ) // Hello World! client . end () callback ( err , res ); }); } Edit Environment variables PGHOST=RDS instance endpoint PGUSER=acgrds PGPASSWORD=test123123 PGDATABASE=ACGRDS PGSSLMODE=require Test the lambda function Change VPC and test again Change VPC subnet and test again. Remove PGSSLMODE variable and test again. How to create an RDS instance for WordPress \u00b6 Create Database from RDS . Select MySQL engine type, free tier template (without Multi-AZ) and fill the database name (db instance identifier) Set initial db name in the additional configuration. If you do not specify a database name, Amazon RDS does not create a database. When creating an RDS instance, you can select the AZ into which you deploy it. automated backups are enabled by default, till 35 days Create a new EC2 instance with our WebDMZ security group, our key pair and the following advanced details: #!/bin/bash yum install httpd php php-mysql -y amazon-linux-extras install -y php7.2 cd /var/www/html wget https://wordpress.org/wordpress-5.4.1.tar.gz tar -xzf wordpress-5.4.1.tar.gz cp -r wordpress/* /var/www/html/ rm -rf wordpress rm -rf wordpress-5.4.1.tar.gz chmod -R 755 wp-content chown -R apache:apache wp-content service httpd start chkconfig httpd on Allow connection between EC2 and RDS clicking on defult VPC security group, inbound rules , edit to add a rule with: MySQL/Aurora type protocol TCP port 3306 WebDMZ security group Browse to the EC2 public IP and see the wordpress home page. Fill db name, user name, pwd and RDS endpoint (rnietoe.cmr9irlg1qe3.us-east-1.rds.amazonaws.com) Create the wp-config.php file manually: ssh ec2-user@52.87.161.80 -i rnietoe.pem cd /var/www/html nano wp-config.php # here paste wp-config.php from the wordpress home page. Ctrl+X to save it From the wordpress home page, Run the installation , fill the same credentials and Install WordPress Login to wordpress Configure a ELB target group to set WordPress settings URL with a DNS address instead of a public IP Finally we create a EC2 instance image, like a snapshot . this is called AMI (Amazon Machine Image) How to create a fault-tolerance wordpress app \u00b6 Create Launch Configuration named MyLaunchConfigurationGroup select AMI MyworkPressTemplate select instance type t2.micro advance settings: #!/bin/bash yum update -y select our Security group (virtual firewall) and key pair Create Auto Scaling Group named MyAutoScalingGroup Select MyLaunchConfigurationGroup Select all subnets (AZ) Enable ELB and select my target group Scale between 2 and 3 instances (group size) Set a scaling policy. for example, when CPU is 90% notifications and tags not required here Select the ASG (Auto Scaling Group) created and see the Activity tab. Two instances are created. Browse to the ELB DNS adress and create new post in wordpress . Delete instances, wait ASG create new instances and check the post is still there Amazon Aurora \u00b6 Multi-Region deployment will best ensure global availability . Hight performance with a low cost. 2-3x faster thant postgreSQL and 5x faster than MySQL Move Logging and Storage layers into a multi tented scale out database optimized service Storage from 10gb to 64tb Max of 32 CPUs and 244 GiB RAM Continuous backup to S3 Aurora DB Cluster till 15 read-replicas Cluster endpoint connection allow write operations Reader endpoint connection allow read operations Custom endpoint connection allow load balancer Instance endpoint connection to a specific instance Aurora global databases primary region - read and write secondary region - read only. Promoted when failure Aurora store by default 6 copies of my data (3x2). Default availability is a minimum of 3 AZs with 2 db copies in each one Create Amazon Aurora database with PostgreSQL compatibility from RDS Three instances are created: regional - with writer and read endpoints writer reader Create aurora read replica from actions (writer and reader node) Aurora Serverless \u00b6 You specify the minimum and maximum amount of resources needed, and Aurora scales the capacity based on database load. This is a good option for intermittent or unpredictable workloads . only an endpoint is created //test lambda function var mysql = require ( 'mysql' ); exports . handler = ( event , context ) => { var connection = mysql . createConnection ({ host : '' , user : '' , password : '' }); connection . connect ( function ( err ) { if ( err ) throw err ; console . log ( \"Connected!\" ); }); connection . query ( \"SELECT 'Hello World!';\" , function ( err , result ) { if ( err ) throw err ; console . log ( result ); context . succeed ( 'Success' ); }); }; DynamoDB \u00b6 DynamoDB (Non Relational Databases - faster due to small transactions) is a key-value and document database that delivers single-digit millisecond performance at any scale. DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. DynamoDB allows for the storage of large text and binary objects, but there is a limit of 400 KB for the combined Value and Name Data is stored on SSDs (Solid State Drives). DynamoDB provide automatic replication across AZs. It is a regional service, there is no need to explicitly create a multi-AZ deployment. DynamoDB is distributed across 3 geographically distinct datacentres by default DAX (DynamoDB Accelerator) is an advanced DynamoDB There will always be a charge for: provisioning read and write capacity the storage of data: $0.25Gb per month Consistency types (database will not be consistent in one moment, but it will be in another moment): Eventual consistent reads (default) sample of 2 seconds Strongly consistent reads. sample of less than 1 second. some disadvantages such as : read might not be available if there is a network delay or outage higher latency global secondary indexes not supported use of more throughput capacity Create DynamoDB table: Create a table instead of a database Set table name and primary key RedShift \u00b6 Quick start to Deploy an Amazon Redshift data warehouse on the AWS Cloud for big data and analytics Amazon's Data WareHousing database (columnar data store) used for OLAP (OnLine Analytics Processing) Used for Business Intelligence availabled in 1 AZ single node of 160 gb up to 160tb $0.25 per hour or $1.000 per Tb per year security: SSL transit encryption AES-256 storage encryption usin AWS KMS not suited for streaming data ElastiCache \u00b6 Improve performance with in-memory cache for the most common queries: Memcached. Simplest model for implementation. hight performance. It does not offer a native encryption service Redis (muti AZ) Pub/Sub Sorted Sets In-Memory Data Store HIPPA or PCI-DSS compliance ElastiCache is only a key-value store and cannot therefore store relational data. Graph Databases \u00b6 Amazon Neptune","title":"8. Databases"},{"location":"AWS/08-Databases/#8-databases","text":"","title":"8. Databases"},{"location":"AWS/08-Databases/#rds","text":"AWS RDS (Relational Database Service): MS SQL Server (port 1433) (up to 16TB of storage when using the Provisioned IOPS and General Purpose SSD storage) Oracle . Includes license model (BYOL - Bring your own license) MariaDB MySQL (port 3306) PostgreSQL (port 5432) Amazon Aurora . MySQL and PostgreSQL compatibility. No free tier. This is up to 5X faster than a traditional MySQL database Some features: Enable deletion protection: delete require this feature disabled. if enable, modify to disable first Multi-AZ for disaster recovery and high availability . By default in production RDS. Primary host replicates to a secondary host when failover. Enable multiAZ impact to the RDS instance if running host -t NS database_endpoint # Query DNS Records on Linux nslookup database_endpoint # Query DNS Records on Windows There is no mulit-region RDS . AWS does have multi-AZ RDS. Backup retention from 0 days (disable) to 35 days Read Replicas for performance improvement. Quering read replica can have a delay of less than a minute. They are usefull when: scaling due to excess read traffic source db unavailable reporting and data warehousing disaster recovery hosting a cross region with lower latency MS SQL Server cannot be read replica Create read replica from the RDS instance (must have backups turned on and check the option Public accesible) A read replica can be promoted as a standalone instance. Note that the promotion process is irreversible. Use of AWS EBS volumes for database and log storage. Only Aurora uses its storage system . We can manage RDS instances (CPU and Memory) using AWS CLI, AWS RDS API or management console. RDS run on VM, but login is not allowed. Patching RDS is Amazon's responsability Scaling vertical scaling (storage, CPU, memory or network) with a new RDS instance of DB instance class. Instance may shutdown multiAZ takes more time for vertical scaling, but RDS is shutted down less time than working on a single AZ horizontal scaling with read replicas scaling with AWS EBS storage Backup options backups in S3 are stored in an RDS own bucket. backup is for the db host, not only the databases. only the differences are stored in the new snapshot. in multiAZ there should not be impact. In single AZ, I/O is suspend from few seconds to few minutes. restoring a backup create a new RDS instance. you cannot restore to an existing db instance. retain DB Parameter Group and SG. Inbound rules should allow the connection using the DB port Backup types: automated backup: in a multiAZ scenario, data is backed up dialy taking a snapshot from the EBS volume of the secondary RDS instance scheduled window retention data can be restored to point in time on intervals of 5 minutes based on transactions logs . manual snapshots. Take DB Snapshot from RDS instance kept till deleted only recommended before large changes we can copy snapshot to different regions (crossregion copy) we can share snapshots as public or private with other aws accounts To mitigate the slow restore process: Restore a RDS instance with high I/O capacity Maximize the I/O during the restore process Pricing DB Engine and version License model DB Engine class Multi-AZ: the price is twice Storage type and allocation Instance hours Database Storage: EBS VS Aurora Size of bakcup storage Data transfer out Data transferred between AZs for replication of Multi-AZ deployments is free Reserve instance when long terms for both Multi-AZ and Single-AZ configurations Security Network isolation, using VPC: Private subnet Security group (firewall) Public accessibility (turn off) ClassicLink Direct Connect : dedicated line from on premise to AWS to improve the network connection VPC Peering IAM for access control: Do not use AWS root credentials IAM for RDS management MFA for extra level of protection Integrated security Active directory for SQL Server IAM Authentication MySQL, PostgreSQL, Aurora Encryption at Rest: must be defined at creation or restoration time it is free to encrypt data using KMS (Key Management System) and AES-256 encryption is replicated to all nodes, backups and snapshots cannot decrypt once encrypted two tier encryption: enable key rotation SSL for db conectivity Monitoring RDS sends metrics to CloudWatch 15-18 metrics based on the instance class (CPU, free storage space, network traffic, database connections and IOPS) Performance Insights is a free database performance dashboard, but it is not available for db.t2 instances. Only Aurora, MySql and PostgreSql support. AWS RDS Events to be notified when events occurs Script to test monitoring: CREATE TABLE scale_data ( section NUMERIC NOT NULL , id1 NUMERIC NOT NULL , id2 NUMERIC NOT NULL ); INSERT INTO scale_data SELECT sections . * , gen . * , CEIL ( RANDOM () * 100 ) FROM GENERATE_SERIES ( 1 , 300 ) sections , GENERATE_SERIES ( 1 , 900000 ) gen WHERE gen <= sections * 3000 ;","title":"RDS"},{"location":"AWS/08-Databases/#how-to-create-an-rds-instance-postgresql","text":"Download pgAdmin and install -> http://127.0.0.1:54689/browser Create a VPC using the wizard: Select VPC with Public and Private Subnets Select all default settings but Elastic IP Allocation ID. If not found, copy and paste Configure RDS subnet groups: Create DB Subnet groups (public and private) DB Subnet Group doesn't meet AZ coverage requirement. Please add subnets to cover at least 2 AZs. Current coverage: 1 Create a new public (10.0.2.0/24) and private (10.0.3.0/24) subnets in a different AZ. Then try to create the subnet group again Edit route table association for the two new subnets. private subnets with NAT and public subnets with IGW (internet gateway) From RDS , click Create Database button Select PostgreSQL engine type, previous to last version, free tier template, db.t2.micro as db instance class, no multi-AZ Select the VPC created above and the public subnet group. Public access Security groups are required for publicly accessible databases . Create new VPC security group, so port 5432 will be opened to allow the PostgreSQL access. Run pgAdmin and create a new server Lab1 using the RDS endpoint as hostname ( mypostgresql.cmr9irlg1qe3.us-east-1.rds.amazonaws.com ), and the user and pwd CREATE TABLE test_data ( section NUMERIC NOT NULL , id1 NUMERIC NOT NULL , id2 NUMERIC NOT NULL ); INSERT INTO test_data SELECT sections . * , gen . * , CEIL ( RANDOM () * 100 ) FROM GENERATE_SERIES ( 1 , 30 ) sections , GENERATE_SERIES ( 1 , 90 ) gen WHERE gen <= sections * 3000 ;","title":"How to create an RDS instance (PostgreSQL)"},{"location":"AWS/08-Databases/#how-to-create-a-secured-rds-instance","text":"Create a *parameter group* from RDS with family postgres10 named PostgresSSL. Edit PostgresSSL parameter group to enable and enforce ssl (ssl and rds.force_ssl = 1) Encryption at rest is not enabled for a db.t2.micro, so a dev/test RDS is required Set No Public Accessible and enable encryption Edit SG rules filling our private subnet (10.0.1.0/24 and 10.0.3.0/24) as source for PostgreSQL type From IAM, create a new Lambda Role using policy AWSLambdaVPCAccessExecutionRole named LambdaVPC From Lambda, create function called ConnectSecureRDS with runtime as node.js 10.x and LambdaVPC role Select our VPC with private subnets and the SG already edited with inbound/outbound rules filling our private subnet (10.0.1.0/24 and 10.0.3.0/24) as source for PostgreSQL type Upload the .zip file for the function code: const { Client } = require ( 'pg' ) // import node.js Postgres library exports . handler = function ( event , context , callback ) { const client = new Client () client . connect () client . query ( 'SELECT $1::text as message' , [ 'Hello world!' ], ( err , res ) => { console . log ( err ? err . stack : res . rows [ 0 ]. message ) // Hello World! client . end () callback ( err , res ); }); } Edit Environment variables PGHOST=RDS instance endpoint PGUSER=acgrds PGPASSWORD=test123123 PGDATABASE=ACGRDS PGSSLMODE=require Test the lambda function Change VPC and test again Change VPC subnet and test again. Remove PGSSLMODE variable and test again.","title":"How to create a secured RDS instance"},{"location":"AWS/08-Databases/#how-to-create-an-rds-instance-for-wordpress","text":"Create Database from RDS . Select MySQL engine type, free tier template (without Multi-AZ) and fill the database name (db instance identifier) Set initial db name in the additional configuration. If you do not specify a database name, Amazon RDS does not create a database. When creating an RDS instance, you can select the AZ into which you deploy it. automated backups are enabled by default, till 35 days Create a new EC2 instance with our WebDMZ security group, our key pair and the following advanced details: #!/bin/bash yum install httpd php php-mysql -y amazon-linux-extras install -y php7.2 cd /var/www/html wget https://wordpress.org/wordpress-5.4.1.tar.gz tar -xzf wordpress-5.4.1.tar.gz cp -r wordpress/* /var/www/html/ rm -rf wordpress rm -rf wordpress-5.4.1.tar.gz chmod -R 755 wp-content chown -R apache:apache wp-content service httpd start chkconfig httpd on Allow connection between EC2 and RDS clicking on defult VPC security group, inbound rules , edit to add a rule with: MySQL/Aurora type protocol TCP port 3306 WebDMZ security group Browse to the EC2 public IP and see the wordpress home page. Fill db name, user name, pwd and RDS endpoint (rnietoe.cmr9irlg1qe3.us-east-1.rds.amazonaws.com) Create the wp-config.php file manually: ssh ec2-user@52.87.161.80 -i rnietoe.pem cd /var/www/html nano wp-config.php # here paste wp-config.php from the wordpress home page. Ctrl+X to save it From the wordpress home page, Run the installation , fill the same credentials and Install WordPress Login to wordpress Configure a ELB target group to set WordPress settings URL with a DNS address instead of a public IP Finally we create a EC2 instance image, like a snapshot . this is called AMI (Amazon Machine Image)","title":"How to create an RDS instance for WordPress"},{"location":"AWS/08-Databases/#how-to-create-a-fault-tolerance-wordpress-app","text":"Create Launch Configuration named MyLaunchConfigurationGroup select AMI MyworkPressTemplate select instance type t2.micro advance settings: #!/bin/bash yum update -y select our Security group (virtual firewall) and key pair Create Auto Scaling Group named MyAutoScalingGroup Select MyLaunchConfigurationGroup Select all subnets (AZ) Enable ELB and select my target group Scale between 2 and 3 instances (group size) Set a scaling policy. for example, when CPU is 90% notifications and tags not required here Select the ASG (Auto Scaling Group) created and see the Activity tab. Two instances are created. Browse to the ELB DNS adress and create new post in wordpress . Delete instances, wait ASG create new instances and check the post is still there","title":"How to create a fault-tolerance wordpress app"},{"location":"AWS/08-Databases/#amazon-aurora","text":"Multi-Region deployment will best ensure global availability . Hight performance with a low cost. 2-3x faster thant postgreSQL and 5x faster than MySQL Move Logging and Storage layers into a multi tented scale out database optimized service Storage from 10gb to 64tb Max of 32 CPUs and 244 GiB RAM Continuous backup to S3 Aurora DB Cluster till 15 read-replicas Cluster endpoint connection allow write operations Reader endpoint connection allow read operations Custom endpoint connection allow load balancer Instance endpoint connection to a specific instance Aurora global databases primary region - read and write secondary region - read only. Promoted when failure Aurora store by default 6 copies of my data (3x2). Default availability is a minimum of 3 AZs with 2 db copies in each one Create Amazon Aurora database with PostgreSQL compatibility from RDS Three instances are created: regional - with writer and read endpoints writer reader Create aurora read replica from actions (writer and reader node)","title":"Amazon Aurora"},{"location":"AWS/08-Databases/#aurora-serverless","text":"You specify the minimum and maximum amount of resources needed, and Aurora scales the capacity based on database load. This is a good option for intermittent or unpredictable workloads . only an endpoint is created //test lambda function var mysql = require ( 'mysql' ); exports . handler = ( event , context ) => { var connection = mysql . createConnection ({ host : '' , user : '' , password : '' }); connection . connect ( function ( err ) { if ( err ) throw err ; console . log ( \"Connected!\" ); }); connection . query ( \"SELECT 'Hello World!';\" , function ( err , result ) { if ( err ) throw err ; console . log ( result ); context . succeed ( 'Success' ); }); };","title":"Aurora Serverless"},{"location":"AWS/08-Databases/#dynamodb","text":"DynamoDB (Non Relational Databases - faster due to small transactions) is a key-value and document database that delivers single-digit millisecond performance at any scale. DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust provisioned throughput capacity on your behalf, in response to actual traffic patterns. DynamoDB allows for the storage of large text and binary objects, but there is a limit of 400 KB for the combined Value and Name Data is stored on SSDs (Solid State Drives). DynamoDB provide automatic replication across AZs. It is a regional service, there is no need to explicitly create a multi-AZ deployment. DynamoDB is distributed across 3 geographically distinct datacentres by default DAX (DynamoDB Accelerator) is an advanced DynamoDB There will always be a charge for: provisioning read and write capacity the storage of data: $0.25Gb per month Consistency types (database will not be consistent in one moment, but it will be in another moment): Eventual consistent reads (default) sample of 2 seconds Strongly consistent reads. sample of less than 1 second. some disadvantages such as : read might not be available if there is a network delay or outage higher latency global secondary indexes not supported use of more throughput capacity Create DynamoDB table: Create a table instead of a database Set table name and primary key","title":"DynamoDB"},{"location":"AWS/08-Databases/#redshift","text":"Quick start to Deploy an Amazon Redshift data warehouse on the AWS Cloud for big data and analytics Amazon's Data WareHousing database (columnar data store) used for OLAP (OnLine Analytics Processing) Used for Business Intelligence availabled in 1 AZ single node of 160 gb up to 160tb $0.25 per hour or $1.000 per Tb per year security: SSL transit encryption AES-256 storage encryption usin AWS KMS not suited for streaming data","title":"RedShift"},{"location":"AWS/08-Databases/#elasticache","text":"Improve performance with in-memory cache for the most common queries: Memcached. Simplest model for implementation. hight performance. It does not offer a native encryption service Redis (muti AZ) Pub/Sub Sorted Sets In-Memory Data Store HIPPA or PCI-DSS compliance ElastiCache is only a key-value store and cannot therefore store relational data.","title":"ElastiCache"},{"location":"AWS/08-Databases/#graph-databases","text":"Amazon Neptune","title":"Graph Databases"},{"location":"AWS/09-Applications/","text":"9. Applications \u00b6 SQS (Simple Queue Service) \u00b6 pull based service to store messages in a queue to decouple the components of an application standard queues (default) FIFO queues (first in first out) If you have an existing application that uses standard queues and you want to take advantage of the ordering or exactly-once processing features of FIFO queues, you need to configure the queue and your application correctly. You can't convert an existing standard queue into a FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue. retention period : the message can remain in the queue for 14 days visibility timeout : time while message is invisible during processing. maximum VisibilityTimeout is 12 hours. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds long polling : retrieve messages from SQS queues, reducing the number of empty responses setting ReceiveMessageWaitTimeSeconds default value = 0 ? attribute DelaySeconds : When a new message is added to the SQS queue, it will be hidden from consumer instances for a fixed period. attribute WaitTimeSeconds : When the consumer instance polls for new work, the SQS service will allow it to wait a certain time for one or more messages to be available before closing the connection. Duplicate messages occur when a consumer does not complete its message processing and the visibility timeout of the message expires, making it visible for another consumer to obtain. Increasing the visibility timeout to enable the consumer processing to complete, will prevent duplicate messages. SQS service guarantees a message will be delivered at least once. automatically scales messages can be up to 256kb multiAZ backlog per instance and acceptable backlog per instance are metrics for auto scaling with SQS SNS (Simple Notification Service) \u00b6 push based web service to send messages / notifications from the cloud to mobile devices, by SMS or mail to SQS or any Http endpoint, for example CloudWatch or Cost Explorer use the publish-subscribe mechanism based on topics, called pub-sub messaging group multiple recipients using topics (a place to put some messages) An ARN (Amazon Resource Name) is created when you create a topic on Amazon SNS messages are stored across multiple AZ to prevent lost DLQ (Deed Letter Queue) is supported in SNS, SQS and Lambda messages can be up to 256kb multiAZ send_message.py #!/usr/bin/env python3 import argparse import logging import sys import uuid from time import sleep import boto3 from botocore.exceptions import ClientError parser = argparse . ArgumentParser () parser . add_argument ( \"--queue-name\" , \"-q\" , default = \"Messages\" , help = \"SQS queue name\" ) parser . add_argument ( \"--interval\" , \"-i\" , default = 0.1 , help = \"timer interval\" , type = float ) parser . add_argument ( \"--message\" , \"-m\" , help = \"message to send\" ) parser . add_argument ( \"--log\" , \"-l\" , default = \"INFO\" , help = \"logging level\" ) args = parser . parse_args () if args . log : logging . basicConfig ( format = \"[ %(levelname)s ] %(message)s \" , level = args . log ) else : parser . print_help ( sys . stderr ) sqs = boto3 . client ( \"sqs\" ) try : logging . info ( f \"Getting queue URL for queue: {args.queue_name} \" ) response = sqs . get_queue_url ( QueueName = args . queue_name ) except ClientError as e : logging . error ( e ) exit ( 1 ) queue_url = response [ \"QueueUrl\" ] logging . info ( f \"Queue URL: {queue_url} \" ) while True : try : message = str ( uuid . uuid4 ()) logging . info ( \"Sending message: \" + message ) response = sqs . send_message ( QueueUrl = queue_url , MessageBody = message ) logging . info ( \"MessageId: \" + response [ \"MessageId\" ]) sleep ( args . interval ) except ClientError as e : logging . error ( e ) exit ( 1 ) receive_messages.py #!/usr/bin/env python3 import logging import time import boto3 from botocore.exceptions import ClientError QUEUE_NAME = \"Messages\" logging . basicConfig ( format = \"[ %(levelname)s ] %(message)s \" , level = \"INFO\" ) sqs = boto3 . client ( \"sqs\" ) try : logging . info ( f \"Getting queue URL for queue: {QUEUE_NAME} \" ) response = sqs . get_queue_url ( QueueName = QUEUE_NAME ) except ClientError as e : logging . error ( e ) exit ( 1 ) queue_url = response [ \"QueueUrl\" ] logging . info ( f \"Queue URL: {queue_url} \" ) logging . info ( \"Receiving messages from queue...\" ) while True : messages = sqs . receive_message ( QueueUrl = queue_url , MaxNumberOfMessages = 10 ) if \"Messages\" in messages : for message in messages [ \"Messages\" ]: logging . info ( f \"Message body: {message['Body']} \" ) time . sleep ( 1 ) # simulate work sqs . delete_message ( QueueUrl = queue_url , ReceiptHandle = message [ \"ReceiptHandle\" ] ) else : logging . info ( \"Queue is now empty\" ) SES (Simple Email Service) \u00b6 AWS SES is for applications that need to send communications via email. It supports custom email header fields, and many MIME types. But it can't be used by CloudWatch to push notifications via email. SWF (Simple Workflow Service) \u00b6 web service to coordinate synchronous and asynchronous work across distributed application components as executable code, web service calls, human actions or scripts, based on workflow tasks it makes sure a task is assinged only onces and never duplicated till 1 year SWF Actors: Workflow starters: start the workflow Deciders : decide what to do next if something fails or finish Activity workers a domain refer to a collection of related workflows Step Functions \u00b6 Step functions eventually replace SWF using state machines with: deciders activity tasks worker tasks support pararell processing Amazon Polly \u00b6 Machine learning service to convert text to audio mp3 (Alexa) SAM (Serverless Application Model) \u00b6 CloudFormation extension optimized for serverless functions, APIs and tables. it can run locally with docker Transform tag sam init sam deploy --guided ECS (Elastic Container Service) \u00b6 Amazon ECS is an orchestration service to deploy, manage, and scale Docker containers running applications, services, and batch processes. Amazon ECS places containers across your cluster based on your resource needs and is integrated with familiar features like ELB, EC2 security groups, EBS volumes and IAM roles. no VM builds required, but Ec2 can be used for more control ECR (Elastic Container Registry) is the managed docker container resigry to store, manage and deploy images. it work with on-premises deployment Use Fargate to automatically build environments. Fargate is a serverless container engine (like docker). It works with both ECS and EKS (Elastic Kubernetes Service) container task service cluster Service quotas \u00b6 Your AWS account has default quotas/limits for each AWS service and region . Service Quotas is an AWS service that helps you manage your quotas for over 100 AWS services , from one location. Along with looking up the quota values, you can also request a quota increase from the Service Quotas console. Workspaces \u00b6 Amazon WorkSpaces provides users with a virtual desktop experience (linux or windows7, windows 10) in the cloud that can be accessed from any connected device to multiple applications storage is persistent (virtual D: drive) AppStream 2.0 \u00b6 allow virtual applications to be accessed from the cloud. Sample of office 365 Analytics \u00b6 Cloud Search \u00b6 Amazon CloudSearch manages all the server resources needed to build and deploy search indexes . All you have to do is upload your data to a search domain and start submitting requests. ES (Elastic Search) \u00b6 for big volume of data using clusters Data Pipeline \u00b6 using data nodes AWS Glue \u00b6 ETL (Extract, Transform and Load) QuickSight \u00b6 business analytics not included in your AWS subscription Athena \u00b6 query data in S3 using SQL Developer Tools \u00b6 AWS Solutions to manage an application project from the beginning to the end: CodeStart \u00b6 full code building and deployment tool CodeCommit \u00b6 source code repository managerment CodeArtifact \u00b6 ? CodeBuild \u00b6 build and test code in the cloud CodeDeploy \u00b6 Automate Code Deployment CodePipeline \u00b6 CI-CD (continuous integration and continuous delivery) Cloud9 \u00b6 like visualstudio code with linux shell interface using an EC2 linux instance AWS X-Ray \u00b6 Developer Tools - X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application\u2019s underlying components.","title":"9. Applications"},{"location":"AWS/09-Applications/#9-applications","text":"","title":"9. Applications"},{"location":"AWS/09-Applications/#sqs-simple-queue-service","text":"pull based service to store messages in a queue to decouple the components of an application standard queues (default) FIFO queues (first in first out) If you have an existing application that uses standard queues and you want to take advantage of the ordering or exactly-once processing features of FIFO queues, you need to configure the queue and your application correctly. You can't convert an existing standard queue into a FIFO queue. To make the move, you must either create a new FIFO queue for your application or delete your existing standard queue and recreate it as a FIFO queue. retention period : the message can remain in the queue for 14 days visibility timeout : time while message is invisible during processing. maximum VisibilityTimeout is 12 hours. The default visibility timeout for a message is 30 seconds. The minimum is 0 seconds long polling : retrieve messages from SQS queues, reducing the number of empty responses setting ReceiveMessageWaitTimeSeconds default value = 0 ? attribute DelaySeconds : When a new message is added to the SQS queue, it will be hidden from consumer instances for a fixed period. attribute WaitTimeSeconds : When the consumer instance polls for new work, the SQS service will allow it to wait a certain time for one or more messages to be available before closing the connection. Duplicate messages occur when a consumer does not complete its message processing and the visibility timeout of the message expires, making it visible for another consumer to obtain. Increasing the visibility timeout to enable the consumer processing to complete, will prevent duplicate messages. SQS service guarantees a message will be delivered at least once. automatically scales messages can be up to 256kb multiAZ backlog per instance and acceptable backlog per instance are metrics for auto scaling with SQS","title":"SQS (Simple Queue Service)"},{"location":"AWS/09-Applications/#sns-simple-notification-service","text":"push based web service to send messages / notifications from the cloud to mobile devices, by SMS or mail to SQS or any Http endpoint, for example CloudWatch or Cost Explorer use the publish-subscribe mechanism based on topics, called pub-sub messaging group multiple recipients using topics (a place to put some messages) An ARN (Amazon Resource Name) is created when you create a topic on Amazon SNS messages are stored across multiple AZ to prevent lost DLQ (Deed Letter Queue) is supported in SNS, SQS and Lambda messages can be up to 256kb multiAZ send_message.py #!/usr/bin/env python3 import argparse import logging import sys import uuid from time import sleep import boto3 from botocore.exceptions import ClientError parser = argparse . ArgumentParser () parser . add_argument ( \"--queue-name\" , \"-q\" , default = \"Messages\" , help = \"SQS queue name\" ) parser . add_argument ( \"--interval\" , \"-i\" , default = 0.1 , help = \"timer interval\" , type = float ) parser . add_argument ( \"--message\" , \"-m\" , help = \"message to send\" ) parser . add_argument ( \"--log\" , \"-l\" , default = \"INFO\" , help = \"logging level\" ) args = parser . parse_args () if args . log : logging . basicConfig ( format = \"[ %(levelname)s ] %(message)s \" , level = args . log ) else : parser . print_help ( sys . stderr ) sqs = boto3 . client ( \"sqs\" ) try : logging . info ( f \"Getting queue URL for queue: {args.queue_name} \" ) response = sqs . get_queue_url ( QueueName = args . queue_name ) except ClientError as e : logging . error ( e ) exit ( 1 ) queue_url = response [ \"QueueUrl\" ] logging . info ( f \"Queue URL: {queue_url} \" ) while True : try : message = str ( uuid . uuid4 ()) logging . info ( \"Sending message: \" + message ) response = sqs . send_message ( QueueUrl = queue_url , MessageBody = message ) logging . info ( \"MessageId: \" + response [ \"MessageId\" ]) sleep ( args . interval ) except ClientError as e : logging . error ( e ) exit ( 1 ) receive_messages.py #!/usr/bin/env python3 import logging import time import boto3 from botocore.exceptions import ClientError QUEUE_NAME = \"Messages\" logging . basicConfig ( format = \"[ %(levelname)s ] %(message)s \" , level = \"INFO\" ) sqs = boto3 . client ( \"sqs\" ) try : logging . info ( f \"Getting queue URL for queue: {QUEUE_NAME} \" ) response = sqs . get_queue_url ( QueueName = QUEUE_NAME ) except ClientError as e : logging . error ( e ) exit ( 1 ) queue_url = response [ \"QueueUrl\" ] logging . info ( f \"Queue URL: {queue_url} \" ) logging . info ( \"Receiving messages from queue...\" ) while True : messages = sqs . receive_message ( QueueUrl = queue_url , MaxNumberOfMessages = 10 ) if \"Messages\" in messages : for message in messages [ \"Messages\" ]: logging . info ( f \"Message body: {message['Body']} \" ) time . sleep ( 1 ) # simulate work sqs . delete_message ( QueueUrl = queue_url , ReceiptHandle = message [ \"ReceiptHandle\" ] ) else : logging . info ( \"Queue is now empty\" )","title":"SNS (Simple Notification Service)"},{"location":"AWS/09-Applications/#ses-simple-email-service","text":"AWS SES is for applications that need to send communications via email. It supports custom email header fields, and many MIME types. But it can't be used by CloudWatch to push notifications via email.","title":"SES (Simple Email Service)"},{"location":"AWS/09-Applications/#swf-simple-workflow-service","text":"web service to coordinate synchronous and asynchronous work across distributed application components as executable code, web service calls, human actions or scripts, based on workflow tasks it makes sure a task is assinged only onces and never duplicated till 1 year SWF Actors: Workflow starters: start the workflow Deciders : decide what to do next if something fails or finish Activity workers a domain refer to a collection of related workflows","title":"SWF (Simple Workflow Service)"},{"location":"AWS/09-Applications/#step-functions","text":"Step functions eventually replace SWF using state machines with: deciders activity tasks worker tasks support pararell processing","title":"Step Functions"},{"location":"AWS/09-Applications/#amazon-polly","text":"Machine learning service to convert text to audio mp3 (Alexa)","title":"Amazon Polly"},{"location":"AWS/09-Applications/#sam-serverless-application-model","text":"CloudFormation extension optimized for serverless functions, APIs and tables. it can run locally with docker Transform tag sam init sam deploy --guided","title":"SAM (Serverless Application Model)"},{"location":"AWS/09-Applications/#ecs-elastic-container-service","text":"Amazon ECS is an orchestration service to deploy, manage, and scale Docker containers running applications, services, and batch processes. Amazon ECS places containers across your cluster based on your resource needs and is integrated with familiar features like ELB, EC2 security groups, EBS volumes and IAM roles. no VM builds required, but Ec2 can be used for more control ECR (Elastic Container Registry) is the managed docker container resigry to store, manage and deploy images. it work with on-premises deployment Use Fargate to automatically build environments. Fargate is a serverless container engine (like docker). It works with both ECS and EKS (Elastic Kubernetes Service) container task service cluster","title":"ECS (Elastic Container Service)"},{"location":"AWS/09-Applications/#service-quotas","text":"Your AWS account has default quotas/limits for each AWS service and region . Service Quotas is an AWS service that helps you manage your quotas for over 100 AWS services , from one location. Along with looking up the quota values, you can also request a quota increase from the Service Quotas console.","title":"Service quotas"},{"location":"AWS/09-Applications/#workspaces","text":"Amazon WorkSpaces provides users with a virtual desktop experience (linux or windows7, windows 10) in the cloud that can be accessed from any connected device to multiple applications storage is persistent (virtual D: drive)","title":"Workspaces"},{"location":"AWS/09-Applications/#appstream-20","text":"allow virtual applications to be accessed from the cloud. Sample of office 365","title":"AppStream 2.0"},{"location":"AWS/09-Applications/#analytics","text":"","title":"Analytics"},{"location":"AWS/09-Applications/#cloud-search","text":"Amazon CloudSearch manages all the server resources needed to build and deploy search indexes . All you have to do is upload your data to a search domain and start submitting requests.","title":"Cloud Search"},{"location":"AWS/09-Applications/#es-elastic-search","text":"for big volume of data using clusters","title":"ES (Elastic Search)"},{"location":"AWS/09-Applications/#data-pipeline","text":"using data nodes","title":"Data Pipeline"},{"location":"AWS/09-Applications/#aws-glue","text":"ETL (Extract, Transform and Load)","title":"AWS Glue"},{"location":"AWS/09-Applications/#quicksight","text":"business analytics not included in your AWS subscription","title":"QuickSight"},{"location":"AWS/09-Applications/#athena","text":"query data in S3 using SQL","title":"Athena"},{"location":"AWS/09-Applications/#developer-tools","text":"AWS Solutions to manage an application project from the beginning to the end:","title":"Developer Tools"},{"location":"AWS/09-Applications/#codestart","text":"full code building and deployment tool","title":"CodeStart"},{"location":"AWS/09-Applications/#codecommit","text":"source code repository managerment","title":"CodeCommit"},{"location":"AWS/09-Applications/#codeartifact","text":"?","title":"CodeArtifact"},{"location":"AWS/09-Applications/#codebuild","text":"build and test code in the cloud","title":"CodeBuild"},{"location":"AWS/09-Applications/#codedeploy","text":"Automate Code Deployment","title":"CodeDeploy"},{"location":"AWS/09-Applications/#codepipeline","text":"CI-CD (continuous integration and continuous delivery)","title":"CodePipeline"},{"location":"AWS/09-Applications/#cloud9","text":"like visualstudio code with linux shell interface using an EC2 linux instance","title":"Cloud9"},{"location":"AWS/09-Applications/#aws-x-ray","text":"Developer Tools - X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. With X-Ray, you can understand how your application and its underlying services are performing to identify and troubleshoot the root cause of performance issues and errors. X-Ray provides an end-to-end view of requests as they travel through your application, and shows a map of your application\u2019s underlying components.","title":"AWS X-Ray"},{"location":"AWS/10-Media/","text":"10. Media \u00b6 Most of the following media services use S3 buckets as repository Elastic Transcoder \u00b6 Amazon Elastic Transcoder covert media files from their original source format in to different formats that will play on any device. Translate \u00b6 on-demand language translation Elemental MediaStore \u00b6 Store and deliver video assets for live streaming media workflows Transcribe \u00b6 speech to text transcription from audio and video (subtitles) based on machine learning Rekognition \u00b6 Deep learning-based visual analysis service to search, verify, and organize millions of images and videos Kinesis \u00b6 Kinesis is a platform to send your streaming data to. Kinesis Data Stream (KDS): store /collect streaming data for processing in shards from 24 hours to 7 days Producers, shards and consumers are components of Kinesis Data Streams Kinesis Firehose: load streaming data into AWS data stores and analytics tools (S3, Redshift, Elasticsearch and Splunk) there is not data persistent. lambda funcions are optional Key components are: delivery streams, records of data and destinations. Kinesis Analytics: analyze and process streaming data in real-time using SQL or Java. Kinesis Video Streams: Capture, process, and store video streams for analytics and machine learning.","title":"10. Media"},{"location":"AWS/10-Media/#10-media","text":"Most of the following media services use S3 buckets as repository","title":"10. Media"},{"location":"AWS/10-Media/#elastic-transcoder","text":"Amazon Elastic Transcoder covert media files from their original source format in to different formats that will play on any device.","title":"Elastic Transcoder"},{"location":"AWS/10-Media/#translate","text":"on-demand language translation","title":"Translate"},{"location":"AWS/10-Media/#elemental-mediastore","text":"Store and deliver video assets for live streaming media workflows","title":"Elemental MediaStore"},{"location":"AWS/10-Media/#transcribe","text":"speech to text transcription from audio and video (subtitles) based on machine learning","title":"Transcribe"},{"location":"AWS/10-Media/#rekognition","text":"Deep learning-based visual analysis service to search, verify, and organize millions of images and videos","title":"Rekognition"},{"location":"AWS/10-Media/#kinesis","text":"Kinesis is a platform to send your streaming data to. Kinesis Data Stream (KDS): store /collect streaming data for processing in shards from 24 hours to 7 days Producers, shards and consumers are components of Kinesis Data Streams Kinesis Firehose: load streaming data into AWS data stores and analytics tools (S3, Redshift, Elasticsearch and Splunk) there is not data persistent. lambda funcions are optional Key components are: delivery streams, records of data and destinations. Kinesis Analytics: analyze and process streaming data in real-time using SQL or Java. Kinesis Video Streams: Capture, process, and store video streams for analytics and machine learning.","title":"Kinesis"},{"location":"AWS/CloudFormation/06-CloudFormation/","text":"CloudFormation \u00b6 AWS CloudFormation is IaaS tool to create , update and delete resources with templates (json or yaml). While Elastic Beanstalk we can create and manage resources based on code... Create stack using the sample template Wordpress blog, where stack is a set of related resources. Select instance type t2.micro and our key value pair rnietoe Create stack . This will create and configure an EC2 instance based on the wordpress template from Output tab, click on the value link. from Template tab, clik on View in Designer AWS Quick Start is a way of deploying environments quickly using CloudFormation templates built by experts AWS Solucions Architects. CloudFormation service is free, but the resources that it provisions have a cost CloudFormation has a wide set of supported resources, but does not support the creation of all AWS resources. Template anatomy : AWSTemplateFormatVersion : \"optional version date. sample: 2010-09-09\" Description : optional string Metadata : # optional template metadata Parameters : # optional set of parameters Mappings : # optional set of mappings Conditions : # set of conditions Transform : # set of transforms Resources : # required set of resources Outputs : # optional set of outputs When updating a stack, a change set (such as tags) is created with the summary of proposed changes. This change set is executed to update the stack. Only the change set is executed instead of all resources defined in the template. At the other hand, when adding a new resource (such as SecurityGroup) for our previous deployed resource (EC2), a new EC2 instance in created with the SecurityGroup, and the old EC2 instance is deleted automatically Resources \u00b6 CloudFormation Resources UserData property: perform actions on system startup only runs on the first boot cycle execution time impact on the startup time Base64 encoded on Windows: Run as local admin Batch commands and/or powersheel support Executed by EC2Config or EC2Launch #script echo Current date and time >> %SystemRoot% \\T em \\t est.log echo %DATE% %TIME% >> %SystemRoot% \\T em \\t est.log #powershell $file = $env :SystemRoot + \"\\Temp\\\" + (Get=Date).ToString(\" MM-dd-yy-hh-mm \") New-Item $file -ItemType file on Linux: Run as root (no need for sudo) Not run interactively (no user feedback) Logs output to /var/log/cloud-init-output.log #!/bin/bash # interpreter yum update -y yum install -y httpd service httpd start Sample template: Resources : EC2Instance : Type : AWS :: EC2 :: Instance Properties : UserData : ! Base64 : | # !/ bin / bash - xe yum update - y yum install httpd - y service httpd start Mappings \u00b6 CloudFormation Mappings Mappings : RegionMap : # MapName us-east-1 : # TopLevelKey 1 AMI : ami-1853ac65 # SecondLevelKey 1 us-west-1 : # TopLevelKey 2 AMI : ami-bf5540df # SecondLevelKey 2 Resources : MyEc2Instance : Type : AWS::EC2::Instance Properties : InstanceType : t2.micro ImageId : !FindInMap : - RegionMap # MapName - !Ref AWS::Region # TopLevelKey - AMI # SecondLevelKey Input Parameters \u00b6 CloudFormation Parameters enable you to input custom values to your template each time you create or update a stack. Supported parameter types: String Number List CommaDelimitedList AWS-specific types(AWS::ec2::Image::Id) Parameters : InstanceTypeParameter : # Parameter ID Type : String # Parameter Type Default : t2.micro # Parameter Property 1 AllowedValues : # Parameter Property 2 - t2.micro - m1.small - m1.large Description : # ParameterProperty 3 EC2 Instance Type Resources : Ec2Instance : Type : AWS::EC2::Instance Properties : InstanceType : !Ref : InstanceTypeParameter # Parameter Metadata \u00b6 CloudFormation Metadata Conditions \u00b6 CloudFormation Conditions Transform \u00b6 CloudFormation Transform Outputs \u00b6 CloudFormation Outputs information about resources, within a cloudformation stack. For example: the public IP or DNS of a EC2 instance the S3 bucket name for a stack Outputs : ServerDns : # OutputID Value : ! GetAtt # Value to return - Ec2Instance - PublicDnsName Export : Name : # Value to export Intrinsic functions \u00b6 CloudFormation Intrinsic functions Join : appends a set of values into a single value { \"Fn::Join\" : [ \":\" , [ \"a\" , \"b\" , \"c\" ] ] } !Join [ \":\", [ a, b, c ] ] Ref : : returns the value of the specified resource. { \"Ref\" : \"resourceId\" } !Ref resourceId FindInMap : returns the value corresponding to keys in a two-level map that is declared in the Mappings section. { \"Fn::FindInMap\" : [ \"MapName\" , \"TopLevelKey\" , \"SecondLevelKey\" ] } !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] GetAtt : returns the value of an attribute from a resource in the template { \"Fn::GetAtt\" : [ \"logicalNameOfResource\" , \"attributeName\" ] } !GetAtt logicalNameOfResource.attributeName Sub : substitutes variables in an input string with values that you specify when you create or update a stack, such as ${AWS::StackName} or ${AWS::Region} { \"Fn::Sub\" : String } !Sub String GetAZs : returns an array that lists AZs for a specified region in alaphabetical order. Specifying an empty string is equivalent to specifying AWS::Region. { \"Fn::GetAZs\" : \"region\" } ``` ```yml !GetAZs region Pseudo parameters \u00b6 CloudFormation Pseudo parameters AWS::AccountId : Returns the AWS account ID of the account in which the stack is being created AWS::NotificationARNs : Returns the list of notification ARNs ( A mazon R esource N ames) for the current stack. AWS::StackId : Returns the ID of the stack AWS::StackName : Returns the Name of the stack AWS::Region : Returns the AWS Region in which the resource is being created Helper scripts \u00b6 Pyhon based helper scripts preinstalled on Amazon Linux (to avoid scripting): cfn-init: : read Metadata to execute AWS::CloudFormation::Init cfn-signal : signal with a CreationPolicy or WaitCondition when the resource or application is ready. cfn-get-metadata : retrieve metadata based on a specific key. cfn-hup : check for updates to metadata and execute custom hooks when changes are detected. single config key sample: AWS::CloudFormation::Init: config: packages: download and install pre-packaged applications and components groups: create Linux/Unix groups and assign group IDs users: create Linux/UNIX users on the EC2 instance sources: download an archive file and unpack it in the target directory on EC2 files: create files on the EC2 instance services: define enabled and siabled services when the instance is launch commands: execute commands on the EC2 instance configSets sample to install a web server: installweb : packages : yum : httpd : [] services : sysvinit : httpd : enabled : true ensureRunning : true installphp : packages : yum : php : [] Resources : EC2Instance : Metadata : AWS :: CloudFormation :: Init : configSets : webphp : - \"installphp\" - \"installweb\" How to setting up a full stack: From CloudFormation , create stack uploading the following template file: Parameters : myKeyPair : Description : Amazon EC2 Key Pair Type : AWS :: EC2 :: KeyPair :: KeyName VpcId : Description : Enter the VpcId Type : AWS :: EC2 :: VPC :: Id SubnetIds : Description : Enter the Subnets Type : List < AWS :: EC2 :: Subnet :: Id > Mappings : RegionMap : us - east - 1 : AMI : ami - 1853 ac65 us - west - 1 : AMI : ami - bf5540df eu - west - 1 : AMI : ami - 3 bfab942 ap - southeast - 1 : AMI : ami - e2adf99e ap - southeast - 2 : AMI : ami - 43874721 Resources : LoadBalancer : # Application Load Balancer Type : AWS :: ElasticLoadBalancingV2 :: LoadBalancer Properties : SecurityGroups : - ! Ref ALBSecurityGroup Subnets : ! Ref SubnetIds LoadBalancerListener : # Port 80 Listener for ALB Type : AWS :: ElasticLoadBalancingV2 :: Listener Properties : LoadBalancerArn : ! Ref LoadBalancer Port : 80 Protocol : HTTP DefaultActions : - Type : forward TargetGroupArn : Ref : TargetGroup TargetGroup : Type : AWS :: ElasticLoadBalancingV2 :: TargetGroup Properties : Port : 80 Protocol : HTTP VpcId : ! Ref VpcId AutoScalingGroup : Type : AWS :: AutoScaling :: AutoScalingGroup Properties : AvailabilityZones : ! GetAZs LaunchConfigurationName : ! Ref LaunchConfiguration MinSize : 1 MaxSize : 3 TargetGroupARNs : - ! Ref TargetGroup LaunchConfiguration : Type : AWS :: AutoScaling :: LaunchConfiguration Metadata : Comment : Install php and httpd AWS :: CloudFormation :: Init : config : packages : yum : httpd : [] php : [] files : /var/www/html/i ndex . php : content : ! Sub | <? php print \"Hello world Abs was here!\" ; ?> services : sysvinit : httpd : enabled : true ensureRunning : true Properties : KeyName : ! Ref myKeyPair InstanceType : t2 . micro SecurityGroups : - ! Ref EC2SecurityGroup ImageId : Fn :: FindInMap : - RegionMap - ! Ref AWS :: Region - AMI UserData : 'Fn::Base64' : ! Sub | # !/ bin / bash - xe # Ensure AWS CFN Bootstrap is the latest yum install - y aws - cfn - bootstrap # Install the files and packages from the metadata /opt/aws/bin/ cfn - init - v -- stack $ { AWS :: StackName } -- resource LaunchConfiguration -- region $ { AWS :: Region } ALBSecurityGroup : Type : AWS :: EC2 :: SecurityGroup Properties : GroupDescription : ALB Security Group VpcId : ! Ref VpcId SecurityGroupIngress : - IpProtocol : tcp FromPort : 80 ToPort : 80 CidrIp : 0.0 . 0.0 / 0 EC2SecurityGroup : Type : AWS :: EC2 :: SecurityGroup Properties : GroupDescription : EC2 Instance EC2InboundRule : # EC2 can only accept traffic from ALB Type : AWS :: EC2 :: SecurityGroupIngress Properties : IpProtocol : tcp FromPort : 80 ToPort : 80 SourceSecurityGroupId : ! GetAtt - ALBSecurityGroup - GroupId GroupId : ! GetAtt - EC2SecurityGroup - GroupId Outputs : PublicDns : Description : The Public DNS Value : ! Sub 'http://${LoadBalancer.DNSName}' Set stack name and parameters and finally create the stack Stak is created with defined resources Check the stack outputs and browse to the EC2 instance uri: ChangeSets \u00b6 Allow to preview how changes will impact to the resources. There are 4 changeSets operations: Create : create a change set. This operation does not modify the stack View : view proposed changes after creating Execute : execute the change set to update the stack Delete : delete change set. This operation does not modify the stack how to create a change set for current stack to update security groups: We are going to remove port 22 from security group in this sample From CloudFormation, select the stack and from stack actions, Create change sets for current stack Upload the template with changes and create See the JSON Changes result: [ { \"resourceChange\" : { \"logicalResourceId\" : \"MySecurityGroup\" , \"action\" : \"Modify\" , // Add | Modify | Remove \"physicalResourceId\" : \"MyFirstStack-MySecurityGroup-1AT0XTT24PZNP\" , \"resourceType\" : \"AWS::EC2::SecurityGroup\" , \"replacement\" : \"False\" , \"moduleInfo\" : null , \"details\" : [ { \"target\" : { \"name\" : \"SecurityGroupIngress\" , \"requiresRecreation\" : \"Never\" , \"attribute\" : \"Properties\" }, \"causingEntity\" : null , \"evaluation\" : \"Static\" , \"changeSource\" : \"DirectModification\" } ], \"changeSetId\" : null , \"scope\" : [ \"Properties\" ] }, \"type\" : \"Resource\" } ] Finally we can delete or execute the change set The actions depends on the resource. We should predict if replacement will be necesary based on these documents: AWS resource and property types reference Update behaviors of stack resources","title":"CloudFormation"},{"location":"AWS/CloudFormation/06-CloudFormation/#cloudformation","text":"AWS CloudFormation is IaaS tool to create , update and delete resources with templates (json or yaml). While Elastic Beanstalk we can create and manage resources based on code... Create stack using the sample template Wordpress blog, where stack is a set of related resources. Select instance type t2.micro and our key value pair rnietoe Create stack . This will create and configure an EC2 instance based on the wordpress template from Output tab, click on the value link. from Template tab, clik on View in Designer AWS Quick Start is a way of deploying environments quickly using CloudFormation templates built by experts AWS Solucions Architects. CloudFormation service is free, but the resources that it provisions have a cost CloudFormation has a wide set of supported resources, but does not support the creation of all AWS resources. Template anatomy : AWSTemplateFormatVersion : \"optional version date. sample: 2010-09-09\" Description : optional string Metadata : # optional template metadata Parameters : # optional set of parameters Mappings : # optional set of mappings Conditions : # set of conditions Transform : # set of transforms Resources : # required set of resources Outputs : # optional set of outputs When updating a stack, a change set (such as tags) is created with the summary of proposed changes. This change set is executed to update the stack. Only the change set is executed instead of all resources defined in the template. At the other hand, when adding a new resource (such as SecurityGroup) for our previous deployed resource (EC2), a new EC2 instance in created with the SecurityGroup, and the old EC2 instance is deleted automatically","title":"CloudFormation"},{"location":"AWS/CloudFormation/06-CloudFormation/#resources","text":"CloudFormation Resources UserData property: perform actions on system startup only runs on the first boot cycle execution time impact on the startup time Base64 encoded on Windows: Run as local admin Batch commands and/or powersheel support Executed by EC2Config or EC2Launch #script echo Current date and time >> %SystemRoot% \\T em \\t est.log echo %DATE% %TIME% >> %SystemRoot% \\T em \\t est.log #powershell $file = $env :SystemRoot + \"\\Temp\\\" + (Get=Date).ToString(\" MM-dd-yy-hh-mm \") New-Item $file -ItemType file on Linux: Run as root (no need for sudo) Not run interactively (no user feedback) Logs output to /var/log/cloud-init-output.log #!/bin/bash # interpreter yum update -y yum install -y httpd service httpd start Sample template: Resources : EC2Instance : Type : AWS :: EC2 :: Instance Properties : UserData : ! Base64 : | # !/ bin / bash - xe yum update - y yum install httpd - y service httpd start","title":"Resources"},{"location":"AWS/CloudFormation/06-CloudFormation/#mappings","text":"CloudFormation Mappings Mappings : RegionMap : # MapName us-east-1 : # TopLevelKey 1 AMI : ami-1853ac65 # SecondLevelKey 1 us-west-1 : # TopLevelKey 2 AMI : ami-bf5540df # SecondLevelKey 2 Resources : MyEc2Instance : Type : AWS::EC2::Instance Properties : InstanceType : t2.micro ImageId : !FindInMap : - RegionMap # MapName - !Ref AWS::Region # TopLevelKey - AMI # SecondLevelKey","title":"Mappings"},{"location":"AWS/CloudFormation/06-CloudFormation/#input-parameters","text":"CloudFormation Parameters enable you to input custom values to your template each time you create or update a stack. Supported parameter types: String Number List CommaDelimitedList AWS-specific types(AWS::ec2::Image::Id) Parameters : InstanceTypeParameter : # Parameter ID Type : String # Parameter Type Default : t2.micro # Parameter Property 1 AllowedValues : # Parameter Property 2 - t2.micro - m1.small - m1.large Description : # ParameterProperty 3 EC2 Instance Type Resources : Ec2Instance : Type : AWS::EC2::Instance Properties : InstanceType : !Ref : InstanceTypeParameter # Parameter","title":"Input Parameters"},{"location":"AWS/CloudFormation/06-CloudFormation/#metadata","text":"CloudFormation Metadata","title":"Metadata"},{"location":"AWS/CloudFormation/06-CloudFormation/#conditions","text":"CloudFormation Conditions","title":"Conditions"},{"location":"AWS/CloudFormation/06-CloudFormation/#transform","text":"CloudFormation Transform","title":"Transform"},{"location":"AWS/CloudFormation/06-CloudFormation/#outputs","text":"CloudFormation Outputs information about resources, within a cloudformation stack. For example: the public IP or DNS of a EC2 instance the S3 bucket name for a stack Outputs : ServerDns : # OutputID Value : ! GetAtt # Value to return - Ec2Instance - PublicDnsName Export : Name : # Value to export","title":"Outputs"},{"location":"AWS/CloudFormation/06-CloudFormation/#intrinsic-functions","text":"CloudFormation Intrinsic functions Join : appends a set of values into a single value { \"Fn::Join\" : [ \":\" , [ \"a\" , \"b\" , \"c\" ] ] } !Join [ \":\", [ a, b, c ] ] Ref : : returns the value of the specified resource. { \"Ref\" : \"resourceId\" } !Ref resourceId FindInMap : returns the value corresponding to keys in a two-level map that is declared in the Mappings section. { \"Fn::FindInMap\" : [ \"MapName\" , \"TopLevelKey\" , \"SecondLevelKey\" ] } !FindInMap [ MapName, TopLevelKey, SecondLevelKey ] GetAtt : returns the value of an attribute from a resource in the template { \"Fn::GetAtt\" : [ \"logicalNameOfResource\" , \"attributeName\" ] } !GetAtt logicalNameOfResource.attributeName Sub : substitutes variables in an input string with values that you specify when you create or update a stack, such as ${AWS::StackName} or ${AWS::Region} { \"Fn::Sub\" : String } !Sub String GetAZs : returns an array that lists AZs for a specified region in alaphabetical order. Specifying an empty string is equivalent to specifying AWS::Region. { \"Fn::GetAZs\" : \"region\" } ``` ```yml !GetAZs region","title":"Intrinsic functions"},{"location":"AWS/CloudFormation/06-CloudFormation/#pseudo-parameters","text":"CloudFormation Pseudo parameters AWS::AccountId : Returns the AWS account ID of the account in which the stack is being created AWS::NotificationARNs : Returns the list of notification ARNs ( A mazon R esource N ames) for the current stack. AWS::StackId : Returns the ID of the stack AWS::StackName : Returns the Name of the stack AWS::Region : Returns the AWS Region in which the resource is being created","title":"Pseudo parameters"},{"location":"AWS/CloudFormation/06-CloudFormation/#helper-scripts","text":"Pyhon based helper scripts preinstalled on Amazon Linux (to avoid scripting): cfn-init: : read Metadata to execute AWS::CloudFormation::Init cfn-signal : signal with a CreationPolicy or WaitCondition when the resource or application is ready. cfn-get-metadata : retrieve metadata based on a specific key. cfn-hup : check for updates to metadata and execute custom hooks when changes are detected. single config key sample: AWS::CloudFormation::Init: config: packages: download and install pre-packaged applications and components groups: create Linux/Unix groups and assign group IDs users: create Linux/UNIX users on the EC2 instance sources: download an archive file and unpack it in the target directory on EC2 files: create files on the EC2 instance services: define enabled and siabled services when the instance is launch commands: execute commands on the EC2 instance configSets sample to install a web server: installweb : packages : yum : httpd : [] services : sysvinit : httpd : enabled : true ensureRunning : true installphp : packages : yum : php : [] Resources : EC2Instance : Metadata : AWS :: CloudFormation :: Init : configSets : webphp : - \"installphp\" - \"installweb\" How to setting up a full stack: From CloudFormation , create stack uploading the following template file: Parameters : myKeyPair : Description : Amazon EC2 Key Pair Type : AWS :: EC2 :: KeyPair :: KeyName VpcId : Description : Enter the VpcId Type : AWS :: EC2 :: VPC :: Id SubnetIds : Description : Enter the Subnets Type : List < AWS :: EC2 :: Subnet :: Id > Mappings : RegionMap : us - east - 1 : AMI : ami - 1853 ac65 us - west - 1 : AMI : ami - bf5540df eu - west - 1 : AMI : ami - 3 bfab942 ap - southeast - 1 : AMI : ami - e2adf99e ap - southeast - 2 : AMI : ami - 43874721 Resources : LoadBalancer : # Application Load Balancer Type : AWS :: ElasticLoadBalancingV2 :: LoadBalancer Properties : SecurityGroups : - ! Ref ALBSecurityGroup Subnets : ! Ref SubnetIds LoadBalancerListener : # Port 80 Listener for ALB Type : AWS :: ElasticLoadBalancingV2 :: Listener Properties : LoadBalancerArn : ! Ref LoadBalancer Port : 80 Protocol : HTTP DefaultActions : - Type : forward TargetGroupArn : Ref : TargetGroup TargetGroup : Type : AWS :: ElasticLoadBalancingV2 :: TargetGroup Properties : Port : 80 Protocol : HTTP VpcId : ! Ref VpcId AutoScalingGroup : Type : AWS :: AutoScaling :: AutoScalingGroup Properties : AvailabilityZones : ! GetAZs LaunchConfigurationName : ! Ref LaunchConfiguration MinSize : 1 MaxSize : 3 TargetGroupARNs : - ! Ref TargetGroup LaunchConfiguration : Type : AWS :: AutoScaling :: LaunchConfiguration Metadata : Comment : Install php and httpd AWS :: CloudFormation :: Init : config : packages : yum : httpd : [] php : [] files : /var/www/html/i ndex . php : content : ! Sub | <? php print \"Hello world Abs was here!\" ; ?> services : sysvinit : httpd : enabled : true ensureRunning : true Properties : KeyName : ! Ref myKeyPair InstanceType : t2 . micro SecurityGroups : - ! Ref EC2SecurityGroup ImageId : Fn :: FindInMap : - RegionMap - ! Ref AWS :: Region - AMI UserData : 'Fn::Base64' : ! Sub | # !/ bin / bash - xe # Ensure AWS CFN Bootstrap is the latest yum install - y aws - cfn - bootstrap # Install the files and packages from the metadata /opt/aws/bin/ cfn - init - v -- stack $ { AWS :: StackName } -- resource LaunchConfiguration -- region $ { AWS :: Region } ALBSecurityGroup : Type : AWS :: EC2 :: SecurityGroup Properties : GroupDescription : ALB Security Group VpcId : ! Ref VpcId SecurityGroupIngress : - IpProtocol : tcp FromPort : 80 ToPort : 80 CidrIp : 0.0 . 0.0 / 0 EC2SecurityGroup : Type : AWS :: EC2 :: SecurityGroup Properties : GroupDescription : EC2 Instance EC2InboundRule : # EC2 can only accept traffic from ALB Type : AWS :: EC2 :: SecurityGroupIngress Properties : IpProtocol : tcp FromPort : 80 ToPort : 80 SourceSecurityGroupId : ! GetAtt - ALBSecurityGroup - GroupId GroupId : ! GetAtt - EC2SecurityGroup - GroupId Outputs : PublicDns : Description : The Public DNS Value : ! Sub 'http://${LoadBalancer.DNSName}' Set stack name and parameters and finally create the stack Stak is created with defined resources Check the stack outputs and browse to the EC2 instance uri:","title":"Helper scripts"},{"location":"AWS/CloudFormation/06-CloudFormation/#changesets","text":"Allow to preview how changes will impact to the resources. There are 4 changeSets operations: Create : create a change set. This operation does not modify the stack View : view proposed changes after creating Execute : execute the change set to update the stack Delete : delete change set. This operation does not modify the stack how to create a change set for current stack to update security groups: We are going to remove port 22 from security group in this sample From CloudFormation, select the stack and from stack actions, Create change sets for current stack Upload the template with changes and create See the JSON Changes result: [ { \"resourceChange\" : { \"logicalResourceId\" : \"MySecurityGroup\" , \"action\" : \"Modify\" , // Add | Modify | Remove \"physicalResourceId\" : \"MyFirstStack-MySecurityGroup-1AT0XTT24PZNP\" , \"resourceType\" : \"AWS::EC2::SecurityGroup\" , \"replacement\" : \"False\" , \"moduleInfo\" : null , \"details\" : [ { \"target\" : { \"name\" : \"SecurityGroupIngress\" , \"requiresRecreation\" : \"Never\" , \"attribute\" : \"Properties\" }, \"causingEntity\" : null , \"evaluation\" : \"Static\" , \"changeSource\" : \"DirectModification\" } ], \"changeSetId\" : null , \"scope\" : [ \"Properties\" ] }, \"type\" : \"Resource\" } ] Finally we can delete or execute the change set The actions depends on the resource. We should predict if replacement will be necesary based on these documents: AWS resource and property types reference Update behaviors of stack resources","title":"ChangeSets"},{"location":"Git/1-Git-Essential-Training_The-Basics/","text":"Git Essential Training: The Basics \u00b6 Configuration \u00b6 Install git from https://git-scm.com System : C:\\Program Files\\Git\\etc\\gitconfig: git config --system --list User : C:\\Users\\rniet.gitconfig: git config --global --list Project : my_project/.git/config: git config --list Other configuration settings: git config --global user.name \"Rafael Nieto\" git config --global user.email \"rnietoe@gmail.com\" git config --global core.editor \"code --wait\"` git config --list Other basic commands: change to my user directory: cd ~ list all items (hidden as well): ls -la display a text file cat .gitconfig Getting started \u00b6 Initialize and clone a repository git init git clone https://github.com/rnietoe/Training.git git clone https://github.com/rnietoe/Training.git local_repository_name Stage and commit changes git add . git commit -m \"commit message\" git log commands git log git log - 2 git log --since=2020-04-04 git log --until=2020-04-04 git log --until=\"3 days ago\" git log - L 100 , 150 : filename . txt git log --author=\"Rafael\" git log --grep=\"bug\" git log --oneline git log --stat git log --format=medium git log --format=short git log --format=oneline git log --graph --all --oneline --decorated List logs as patches (diff) git log --pathces git log -p Git Concepts and Architecture \u00b6 Make changes to files \u00b6 Status \u00b6 git status Untracked : created but unstagged yet Added : created and stagged (\"cached\") Modified Deleted Renamed Differences \u00b6 compare the staging tree and your working directory git diff compare the repository and the staging tree: git diff --stagged show only the words that are different: git diff --color-words Other changes \u00b6 remove the file from the working directory and stage the change in a single git command: git rm filename rename filename as newname and stage the change in a single git command: git mv filename newname Commit all \u00b6 Stage and commit all changes from working directory directly to the repository (instead to the staging tree) git commit -all git commit -am \"commit message\" it does not include untracked files Multiline commit message \u00b6 git commit -a Because of we didn't specify the message parameter, the .git/COMMIT_EDITMSG file is opened in our editor to enter a multiline commit message. When git/COMMIT_EDITMSG file is closed, the commit is executed. The commit is aborted if commit message is empty. Inspect a commit \u00b6 git show HEAD git show commitID git show commitID --color-words where commitID is the first characters of commit Id (6 to 8 characters are enough) Use space or f to go forward in the paginator, b to go backward, / to search words and q to quit. Compare commits \u00b6 git diff commitID1..HEAD git diff commitID1..commitID2 git diff commitID1..commitID2 --color-words Undo changes \u00b6 To discard changes in the working directory: git checkout -- \"filename\" git checkout -- . Above command does not check out any branch. Instead, it checks out a single file or every file, from the repository To discard changes in the staging tree (to the working directory): git reset HEAD filename Amend/Edit commits \u00b6 git commit --amend -m \"new commit message\" It takes what's in staging and add it to the latest commit amend command is also usefull to update the message of the latest commit Retrieve old versions \u00b6 Retrieve files with changes of desired commit ID git checkout commitId -- . git checkout commitId -- filename Revert a commit \u00b6 git revert commitID Remove untracked \u00b6 Remove untracked files from the working directory git clean -n # display what would be removed git clean -f # remove untracked files Ignore files \u00b6 project/.gitignore using: regexp with: * ? [aeiou] [0-9]. Sample of ignore all log files from logs directory: logs/*.log.[0-9] negative expressions with !: Sample of ignore all php files with *.php, but do not ignore index file with !index.php trailing slash /: Sample of ignore all files in a directory: asset/videos/ # This is a comment in the project/.gitignore file We should ignore: compiled source code packages and compressed files logs and databases Operative System generated files User-uploaded assets (images, PDFs, videos) .gitignore templates Ignore files globally \u00b6 Ignore files globally (instead by project): git config --global core.excludesfile ~/.gitignore_global Ignore files to be tracked \u00b6 git rm --cached filename Track empty directories \u00b6 Create .gitkeep file to track empty directories touch dirname/.gitkeep","title":"Git Essential Training: The Basics"},{"location":"Git/1-Git-Essential-Training_The-Basics/#git-essential-training-the-basics","text":"","title":"Git Essential Training: The Basics"},{"location":"Git/1-Git-Essential-Training_The-Basics/#configuration","text":"Install git from https://git-scm.com System : C:\\Program Files\\Git\\etc\\gitconfig: git config --system --list User : C:\\Users\\rniet.gitconfig: git config --global --list Project : my_project/.git/config: git config --list Other configuration settings: git config --global user.name \"Rafael Nieto\" git config --global user.email \"rnietoe@gmail.com\" git config --global core.editor \"code --wait\"` git config --list Other basic commands: change to my user directory: cd ~ list all items (hidden as well): ls -la display a text file cat .gitconfig","title":"Configuration"},{"location":"Git/1-Git-Essential-Training_The-Basics/#getting-started","text":"Initialize and clone a repository git init git clone https://github.com/rnietoe/Training.git git clone https://github.com/rnietoe/Training.git local_repository_name Stage and commit changes git add . git commit -m \"commit message\" git log commands git log git log - 2 git log --since=2020-04-04 git log --until=2020-04-04 git log --until=\"3 days ago\" git log - L 100 , 150 : filename . txt git log --author=\"Rafael\" git log --grep=\"bug\" git log --oneline git log --stat git log --format=medium git log --format=short git log --format=oneline git log --graph --all --oneline --decorated List logs as patches (diff) git log --pathces git log -p","title":"Getting started"},{"location":"Git/1-Git-Essential-Training_The-Basics/#git-concepts-and-architecture","text":"","title":"Git Concepts and Architecture"},{"location":"Git/1-Git-Essential-Training_The-Basics/#make-changes-to-files","text":"","title":"Make changes to files"},{"location":"Git/1-Git-Essential-Training_The-Basics/#status","text":"git status Untracked : created but unstagged yet Added : created and stagged (\"cached\") Modified Deleted Renamed","title":"Status"},{"location":"Git/1-Git-Essential-Training_The-Basics/#differences","text":"compare the staging tree and your working directory git diff compare the repository and the staging tree: git diff --stagged show only the words that are different: git diff --color-words","title":"Differences"},{"location":"Git/1-Git-Essential-Training_The-Basics/#other-changes","text":"remove the file from the working directory and stage the change in a single git command: git rm filename rename filename as newname and stage the change in a single git command: git mv filename newname","title":"Other changes"},{"location":"Git/1-Git-Essential-Training_The-Basics/#commit-all","text":"Stage and commit all changes from working directory directly to the repository (instead to the staging tree) git commit -all git commit -am \"commit message\" it does not include untracked files","title":"Commit all"},{"location":"Git/1-Git-Essential-Training_The-Basics/#multiline-commit-message","text":"git commit -a Because of we didn't specify the message parameter, the .git/COMMIT_EDITMSG file is opened in our editor to enter a multiline commit message. When git/COMMIT_EDITMSG file is closed, the commit is executed. The commit is aborted if commit message is empty.","title":"Multiline commit message"},{"location":"Git/1-Git-Essential-Training_The-Basics/#inspect-a-commit","text":"git show HEAD git show commitID git show commitID --color-words where commitID is the first characters of commit Id (6 to 8 characters are enough) Use space or f to go forward in the paginator, b to go backward, / to search words and q to quit.","title":"Inspect a commit"},{"location":"Git/1-Git-Essential-Training_The-Basics/#compare-commits","text":"git diff commitID1..HEAD git diff commitID1..commitID2 git diff commitID1..commitID2 --color-words","title":"Compare commits"},{"location":"Git/1-Git-Essential-Training_The-Basics/#undo-changes","text":"To discard changes in the working directory: git checkout -- \"filename\" git checkout -- . Above command does not check out any branch. Instead, it checks out a single file or every file, from the repository To discard changes in the staging tree (to the working directory): git reset HEAD filename","title":"Undo changes"},{"location":"Git/1-Git-Essential-Training_The-Basics/#amendedit-commits","text":"git commit --amend -m \"new commit message\" It takes what's in staging and add it to the latest commit amend command is also usefull to update the message of the latest commit","title":"Amend/Edit commits"},{"location":"Git/1-Git-Essential-Training_The-Basics/#retrieve-old-versions","text":"Retrieve files with changes of desired commit ID git checkout commitId -- . git checkout commitId -- filename","title":"Retrieve old versions"},{"location":"Git/1-Git-Essential-Training_The-Basics/#revert-a-commit","text":"git revert commitID","title":"Revert a commit"},{"location":"Git/1-Git-Essential-Training_The-Basics/#remove-untracked","text":"Remove untracked files from the working directory git clean -n # display what would be removed git clean -f # remove untracked files","title":"Remove untracked"},{"location":"Git/1-Git-Essential-Training_The-Basics/#ignore-files","text":"project/.gitignore using: regexp with: * ? [aeiou] [0-9]. Sample of ignore all log files from logs directory: logs/*.log.[0-9] negative expressions with !: Sample of ignore all php files with *.php, but do not ignore index file with !index.php trailing slash /: Sample of ignore all files in a directory: asset/videos/ # This is a comment in the project/.gitignore file We should ignore: compiled source code packages and compressed files logs and databases Operative System generated files User-uploaded assets (images, PDFs, videos) .gitignore templates","title":"Ignore files"},{"location":"Git/1-Git-Essential-Training_The-Basics/#ignore-files-globally","text":"Ignore files globally (instead by project): git config --global core.excludesfile ~/.gitignore_global","title":"Ignore files globally"},{"location":"Git/1-Git-Essential-Training_The-Basics/#ignore-files-to-be-tracked","text":"git rm --cached filename","title":"Ignore files to be tracked"},{"location":"Git/1-Git-Essential-Training_The-Basics/#track-empty-directories","text":"Create .gitkeep file to track empty directories touch dirname/.gitkeep","title":"Track empty directories"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/","text":"Git: Branches, Merges, and Remotes \u00b6 Tree-ish \u00b6 Git references \u00b6 SHA1 hash: the commit ID HEAD Branch Tag Ancestry Parents: abcd1234^, master^, HEAD^, HEAD~1, HEAD~ Grandparent: abcd1234^^, master^^, HEAD^^, HEAD~2 Grat-Grandprarents: abcd1234^^^, master^^^, HEAD^^^, HEAD~3 git show HEAD^ git show HEAD^^ git show HEAD~3 Tree Listing \u00b6 List of blobs ( b inary l arge ob ject = file) and trees (tree = directory) of a tree git ls-tree HEAD git ls-tree HEAD^ git ls-tree HEAD tree/ Filter the commit log after a commitID to HEAD git log abcd1234.. git log abcd1234..HEAD git log <SHA>..<SHA> Filter the commit log by file or directory git log filename git log dirname Branches \u00b6 Create a new branch: git branch branch_name Note that new branch and its parent branch are the same at the moment. There are not differences because there were not commits yet. List all branches. Currently checkout branch is shown in green color: git branch Switch branches git checkout branch_name Switch to a new branch git checkout -b branch_name Git rejects to switch with uncommited changes. The options are: commit the changes to the current branch remove the changes checking out the file stash the changes Compare branches \u00b6 git diff master..new_feature git diff --color-words <SHA>..<SHA> As a general rule, the older branch should go firsts, so changes shown have occurred since that point in time. Rename branches git branch -m new_branch_name Delete branches git branch -d branch_name Cannot delete the checked out branch Cannot delete branches not fully merged. Instead, use capital -D , but commits in the branch will be lost. Reset Branches \u00b6 Back up the latest commitID before, in case you want to undo the reset execution Soft reset: Latest commits changes are staged and pending to commit. Useful when you want to back things up in the commit timeline. git reset --soft <SHA> Mixed reset (default reset option): Latest commits changes are unstaged, pending to stage and commit. git reset --mixed <SHA> git reset <SHA> Hard reset: Latest commits changes are nowhere. Useful to permanently undo commits (SHA=commitID) or if you want to make one branch look like another (sha=branchID) git reset --hard <SHA> Create a new branch as backup before a hard reset Merge Branches \u00b6 The merge concept is like a commit to the checked out branch from another branch. Run a merge with a clean working directory: git merge <branch_name> Find out which other branches already have merged all their commits into this branch git branch --merged Find out which other branches have NOT merged their commits into this branch yet git branch --no-merged Use git log --graph --all --oneline --decorated to check how branches were merged A conflict occurs when two different commits have changes in the same line or set of lines. Solutions: Abort merge git merge --abort Resolve the conflict manually git show --color-words git commit Use the merge tool git mergetool Use a graphical user interface tool, like visual studio To reduce conflicts: Keep lines short Keep commmits small and focused Beware edits to whitespace (spaces, tabs, line return) Merge to master often Merge from master (tracking) Stash Changes \u00b6 Create and view stash: git stash save \"stash_name\" git stash list git stash show stash@{0} git stash show -p stash@{0} Apply stash changes to the workgin directory, removing the single stash: git stash pop git stash pop stash@{0} Apply stash changes to the workgin directory, keeping the single stash: git stash apply git stash apply stash@{0} Delete stash (stash that was not deleted using apply ) git stash drop stash@{0} git stash clear ??? git stash -u git stash --include-untracked Colaborate remotely \u00b6 git commit -m \"from-my-computer\" git push git push origin master If you try to do a push and Git rejects it, then you need to fetch, merge, and then push again. git commit -m \"from-remote-server\" git fetch git fetch does not change the local branch. fetch before you start to work everyday fetch before you push fetch before you go offline fetch often git fetch + git merge = git pull git merge Remote repositories \u00b6 Add a remote repository echo \"# repository_name\" >> README.md git init git add README.md git commit -m \"first commit\" git remote add origin git@github.com:rnietoe/repository_name.git git push -u origin master List remote repositories git remote git remote -v cat .git/config Remote branches \u00b6 Add a remote branch git push -u origin master List remote and all branches git branch -r git branch -a Delete remote branches git push origin :branch_name git push origin --delete branch_name Tracking/Untraking remote branch \u00b6 git branch branch_name origin/branch_name git checkout -b branch_name origin/branch_name ??? git branch -u origin/branch_name branch_name git branch --unset-upstream branch_name Push workflow \u00b6 git checkout master git fetch git merge origin/master git merge local_branch git push Next steps \u00b6 Git Aliases for commonly-used commands \u00b6 Add alias \"st = status\" to ~/.gitconfig git config --global alias.st \"status\" Other examples: st = status co = checkout ci = commit br = branch df = diff dfs = diff -stagged logg = log --graph --decorate --oneline --all Set up SSH keys for remote login \u00b6 Authenticating with GitHub from Git Integrated Development Environments (IDEs) \u00b6 Integrate source code editing with Git features Graphical User Interfaces (GUIs) \u00b6 They have a point-and-click interface for performing Git actions.","title":"Git: Branches, Merges, and Remotes"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#git-branches-merges-and-remotes","text":"","title":"Git: Branches, Merges, and Remotes"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#tree-ish","text":"","title":"Tree-ish"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#git-references","text":"SHA1 hash: the commit ID HEAD Branch Tag Ancestry Parents: abcd1234^, master^, HEAD^, HEAD~1, HEAD~ Grandparent: abcd1234^^, master^^, HEAD^^, HEAD~2 Grat-Grandprarents: abcd1234^^^, master^^^, HEAD^^^, HEAD~3 git show HEAD^ git show HEAD^^ git show HEAD~3","title":"Git references"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#tree-listing","text":"List of blobs ( b inary l arge ob ject = file) and trees (tree = directory) of a tree git ls-tree HEAD git ls-tree HEAD^ git ls-tree HEAD tree/ Filter the commit log after a commitID to HEAD git log abcd1234.. git log abcd1234..HEAD git log <SHA>..<SHA> Filter the commit log by file or directory git log filename git log dirname","title":"Tree Listing"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#branches","text":"Create a new branch: git branch branch_name Note that new branch and its parent branch are the same at the moment. There are not differences because there were not commits yet. List all branches. Currently checkout branch is shown in green color: git branch Switch branches git checkout branch_name Switch to a new branch git checkout -b branch_name Git rejects to switch with uncommited changes. The options are: commit the changes to the current branch remove the changes checking out the file stash the changes","title":"Branches"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#compare-branches","text":"git diff master..new_feature git diff --color-words <SHA>..<SHA> As a general rule, the older branch should go firsts, so changes shown have occurred since that point in time. Rename branches git branch -m new_branch_name Delete branches git branch -d branch_name Cannot delete the checked out branch Cannot delete branches not fully merged. Instead, use capital -D , but commits in the branch will be lost.","title":"Compare branches"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#reset-branches","text":"Back up the latest commitID before, in case you want to undo the reset execution Soft reset: Latest commits changes are staged and pending to commit. Useful when you want to back things up in the commit timeline. git reset --soft <SHA> Mixed reset (default reset option): Latest commits changes are unstaged, pending to stage and commit. git reset --mixed <SHA> git reset <SHA> Hard reset: Latest commits changes are nowhere. Useful to permanently undo commits (SHA=commitID) or if you want to make one branch look like another (sha=branchID) git reset --hard <SHA> Create a new branch as backup before a hard reset","title":"Reset Branches"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#merge-branches","text":"The merge concept is like a commit to the checked out branch from another branch. Run a merge with a clean working directory: git merge <branch_name> Find out which other branches already have merged all their commits into this branch git branch --merged Find out which other branches have NOT merged their commits into this branch yet git branch --no-merged Use git log --graph --all --oneline --decorated to check how branches were merged A conflict occurs when two different commits have changes in the same line or set of lines. Solutions: Abort merge git merge --abort Resolve the conflict manually git show --color-words git commit Use the merge tool git mergetool Use a graphical user interface tool, like visual studio To reduce conflicts: Keep lines short Keep commmits small and focused Beware edits to whitespace (spaces, tabs, line return) Merge to master often Merge from master (tracking)","title":"Merge Branches"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#stash-changes","text":"Create and view stash: git stash save \"stash_name\" git stash list git stash show stash@{0} git stash show -p stash@{0} Apply stash changes to the workgin directory, removing the single stash: git stash pop git stash pop stash@{0} Apply stash changes to the workgin directory, keeping the single stash: git stash apply git stash apply stash@{0} Delete stash (stash that was not deleted using apply ) git stash drop stash@{0} git stash clear ??? git stash -u git stash --include-untracked","title":"Stash Changes"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#colaborate-remotely","text":"git commit -m \"from-my-computer\" git push git push origin master If you try to do a push and Git rejects it, then you need to fetch, merge, and then push again. git commit -m \"from-remote-server\" git fetch git fetch does not change the local branch. fetch before you start to work everyday fetch before you push fetch before you go offline fetch often git fetch + git merge = git pull git merge","title":"Colaborate remotely"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#remote-repositories","text":"Add a remote repository echo \"# repository_name\" >> README.md git init git add README.md git commit -m \"first commit\" git remote add origin git@github.com:rnietoe/repository_name.git git push -u origin master List remote repositories git remote git remote -v cat .git/config","title":"Remote repositories"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#remote-branches","text":"Add a remote branch git push -u origin master List remote and all branches git branch -r git branch -a Delete remote branches git push origin :branch_name git push origin --delete branch_name","title":"Remote branches"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#trackinguntraking-remote-branch","text":"git branch branch_name origin/branch_name git checkout -b branch_name origin/branch_name ??? git branch -u origin/branch_name branch_name git branch --unset-upstream branch_name","title":"Tracking/Untraking remote branch"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#push-workflow","text":"git checkout master git fetch git merge origin/master git merge local_branch git push","title":"Push workflow"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#next-steps","text":"","title":"Next steps"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#git-aliases-for-commonly-used-commands","text":"Add alias \"st = status\" to ~/.gitconfig git config --global alias.st \"status\" Other examples: st = status co = checkout ci = commit br = branch df = diff dfs = diff -stagged logg = log --graph --decorate --oneline --all","title":"Git Aliases for commonly-used commands"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#set-up-ssh-keys-for-remote-login","text":"Authenticating with GitHub from Git","title":"Set up SSH keys for remote login"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#integrated-development-environments-ides","text":"Integrate source code editing with Git features","title":"Integrated Development Environments (IDEs)"},{"location":"Git/2-Git_Branches-Merges-and-Remotes/#graphical-user-interfaces-guis","text":"They have a point-and-click interface for performing Git actions.","title":"Graphical User Interfaces (GUIs)"},{"location":"Git/3-Git-Intermediate-Techniques/","text":"Git Intermediate Techniques \u00b6 Prune \u00b6 Delete remote-tracking branches git remote prune origin --dry-run git remote prune origin git fetch --prune git fetch - p Prune all unreachable objects. Do not need to use git prune Tags \u00b6 Reference names to a commit (v1.0, v1.1, v2.0): Lightweight tag git tag tag_name commit_id Annotated tag (most common) git tag -a v1.1 -m \"Version 1.0\" commitId git tag -am \"Version 1.0\" v1.1 commitId List tags git tag git tag --list git tag -l git tag -l \"v1*\" git tag -l -n Work with tags git show v1.1 git diff v1.0..v1.1 Delete a tag git tag --delete v1.1 git tag -d v1.1 Push tags to remote server git push origin v1.1 git push origin --tags Delete remote tags git push origin :v1.1 git push --delete origin v1.1 git push -d origin v1.1 Check out tags git checkout -b new_branch v1.1 Cherry-pick \u00b6 copy a single commit or a range of commits paste in the branch (new commits have different SHAs). git cherry-pick SHA git cherry-pick SHA --edit \"edit commit message\" git cherry-pick SHA -e \"edit commit message\" git cherry-pick SHA..SHA when there are conflicts: git cherry-pick --continue git cherry-pick --abort Stage changes interactively \u00b6 Interactive way to stage changes from the working directory to the staging area: git add --interactive git add -i Hunk \u00b6 Hunk is an area where two files differ. It can be staged, skipped or split. In interactive mode, select option 5 (Patch) and use the wizard with options e (edit), s (split), y (yes) and n (not) to patch portions of codes. Patch mode is not only for interactive mode: git add --patch git add -p Patch parameter is also used in other commands: git stash -p git reset -p git checkout -p git commit -p Patches \u00b6 Diff Patches \u00b6 git diff from-commit to-commit > output.diff Apply difference to our working directory git apply output.diff Formatted Patches \u00b6 Export each commit in Unix mailbox format, for example: export all commits in a range (optional to a file) git format-patch from-commit..to-commit git format-patch from-commit..to-commit --stdout > feature.patch export a single commit git format-patch -1 commitId export all commits on current branch, which are not in master branch (optional to a directory) git format-patch master git format-patch master -o output_directory Apply formatted patches ( a pply m ailbox): git am path/file.patch git am path/*.patch Rebase \u00b6 Usefull to clean commits and keep the history cleaner. Copy commits from one branch, one-by-one, and then replaying/incorporating them at the end of another branch (commits are moved). git rebase master git rebase master new_feature Get the commitID where branch diverges (the latest commit in common) git merge-base master new_feature Merge VS rebase \u00b6 Merge and Rebase have similar ends: they incorporate changes from one branch into another branch Use merge to bring large branches back into master Use rebase to add minor commits in master to a branch Rebasing should be only used on your local, private branches or on branches that you use exclusively; not branches that others are using. Rebase is destructive when there are conflicts. Conflicts will be resolved with new changes in commits, and then: git rebase --continue git rebase --skip git rebase --abort Rebase onto other branches \u00b6 git rebase --onto base upstream branch For example: git rebase --onto master camping expenses Undo rebase \u00b6 Undoing complex rebases may loses SHAs, commit messages, change sets, etc. git reset --hard ORIG_HEAD git rebase --onto camping_commitID master expenses Interactive rebase \u00b6 git rebase -i master new_feature git rebase -i HEAD~3 with options: pick : use commit drop : remove commit reword : use commit, but edit the commit message edit : use commit, but stop for amending squash : use commit, but meld into previous commit fixup : like squash , but discard this commit's log message exec : Pull rebase \u00b6 Fetch from remote, and then rebase instead or merge git pull --rebase git pull -r git pull --rebase=preserve git pull --rebase=interactive Track Problems \u00b6 Blame \u00b6 How wrote this code and when? - who I should blame? git blame filename.txt git blame -w filename.txt git blame -L 100,150 filename.txt git blame -L 100,+50 filename.txt git blame SHA filename.txt where -w ignore whitespace Add a global alias for blame as praise git config --global alias.praise blame Annotate \u00b6 Similar to blame, with different output format git annotate filename.txt Bisect \u00b6 Find the commit that introduced a bug or regression. The last good revision and the first bad revision are marked git bisect start git bisect bad <treeish> git bisect good <treeish> git bisect reset","title":"Git Intermediate Techniques"},{"location":"Git/3-Git-Intermediate-Techniques/#git-intermediate-techniques","text":"","title":"Git Intermediate Techniques"},{"location":"Git/3-Git-Intermediate-Techniques/#prune","text":"Delete remote-tracking branches git remote prune origin --dry-run git remote prune origin git fetch --prune git fetch - p Prune all unreachable objects. Do not need to use git prune","title":"Prune"},{"location":"Git/3-Git-Intermediate-Techniques/#tags","text":"Reference names to a commit (v1.0, v1.1, v2.0): Lightweight tag git tag tag_name commit_id Annotated tag (most common) git tag -a v1.1 -m \"Version 1.0\" commitId git tag -am \"Version 1.0\" v1.1 commitId List tags git tag git tag --list git tag -l git tag -l \"v1*\" git tag -l -n Work with tags git show v1.1 git diff v1.0..v1.1 Delete a tag git tag --delete v1.1 git tag -d v1.1 Push tags to remote server git push origin v1.1 git push origin --tags Delete remote tags git push origin :v1.1 git push --delete origin v1.1 git push -d origin v1.1 Check out tags git checkout -b new_branch v1.1","title":"Tags"},{"location":"Git/3-Git-Intermediate-Techniques/#cherry-pick","text":"copy a single commit or a range of commits paste in the branch (new commits have different SHAs). git cherry-pick SHA git cherry-pick SHA --edit \"edit commit message\" git cherry-pick SHA -e \"edit commit message\" git cherry-pick SHA..SHA when there are conflicts: git cherry-pick --continue git cherry-pick --abort","title":"Cherry-pick"},{"location":"Git/3-Git-Intermediate-Techniques/#stage-changes-interactively","text":"Interactive way to stage changes from the working directory to the staging area: git add --interactive git add -i","title":"Stage changes interactively"},{"location":"Git/3-Git-Intermediate-Techniques/#hunk","text":"Hunk is an area where two files differ. It can be staged, skipped or split. In interactive mode, select option 5 (Patch) and use the wizard with options e (edit), s (split), y (yes) and n (not) to patch portions of codes. Patch mode is not only for interactive mode: git add --patch git add -p Patch parameter is also used in other commands: git stash -p git reset -p git checkout -p git commit -p","title":"Hunk"},{"location":"Git/3-Git-Intermediate-Techniques/#patches","text":"","title":"Patches"},{"location":"Git/3-Git-Intermediate-Techniques/#diff-patches","text":"git diff from-commit to-commit > output.diff Apply difference to our working directory git apply output.diff","title":"Diff Patches"},{"location":"Git/3-Git-Intermediate-Techniques/#formatted-patches","text":"Export each commit in Unix mailbox format, for example: export all commits in a range (optional to a file) git format-patch from-commit..to-commit git format-patch from-commit..to-commit --stdout > feature.patch export a single commit git format-patch -1 commitId export all commits on current branch, which are not in master branch (optional to a directory) git format-patch master git format-patch master -o output_directory Apply formatted patches ( a pply m ailbox): git am path/file.patch git am path/*.patch","title":"Formatted Patches"},{"location":"Git/3-Git-Intermediate-Techniques/#rebase","text":"Usefull to clean commits and keep the history cleaner. Copy commits from one branch, one-by-one, and then replaying/incorporating them at the end of another branch (commits are moved). git rebase master git rebase master new_feature Get the commitID where branch diverges (the latest commit in common) git merge-base master new_feature","title":"Rebase"},{"location":"Git/3-Git-Intermediate-Techniques/#merge-vs-rebase","text":"Merge and Rebase have similar ends: they incorporate changes from one branch into another branch Use merge to bring large branches back into master Use rebase to add minor commits in master to a branch Rebasing should be only used on your local, private branches or on branches that you use exclusively; not branches that others are using. Rebase is destructive when there are conflicts. Conflicts will be resolved with new changes in commits, and then: git rebase --continue git rebase --skip git rebase --abort","title":"Merge VS rebase"},{"location":"Git/3-Git-Intermediate-Techniques/#rebase-onto-other-branches","text":"git rebase --onto base upstream branch For example: git rebase --onto master camping expenses","title":"Rebase onto other branches"},{"location":"Git/3-Git-Intermediate-Techniques/#undo-rebase","text":"Undoing complex rebases may loses SHAs, commit messages, change sets, etc. git reset --hard ORIG_HEAD git rebase --onto camping_commitID master expenses","title":"Undo rebase"},{"location":"Git/3-Git-Intermediate-Techniques/#interactive-rebase","text":"git rebase -i master new_feature git rebase -i HEAD~3 with options: pick : use commit drop : remove commit reword : use commit, but edit the commit message edit : use commit, but stop for amending squash : use commit, but meld into previous commit fixup : like squash , but discard this commit's log message exec :","title":"Interactive rebase"},{"location":"Git/3-Git-Intermediate-Techniques/#pull-rebase","text":"Fetch from remote, and then rebase instead or merge git pull --rebase git pull -r git pull --rebase=preserve git pull --rebase=interactive","title":"Pull rebase"},{"location":"Git/3-Git-Intermediate-Techniques/#track-problems","text":"","title":"Track Problems"},{"location":"Git/3-Git-Intermediate-Techniques/#blame","text":"How wrote this code and when? - who I should blame? git blame filename.txt git blame -w filename.txt git blame -L 100,150 filename.txt git blame -L 100,+50 filename.txt git blame SHA filename.txt where -w ignore whitespace Add a global alias for blame as praise git config --global alias.praise blame","title":"Blame"},{"location":"Git/3-Git-Intermediate-Techniques/#annotate","text":"Similar to blame, with different output format git annotate filename.txt","title":"Annotate"},{"location":"Git/3-Git-Intermediate-Techniques/#bisect","text":"Find the commit that introduced a bug or regression. The last good revision and the first bad revision are marked git bisect start git bisect bad <treeish> git bisect good <treeish> git bisect reset","title":"Bisect"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/","text":"Building React and ASP.NET Core Applications \u00b6 Create new Application \u00b6 dotnet new react dotnet run Other Web Application Templates: dotnet new mvc webapp angular react reactredux webapi ... index.html \u00b6 All the React related files can be found inside the ClientApp folder. Inside the public folder, we can see the index.html file. This is the first page that is loaded when the application starts. < div id = \"root\" ></ div > This will be the only html file in the entire application since React is generally written in JavaScript index.js \u00b6 Inside the src folder, you can see the index.js file. This is the JavaScript file that corresponds to the index.html . const rootElement = document . getElementById ( 'root' ); ReactDOM . render ( < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter>, rootElement ); App.js \u00b6 The app component is the main component in React which acts as a container for all the other components. render () { return ( < Layout > < Route exact path = '/' component = { Home } /> < Route path = '/create' component = { Create } /> < Route path = '/trips' component = { Trips } /> < /Layout> ); } Web API \u00b6 We can create a folder as \"Data\" to put in all the data-related files such as models, services, etc. Then create: /Data/Models /Data/Services /Controllers Getting starting with React \u00b6 In React, the component is an individual item that can obtain other items. Extension file is .jsx Getting data into components using: props : html attribute context : data through many levels state : place to store data JSX lifecycle methods ( componentDidMount , etc.) You can only return one single parent module from a react component. This is why we always return a parents div in the render method: render () { return ( < div >< /div> ); } List View \u00b6 axios is required: npm install axios --save componentDidMount lifecycle method is used to get the data from the API endpoint once the UI has been loaded: import React , { Component } from 'react' ; import axios from 'axios' ; export class Trips extends Component { constructor ( props ){ super ( props ); this . state = { trips : [], loading : true } } //Once the component is mounted, it sends a request to get all data componentDidMount (){ this . populateTripsData (); } populateTripsData (){ axios . get ( \"api/Trips/GetTrips\" ). then ( result => { const response = result . data ; this . setState ({ trips : response , loading : false }); }) } renderAllTripsTable ( trips ){ return ( < table className = \"table table-striped\" > < thead > < tr > < th > Name < /th> < th > Description < /th> < th > Date started < /th> < th > Date completed < /th> < th > Action < /th> < /tr> < /thead> < tbody > { trips . map ( trip => ( < tr key = { trip . id } > < td > { trip . name } < /td> < td > { trip . description } < /td> < td > { new Date ( trip . dateStarted ). toISOString (). slice ( 0 , 10 )} < /td> < td > { trip . dateCompleted ? new Date ( trip . dateCompleted ). toISOString (). slice ( 0 , 10 ) : '-' } < /td> < td > - < /td> < /tr> )) } < /tbody> < /table> ); } render (){ let content = this . state . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( this . renderAllTripsTable ( this . state . trips ) ) return ( < div > < h1 > All trips < /h1> < p > Here you can see all trips < /p> { content } < /div> ); } } Create View \u00b6 import React , { Component } from 'react' ; import axios from 'axios' ; export class Create extends Component { constructor ( props ){ super ( props ); this . onChange = this . onChange . bind ( this ); this . onSubmit = this . onSubmit . bind ( this ); this . state = { name : '' , description : '' , dateStarted : null , dateCompleted : null } } onChange ( value , event ) { this . setState ({ [ event . target . name ] : value }); } onSubmit ( e ){ e . preventDefault (); let tripObject = { Id : Math . floor ( Math . random () * 1000 ), name : this . state . name , description : this . state . description , dateStarted : this . state . dateStarted , dateCompleted : this . state . dateCompleted } //post request and redirect to /trips component const { history } = this . props ; axios . post ( \"api/Trips/AddTrip\" , tripObject ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div className = \"trip-form\" > < h3 > Add new trip < /h3> < form onSubmit = { this . onSubmit } > < div className = \"form-group\" > < label > Trip name : < /label> < input type = \"text\" className = \"form-control\" value = { this . state . name } onChange = { this . onChange } /> < /div> < div className = \"form-group\" > < label > Trip description : < /label> < textarea type = \"text\" className = \"form-control\" value = { this . state . description } onChange = { this . onChange } /> < /div> < div className = \"row\" > < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of start : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateStarted } onChange = { this . onChange } /> < /div> < /div> < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of completion : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateCompleted } onChange = { this . onChange } /> < /div> < /div> < /div> < div className = \"form-group\" > < input type = \"submit\" value = \"Add trip\" className = \"btn btn-primary\" /> < /div> < /form> < /div> ) } } Edit View \u00b6 Trips.js onTripUpdate ( id ){ const { history } = this . props ; history . push ( '/update/' + id ); } ... < div className = \"form-group\" > < button onClick = {() => this . onTripUpdate ( trip . id )} className = \"btn btn-success\" > Update < /button> < /div> Apps.js render () { return ( < Layout > ... < Route path = '/update/:id' component = { Update } /> < /Layout> ); } Update.jsx import React , { Component } from 'react' ; import axios from 'axios' ; export class Update extends Component { constructor ( props ){ super ( props ); this . onChange = this . onChange . bind ( this ); this . onUpdateCancel = this . onUpdateCancel . bind ( this ); this . onSubmit = this . onSubmit . bind ( this ); this . state = { name : '' , description : '' , dateStarted : null , dateCompleted : null } } //Once the component is mounted, it sends a request to get data by id componentDidMount (){ const { id } = this . props . match . params ; axios . get ( \"api/Trips/SingleTrip/\" + id ). then ( trip => { const response = trip . data ; this . setState ({ name : response . name , description : response . description , dateStarted : new Date ( response . dateStarted ). toISOString (). slice ( 0 , 10 ), dateCompleted : response . dateCompleted ? new Date ( response . dateCompleted ). toISOString (). slice ( 0 , 10 ) : null }) }) } onChange ( value , event ) { this . setState ({ [ event . target . name ] : value }); } onUpdateCancel (){ //cancel and redirect to /trips component const { history } = this . props ; history . push ( '/trips' ); } onSubmit ( e ){ e . preventDefault (); let tripObject = { name : this . state . name , description : this . state . description , dateStarted : new Date ( this . state . dateStarted ). toISOString (), dateCompleted : this . state . dateCompleted ? new Date ( this . state . dateCompleted ). toISOString () : null } //PUT request and redirect to /trips component const { history } = this . props ; const { id } = this . props . match . params ; axios . put ( \"api/Trips/updateTrip/\" + id , tripObject ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div className = \"trip-form\" > < h3 > Add new trip < /h3> < form onSubmit = { this . onSubmit } > < div className = \"form-group\" > < label > Trip name : < /label> < input type = \"text\" className = \"form-control\" value = { this . state . name } onChange = { this . onChangeName } /> < /div> < div className = \"form-group\" > < label > Trip description : < /label> < textarea type = \"text\" className = \"form-control\" value = { this . state . description } onChange = { this . onChangeDescription } /> < /div> < div className = \"row\" > < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of start : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateStarted } onChange = { this . onChangeDateStarted } /> < /div> < /div> < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of completion : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateCompleted } onChange = { this . onChangeDateCompleted } /> < /div> < /div> < /div> < div className = \"form-group\" > < button onClick = { this . onUpdateCancel } className = \"btn btn-default\" > Cancel < /button> < button type = \"submit\" className = \"btn btn-success\" > Update < /button> < /div> < /form> < /div> ) } } Delete View \u00b6 Trips.js onTripDelete ( id ){ const { history } = this . props ; history . push ( '/delete/' + id ); } ... < div className = \"form-group\" > < button onClick = {() => this . onTripDelete ( trip . id )} className = \"btn btn-danger\" > Delete < /button> < /div> Apps.js render () { return ( < Layout > ... < Route path = '/delete/:id' component = { Delete } /> < /Layout> ); } Delete.jsx import React , { Component } from 'react' ; import axios from 'axios' ; export class Delete extends Component { constructor ( props ){ super ( props ); this . onCancel = this . onCancel . bind ( this ); this . onConfirmation = this . onConfirmation . bind ( this ); this . state = { name : \"\" , description : \"\" } } //Once the component is mounted, it sends a request to get data by id componentDidMount (){ const { id } = this . props . match . params ; axios . get ( \"api/Trips/SingleTrip/\" + id ). then ( trip => { const response = trip . data ; this . setState ({ name : response . name , description : response . description }) }) } onCancel ( e ){ //cancel and redirect to /trips component const { history } = this . props ; history . push ( '/trips' ); } onConfirmation ( e ){ const { id } = this . props . match . params ; const { history } = this . props ; axios . delete ( \"api/Trips/DeleteTrip/\" + id ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div style = {{ marginTop : 10 }} > < h2 > Delete trip confirmation < /h2> < div class = \"card\" > < div class = \"card-body\" > < h4 class = \"card-title\" > { this . state . name } < /h4> < p class = \"card-text\" > { this . state . description } < /p> < button onClick = { this . onCancel } class = \"btn btn-default\" > Cancel < /button> < button onClick = { this . onConfirmation } class = \"btn btn-danger\" > Confirm < /button> < /div> < /div> < /div> ) } } Handling errors \u00b6 Using axios.catch(error) import React , { Component } from 'react' ; import axios from 'axios' ; export class Trips extends Component { constructor ( props ){ super ( props ); this . onTripUpdate = this . onTripUpdate . bind ( this ); this . onTripDelete = this . onTripDelete . bind ( this ); this . state = { trips : [], loading : true , failed : false , error : '' //error message } } ... populateTripsData (){ axios . get ( \"api/Trips/GetTrips\" ). then ( result => { const response = result . data ; this . setState ({ trips : response , loading : false , failed : false , error : \"\" }); }). catch ( error => { this . setState ({ trips : [], loading : false , failed : true , error : \"Trips could not be loaded\" }); }); } ... render (){ let content = this . state . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( this . state . failed ? ( < div className = \"text-danger\" > < em > { this . state . error } < /em> < /div> ) : ( this . renderAllTripsTable ( this . state . trips )) ) return ( < div > < h1 > All trips < /h1> < p > Here you can see all trips < /p> { content } < /div> ); } } Redux \u00b6 Redux is a state magement tool for JS applications. The entire state of an application is stored in one central location: The stores store the whole state. The reducers return parts of this state The actions are predefined, user-triggered events that define how a state should change. Require: npm install redux react-redux react-thunk npm install react-thunk --save Actions \u00b6 Actions are payloads of information that send data from your application to your store .\\src\\actions\\tripsActions.js import axios from 'axios' ; export const GET_ALL_TRIPS_REQUEST = 'GET_ALL_TRIPS_REQUEST' ; export const GET_ALL_TRIPS_SUCCESS = 'GET_ALL_TRIPS_SUCCESS' ; export const GET_ALL_TRIPS_ERROR = 'GET_ALL_TRIPS_ERROR' ; const getTripsSuccess = payload => ({ type : GET_ALL_TRIPS_SUCCESS , payload }); const getTripsError = payload => ({ type : GET_ALL_TRIPS_ERROR , payload }); //export the action to be able to use it export const getAllTrips = () => dispatch => { dispatch ({ type : GET_ALL_TRIPS_REQUEST }); return axios . get ( 'api/Trips/GetTrips' ). then ( res => { const response = res . data ; dispatch ( getTripsSuccess ( response )); }). catch ( error => { dispatch ( getTripsError ( \"Something went wrong!\" )); return Promise . reject ({}); }) } Reducers \u00b6 Reducers specify how the application's state change .\\src\\reducers\\tripsReducers.js import { GET_ALL_TRIPS_REQUEST , GET_ALL_TRIPS_SUCCESS , GET_ALL_TRIPS_ERROR } from '../actions/tripActions' ; const INITIAL_STATE = { loading : false , hasError : false , error : null , data : [] } export default ( state = INITIAL_STATE , action ) => { switch ( action . type ){ case GET_ALL_TRIPS_REQUEST : return { ... state , loading : true }; case GET_ALL_TRIPS_SUCCESS : return { ... state , loading : false , hasError : false , data : action . payload }; case GET_ALL_TRIPS_ERROR : return { ... state , loading : false , hasError : true , error : action . payload }; default : return state ; } } .\\src\\reducers\\index.js import { combineReducers } from 'redux' ; import tripReducers from './tripReducers' ; const rootReducer = combineReducers ({ trips : tripReducers }); //export the root reducer to be able to use it export default rootReducer ; Store \u00b6 getSate() - access application state dispatch(action) - udpate application state subscribe(listener) - register and unregister listeners Only have a single store in a Redux application: .\\src\\store\\store.js import { createStore , applyMiddleware } from 'redux' import thunk from 'redux-thunk' ; import rootReducer from '../reducers' ; const configureStore = () => applyMiddleware ( thunk )( createStore )( rootReducer ); //export the configure store to be able to use it export default configureStore ; List View with redux \u00b6 We want to load all the trips using the redux instead of just calling them directly from our component to web API: .\\src\\index.js import { Provider } from 'react-redux' ; //required for redux import configureStore from './store/store' ; //required for redux ... const store = configureStore ({}); ... ReactDOM . render ( < Provider store = { store } > //using the store < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter> < /Provider>, rootElement ); ... .\\src\\Apps.js ... import Trips from './components/Trip/Trips' ; //instead of { Trips } ... .\\src\\components\\Trip\\Trips.jsx import { connect } from 'react-redux' ; import { getAllTrips } from '../../actions/tripActions' ; ... componentDidMount (){ this . props . getAllTrips (); //get trips from tripActions } componentDidUpdate ( prevProps ){ if ( prevProps . trips . data != this . props . trips . data ){ this . setState ({ trips : this . props . trips . data }); } } ... render (){ let content = this . props . trips . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( //build the table if there is one or more trips this . state . trips . length && this . renderAllTripsTable ( this . state . trips ) ); ... const mapStateToProps = ({ trips }) => ({ trips }); //Allow the Trips component to access data from store. export default connect ( mapStateToProps , { getAllTrips })( Trips ); Authentication using Auth0 \u00b6 Log in into https://auth0.com . signed in as rnietoe.eu.auth0.com Create Single Page Web Application and configure Auth0 from Settings Set Allowed Callback Urls equals https://localhost:5001 Set Allowed Web Origins equals https://localhost:5001 Set Allowed Logout Urls equals https://localhost:5001 Install dependencies: npm install @auth0-spa-js --save Install the Auth0 React wrapper ( react-auth0-spa.js ) in our src directory. Copy paste from documenation . Create/Update the NavMenu component (./src/components/NavMenu.js) import React , { Component } from 'react' ; import { Collapse , Container , Navbar , NavbarBrand , NavbarToggler , NavItem , NavLink } from 'reactstrap' ; import { Link } from 'react-router-dom' ; import { useAuth0 } from '../auth0-wrapper' ; //allow to use methods from the authentication wrapper import './NavMenu.css' ; //change class component to funcion component const NavMenu = () => { //define methods from the authentication wrapper const { isAuthenticated , loginWithRedirect , logout } = useAuth0 (); return ( < header > < Navbar className = \"navbar-expand-sm navbar-toggleable-sm ng-white border-bottom box-shadow mb-3\" light > < Container > < NavbarBrand tag = { Link } to = \"/\" > Trips < /NavbarBrand> { isAuthenticated ? ( < ul className = \"navbar-nav flex-grow\" > < NavItem > < NavLink tag = { Link } className = \"text-dark\" to = \"/create\" > Create < /NavLink> < /NavItem> < NavItem > < NavLink tag = { Link } className = \"text-dark\" to = \"/trips\" > Trips < /NavLink> < /NavItem> < NavItem > < button className = \"btn btn-danger\" onClick = {() => logout ()} > Log out < /button> < /NavItem> < /ul> ) : ( < ul className = \"navbar-nav flex-grow\" > < NavItem > < button className = \"btn btn-success\" onClick = {() => loginWithRedirect ()} > Log In < /button> < /NavItem> < /ul> )} < /Container> < /Navbar> < /header> ); } //allow to use the NavMenu export default NavMenu ; Update Index (.\\src\\index.js) ... import { Auth0Provider } from './auth0-wrapper' ; import config from './auth_config.json' //function that will route the user after the user logs in const onRedirectCallback = appState => { window . history . replaceState ( {}, document . title , appState && appState . targetUrl ? appState . targetUrl : window . location . pathname ); }; ReactDOM . render ( < Auth0Provider domain = { config . domain } client_id = { config . clientId } redirect_uri = { window . location . origin } onRedirectCallback = { onRedirectCallback } > < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter> < /Auth0Provider> , rootElement ); registerServiceWorker (); .\\src\\auth_config.json { \"domain\" : \"rnietoe.eu.auth0.com\" , \"clientId\" : \"0D2Ab46Xq5dBEueBDtX4Cg9vl8rsUyz5\" } .\\src\\components\\Layout.js ... import NavMenu from './components/NavMenu' ; //instead of { NavMenu } ...","title":"Building React and ASP.NET Core Applications"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#building-react-and-aspnet-core-applications","text":"","title":"Building React and ASP.NET Core Applications"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#create-new-application","text":"dotnet new react dotnet run Other Web Application Templates: dotnet new mvc webapp angular react reactredux webapi ...","title":"Create new Application"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#indexhtml","text":"All the React related files can be found inside the ClientApp folder. Inside the public folder, we can see the index.html file. This is the first page that is loaded when the application starts. < div id = \"root\" ></ div > This will be the only html file in the entire application since React is generally written in JavaScript","title":"index.html"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#indexjs","text":"Inside the src folder, you can see the index.js file. This is the JavaScript file that corresponds to the index.html . const rootElement = document . getElementById ( 'root' ); ReactDOM . render ( < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter>, rootElement );","title":"index.js"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#appjs","text":"The app component is the main component in React which acts as a container for all the other components. render () { return ( < Layout > < Route exact path = '/' component = { Home } /> < Route path = '/create' component = { Create } /> < Route path = '/trips' component = { Trips } /> < /Layout> ); }","title":"App.js"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#web-api","text":"We can create a folder as \"Data\" to put in all the data-related files such as models, services, etc. Then create: /Data/Models /Data/Services /Controllers","title":"Web API"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#getting-starting-with-react","text":"In React, the component is an individual item that can obtain other items. Extension file is .jsx Getting data into components using: props : html attribute context : data through many levels state : place to store data JSX lifecycle methods ( componentDidMount , etc.) You can only return one single parent module from a react component. This is why we always return a parents div in the render method: render () { return ( < div >< /div> ); }","title":"Getting starting with React"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#list-view","text":"axios is required: npm install axios --save componentDidMount lifecycle method is used to get the data from the API endpoint once the UI has been loaded: import React , { Component } from 'react' ; import axios from 'axios' ; export class Trips extends Component { constructor ( props ){ super ( props ); this . state = { trips : [], loading : true } } //Once the component is mounted, it sends a request to get all data componentDidMount (){ this . populateTripsData (); } populateTripsData (){ axios . get ( \"api/Trips/GetTrips\" ). then ( result => { const response = result . data ; this . setState ({ trips : response , loading : false }); }) } renderAllTripsTable ( trips ){ return ( < table className = \"table table-striped\" > < thead > < tr > < th > Name < /th> < th > Description < /th> < th > Date started < /th> < th > Date completed < /th> < th > Action < /th> < /tr> < /thead> < tbody > { trips . map ( trip => ( < tr key = { trip . id } > < td > { trip . name } < /td> < td > { trip . description } < /td> < td > { new Date ( trip . dateStarted ). toISOString (). slice ( 0 , 10 )} < /td> < td > { trip . dateCompleted ? new Date ( trip . dateCompleted ). toISOString (). slice ( 0 , 10 ) : '-' } < /td> < td > - < /td> < /tr> )) } < /tbody> < /table> ); } render (){ let content = this . state . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( this . renderAllTripsTable ( this . state . trips ) ) return ( < div > < h1 > All trips < /h1> < p > Here you can see all trips < /p> { content } < /div> ); } }","title":"List View"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#create-view","text":"import React , { Component } from 'react' ; import axios from 'axios' ; export class Create extends Component { constructor ( props ){ super ( props ); this . onChange = this . onChange . bind ( this ); this . onSubmit = this . onSubmit . bind ( this ); this . state = { name : '' , description : '' , dateStarted : null , dateCompleted : null } } onChange ( value , event ) { this . setState ({ [ event . target . name ] : value }); } onSubmit ( e ){ e . preventDefault (); let tripObject = { Id : Math . floor ( Math . random () * 1000 ), name : this . state . name , description : this . state . description , dateStarted : this . state . dateStarted , dateCompleted : this . state . dateCompleted } //post request and redirect to /trips component const { history } = this . props ; axios . post ( \"api/Trips/AddTrip\" , tripObject ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div className = \"trip-form\" > < h3 > Add new trip < /h3> < form onSubmit = { this . onSubmit } > < div className = \"form-group\" > < label > Trip name : < /label> < input type = \"text\" className = \"form-control\" value = { this . state . name } onChange = { this . onChange } /> < /div> < div className = \"form-group\" > < label > Trip description : < /label> < textarea type = \"text\" className = \"form-control\" value = { this . state . description } onChange = { this . onChange } /> < /div> < div className = \"row\" > < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of start : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateStarted } onChange = { this . onChange } /> < /div> < /div> < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of completion : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateCompleted } onChange = { this . onChange } /> < /div> < /div> < /div> < div className = \"form-group\" > < input type = \"submit\" value = \"Add trip\" className = \"btn btn-primary\" /> < /div> < /form> < /div> ) } }","title":"Create View"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#edit-view","text":"Trips.js onTripUpdate ( id ){ const { history } = this . props ; history . push ( '/update/' + id ); } ... < div className = \"form-group\" > < button onClick = {() => this . onTripUpdate ( trip . id )} className = \"btn btn-success\" > Update < /button> < /div> Apps.js render () { return ( < Layout > ... < Route path = '/update/:id' component = { Update } /> < /Layout> ); } Update.jsx import React , { Component } from 'react' ; import axios from 'axios' ; export class Update extends Component { constructor ( props ){ super ( props ); this . onChange = this . onChange . bind ( this ); this . onUpdateCancel = this . onUpdateCancel . bind ( this ); this . onSubmit = this . onSubmit . bind ( this ); this . state = { name : '' , description : '' , dateStarted : null , dateCompleted : null } } //Once the component is mounted, it sends a request to get data by id componentDidMount (){ const { id } = this . props . match . params ; axios . get ( \"api/Trips/SingleTrip/\" + id ). then ( trip => { const response = trip . data ; this . setState ({ name : response . name , description : response . description , dateStarted : new Date ( response . dateStarted ). toISOString (). slice ( 0 , 10 ), dateCompleted : response . dateCompleted ? new Date ( response . dateCompleted ). toISOString (). slice ( 0 , 10 ) : null }) }) } onChange ( value , event ) { this . setState ({ [ event . target . name ] : value }); } onUpdateCancel (){ //cancel and redirect to /trips component const { history } = this . props ; history . push ( '/trips' ); } onSubmit ( e ){ e . preventDefault (); let tripObject = { name : this . state . name , description : this . state . description , dateStarted : new Date ( this . state . dateStarted ). toISOString (), dateCompleted : this . state . dateCompleted ? new Date ( this . state . dateCompleted ). toISOString () : null } //PUT request and redirect to /trips component const { history } = this . props ; const { id } = this . props . match . params ; axios . put ( \"api/Trips/updateTrip/\" + id , tripObject ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div className = \"trip-form\" > < h3 > Add new trip < /h3> < form onSubmit = { this . onSubmit } > < div className = \"form-group\" > < label > Trip name : < /label> < input type = \"text\" className = \"form-control\" value = { this . state . name } onChange = { this . onChangeName } /> < /div> < div className = \"form-group\" > < label > Trip description : < /label> < textarea type = \"text\" className = \"form-control\" value = { this . state . description } onChange = { this . onChangeDescription } /> < /div> < div className = \"row\" > < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of start : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateStarted } onChange = { this . onChangeDateStarted } /> < /div> < /div> < div className = \"col col-md-6 col-sm-6 col-xs-12\" > < div className = \"form-group\" > < label > Date of completion : < /label> < input type = \"date\" className = \"form-control\" value = { this . state . dateCompleted } onChange = { this . onChangeDateCompleted } /> < /div> < /div> < /div> < div className = \"form-group\" > < button onClick = { this . onUpdateCancel } className = \"btn btn-default\" > Cancel < /button> < button type = \"submit\" className = \"btn btn-success\" > Update < /button> < /div> < /form> < /div> ) } }","title":"Edit View"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#delete-view","text":"Trips.js onTripDelete ( id ){ const { history } = this . props ; history . push ( '/delete/' + id ); } ... < div className = \"form-group\" > < button onClick = {() => this . onTripDelete ( trip . id )} className = \"btn btn-danger\" > Delete < /button> < /div> Apps.js render () { return ( < Layout > ... < Route path = '/delete/:id' component = { Delete } /> < /Layout> ); } Delete.jsx import React , { Component } from 'react' ; import axios from 'axios' ; export class Delete extends Component { constructor ( props ){ super ( props ); this . onCancel = this . onCancel . bind ( this ); this . onConfirmation = this . onConfirmation . bind ( this ); this . state = { name : \"\" , description : \"\" } } //Once the component is mounted, it sends a request to get data by id componentDidMount (){ const { id } = this . props . match . params ; axios . get ( \"api/Trips/SingleTrip/\" + id ). then ( trip => { const response = trip . data ; this . setState ({ name : response . name , description : response . description }) }) } onCancel ( e ){ //cancel and redirect to /trips component const { history } = this . props ; history . push ( '/trips' ); } onConfirmation ( e ){ const { id } = this . props . match . params ; const { history } = this . props ; axios . delete ( \"api/Trips/DeleteTrip/\" + id ). then ( result => { history . push ( '/trips' ); }) } render (){ return ( < div style = {{ marginTop : 10 }} > < h2 > Delete trip confirmation < /h2> < div class = \"card\" > < div class = \"card-body\" > < h4 class = \"card-title\" > { this . state . name } < /h4> < p class = \"card-text\" > { this . state . description } < /p> < button onClick = { this . onCancel } class = \"btn btn-default\" > Cancel < /button> < button onClick = { this . onConfirmation } class = \"btn btn-danger\" > Confirm < /button> < /div> < /div> < /div> ) } }","title":"Delete View"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#handling-errors","text":"Using axios.catch(error) import React , { Component } from 'react' ; import axios from 'axios' ; export class Trips extends Component { constructor ( props ){ super ( props ); this . onTripUpdate = this . onTripUpdate . bind ( this ); this . onTripDelete = this . onTripDelete . bind ( this ); this . state = { trips : [], loading : true , failed : false , error : '' //error message } } ... populateTripsData (){ axios . get ( \"api/Trips/GetTrips\" ). then ( result => { const response = result . data ; this . setState ({ trips : response , loading : false , failed : false , error : \"\" }); }). catch ( error => { this . setState ({ trips : [], loading : false , failed : true , error : \"Trips could not be loaded\" }); }); } ... render (){ let content = this . state . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( this . state . failed ? ( < div className = \"text-danger\" > < em > { this . state . error } < /em> < /div> ) : ( this . renderAllTripsTable ( this . state . trips )) ) return ( < div > < h1 > All trips < /h1> < p > Here you can see all trips < /p> { content } < /div> ); } }","title":"Handling errors"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#redux","text":"Redux is a state magement tool for JS applications. The entire state of an application is stored in one central location: The stores store the whole state. The reducers return parts of this state The actions are predefined, user-triggered events that define how a state should change. Require: npm install redux react-redux react-thunk npm install react-thunk --save","title":"Redux"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#actions","text":"Actions are payloads of information that send data from your application to your store .\\src\\actions\\tripsActions.js import axios from 'axios' ; export const GET_ALL_TRIPS_REQUEST = 'GET_ALL_TRIPS_REQUEST' ; export const GET_ALL_TRIPS_SUCCESS = 'GET_ALL_TRIPS_SUCCESS' ; export const GET_ALL_TRIPS_ERROR = 'GET_ALL_TRIPS_ERROR' ; const getTripsSuccess = payload => ({ type : GET_ALL_TRIPS_SUCCESS , payload }); const getTripsError = payload => ({ type : GET_ALL_TRIPS_ERROR , payload }); //export the action to be able to use it export const getAllTrips = () => dispatch => { dispatch ({ type : GET_ALL_TRIPS_REQUEST }); return axios . get ( 'api/Trips/GetTrips' ). then ( res => { const response = res . data ; dispatch ( getTripsSuccess ( response )); }). catch ( error => { dispatch ( getTripsError ( \"Something went wrong!\" )); return Promise . reject ({}); }) }","title":"Actions"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#reducers","text":"Reducers specify how the application's state change .\\src\\reducers\\tripsReducers.js import { GET_ALL_TRIPS_REQUEST , GET_ALL_TRIPS_SUCCESS , GET_ALL_TRIPS_ERROR } from '../actions/tripActions' ; const INITIAL_STATE = { loading : false , hasError : false , error : null , data : [] } export default ( state = INITIAL_STATE , action ) => { switch ( action . type ){ case GET_ALL_TRIPS_REQUEST : return { ... state , loading : true }; case GET_ALL_TRIPS_SUCCESS : return { ... state , loading : false , hasError : false , data : action . payload }; case GET_ALL_TRIPS_ERROR : return { ... state , loading : false , hasError : true , error : action . payload }; default : return state ; } } .\\src\\reducers\\index.js import { combineReducers } from 'redux' ; import tripReducers from './tripReducers' ; const rootReducer = combineReducers ({ trips : tripReducers }); //export the root reducer to be able to use it export default rootReducer ;","title":"Reducers"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#store","text":"getSate() - access application state dispatch(action) - udpate application state subscribe(listener) - register and unregister listeners Only have a single store in a Redux application: .\\src\\store\\store.js import { createStore , applyMiddleware } from 'redux' import thunk from 'redux-thunk' ; import rootReducer from '../reducers' ; const configureStore = () => applyMiddleware ( thunk )( createStore )( rootReducer ); //export the configure store to be able to use it export default configureStore ;","title":"Store"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#list-view-with-redux","text":"We want to load all the trips using the redux instead of just calling them directly from our component to web API: .\\src\\index.js import { Provider } from 'react-redux' ; //required for redux import configureStore from './store/store' ; //required for redux ... const store = configureStore ({}); ... ReactDOM . render ( < Provider store = { store } > //using the store < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter> < /Provider>, rootElement ); ... .\\src\\Apps.js ... import Trips from './components/Trip/Trips' ; //instead of { Trips } ... .\\src\\components\\Trip\\Trips.jsx import { connect } from 'react-redux' ; import { getAllTrips } from '../../actions/tripActions' ; ... componentDidMount (){ this . props . getAllTrips (); //get trips from tripActions } componentDidUpdate ( prevProps ){ if ( prevProps . trips . data != this . props . trips . data ){ this . setState ({ trips : this . props . trips . data }); } } ... render (){ let content = this . props . trips . loading ? ( < p > < em > Loading ... < /em> < /p> ) : ( //build the table if there is one or more trips this . state . trips . length && this . renderAllTripsTable ( this . state . trips ) ); ... const mapStateToProps = ({ trips }) => ({ trips }); //Allow the Trips component to access data from store. export default connect ( mapStateToProps , { getAllTrips })( Trips );","title":"List View with redux"},{"location":"React/Building-React-and-ASP.NET-Core-Applications/#authentication-using-auth0","text":"Log in into https://auth0.com . signed in as rnietoe.eu.auth0.com Create Single Page Web Application and configure Auth0 from Settings Set Allowed Callback Urls equals https://localhost:5001 Set Allowed Web Origins equals https://localhost:5001 Set Allowed Logout Urls equals https://localhost:5001 Install dependencies: npm install @auth0-spa-js --save Install the Auth0 React wrapper ( react-auth0-spa.js ) in our src directory. Copy paste from documenation . Create/Update the NavMenu component (./src/components/NavMenu.js) import React , { Component } from 'react' ; import { Collapse , Container , Navbar , NavbarBrand , NavbarToggler , NavItem , NavLink } from 'reactstrap' ; import { Link } from 'react-router-dom' ; import { useAuth0 } from '../auth0-wrapper' ; //allow to use methods from the authentication wrapper import './NavMenu.css' ; //change class component to funcion component const NavMenu = () => { //define methods from the authentication wrapper const { isAuthenticated , loginWithRedirect , logout } = useAuth0 (); return ( < header > < Navbar className = \"navbar-expand-sm navbar-toggleable-sm ng-white border-bottom box-shadow mb-3\" light > < Container > < NavbarBrand tag = { Link } to = \"/\" > Trips < /NavbarBrand> { isAuthenticated ? ( < ul className = \"navbar-nav flex-grow\" > < NavItem > < NavLink tag = { Link } className = \"text-dark\" to = \"/create\" > Create < /NavLink> < /NavItem> < NavItem > < NavLink tag = { Link } className = \"text-dark\" to = \"/trips\" > Trips < /NavLink> < /NavItem> < NavItem > < button className = \"btn btn-danger\" onClick = {() => logout ()} > Log out < /button> < /NavItem> < /ul> ) : ( < ul className = \"navbar-nav flex-grow\" > < NavItem > < button className = \"btn btn-success\" onClick = {() => loginWithRedirect ()} > Log In < /button> < /NavItem> < /ul> )} < /Container> < /Navbar> < /header> ); } //allow to use the NavMenu export default NavMenu ; Update Index (.\\src\\index.js) ... import { Auth0Provider } from './auth0-wrapper' ; import config from './auth_config.json' //function that will route the user after the user logs in const onRedirectCallback = appState => { window . history . replaceState ( {}, document . title , appState && appState . targetUrl ? appState . targetUrl : window . location . pathname ); }; ReactDOM . render ( < Auth0Provider domain = { config . domain } client_id = { config . clientId } redirect_uri = { window . location . origin } onRedirectCallback = { onRedirectCallback } > < BrowserRouter basename = { baseUrl } > < App /> < /BrowserRouter> < /Auth0Provider> , rootElement ); registerServiceWorker (); .\\src\\auth_config.json { \"domain\" : \"rnietoe.eu.auth0.com\" , \"clientId\" : \"0D2Ab46Xq5dBEueBDtX4Cg9vl8rsUyz5\" } .\\src\\components\\Layout.js ... import NavMenu from './components/NavMenu' ; //instead of { NavMenu } ...","title":"Authentication using Auth0"},{"location":"React/Become-a-React-Developer/","text":"Index \u00b6 Learning React.js React.js Essential Training Learning Redux React.js: Building an Interface React: Creating and Hosting a Full-Stack Site React: Ecosystems","title":"Index"},{"location":"React/Become-a-React-Developer/#index","text":"Learning React.js React.js Essential Training Learning Redux React.js: Building an Interface React: Creating and Hosting a Full-Stack Site React: Ecosystems","title":"Index"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/","text":"Learning React.js \u00b6 Getting Started \u00b6 React is a JS library built at Facebook. Setup React Developer Tools using chrome: React Developer Tools extension . Edit extension setting with \"Allow access to file URLs\" enabled. React Element and JSX \u00b6 Create a react element using React.createElement : <!DOCTYPE html> < html > < head > < title > React Project </ title > < script src = \"https://unpkg.com/react@16.7.0/umd/react.development.js\" ></ script > < script src = \"https://unpkg.com/react-dom@16.7.0/umd/react-dom.development.js\" ></ script > </ head > < body > < div id = \"root\" ></ div > < script type = \"text/javascript\" > ReactDOM . render ( React . createElement ( \"div\" , //type null , //properties React . createElement ( //children \"h1\" , //type null , //properties \"Oh hello!\" //children ) ), document . getElementById ( \"root\" ) ); </ script > </ body > </ html > Using JSX (JS as XML). Require script type=\"text/babel\" and className instead of class: < script src = \"https://unpkg.com/@babel/standalone/babel.min.js\" ></ script > < script type = \"text/babel\" > let city = \"Madrid\" ; ReactDOM . render ( < h1 className = \"heading\" > Hello { city } < /h1>, document . getElementById ( \"root\" ) ); </ script > React Components \u00b6 All react components should be capitalized using Pascal Case Functions as const : const Hello = ( props ) => { return ( < div className = \"heading\" > < h1 > Welcome to { props . library } < /h1> < p > { props . message } < /p> < /div> ) }; ReactDOM . render ( < Hello library = \"React\" message = \"Enjoy!\" /> , document . getElementById ( \"root\" ) ); Properties: see below example using a functon with properties const Lake = ({ name , country }) => { return ( < div className = \"heading\" > < h1 > { name } < /h1> < p > { country } < /p> < /div> ) }; const App = () => ( < div > < Lake name = \"Lake Tahoe\" country = \"USA\" /> < Lake name = \"Angora Lake\" country = \"Angola\" /> < Lake name = \"Shirley Lake\" country = \"Australia\" /> < /div> ); ReactDOM . render ( < App /> , document . getElementById ( \"root\" ) ); Classes: always include the render method const Lake = ({ name }) => < h1 > { name } < /h1>; class App extends React . Component { render () { return ( < div > < Lake name = \"Lake Tahoe\" /> < Lake name = \"Angora Lake\" /> < Lake name = \"Shirley Lake\" /> < /div> ) } } See below example using a class with properties class Message extends React . Component { render () { return ( < div > < h1 style = {{ color : this . props . color }} > { this . props . msg } < /h1> < p > I will check back in { this . props . minutes } minutes < /p> < /div> ) } } ReactDOM . render ( < Message color = \"blue\" msg = \"how are you?\" minutes = { 5 } /> , document . getElementById ( 'root' ) ) State: When a component's State data changes, the render function will be called again to re-render the state change. class App extends React . Component { state = { logged : false } render () { return ( < div > The user is { this . state . logged ? \"logged in\" : \"logged out\" }. < /div> ) } } Render Components \u00b6 Render from a list: const lakeList = [ \"Echo Lake\" , \"Maud Lake\" , \"Cascade Lake\" ]; const App = ({ lakes }) => ( < ul > { lakes . map (( lake , i ) => < li key = { i } > { lake } < /li> )} < /ul> ) ReactDOM . render ( < App lakes = { lakeList } /> , document . getElementById ( \"root\" ) ); Render from a list of objects: const lakeList = [ { id : 1 , name : \"Echo Lake\" , trailhead : \"Echo Lake\" }, { id : 2 , name : \"Maud Lake\" , trailhead : \"Wright's Lake\" }, { id : 3 , name : \"Cascade Lake\" , trailhead : \"Bayview\" } ]; const App = ({ lakes }) => ( < ul > { lakes . map ( lake => < li key = { lake . id } > { lake . name } | Trailhead : { lake . trailhead } < /li> )} < /ul> ) ReactDOM . render ( < App lakes = { lakeList } /> , document . getElementById ( \"root\" ) ); React Events \u00b6 state = { loggedIn : false } logIn = () => this . setState ({ loggedIn : true }) logOut = () => this . setState ({ loggedIn : false }) render () { return ( < div > < button onClick = { this . logIn } > Log In < /button> < button onClick = { this . logOut } > Log Out < /button> < div > The user is { this . state . logged ? \"logged in\" : \"logged out\" }. < /div> < /div> ) } If Else \u00b6 const Lake = ({ name }) => < h1 > { name } < /h1>; const Resort = ({ name }) => < h1 > { name } < /h1>; const App = ({ summer }) => ( < div > // If summer then Lake Else Resort { summer ? < Lake name = \"Lake1\" /> : < Resort name = \"Resort2\" /> } < /div> ) ReactDOM . render ( < App summer = { false } /> , document . getElementById ( \"root\" ) ); Create React App \u00b6 Create React App requires nodejs npm -v node -v npm install -g create-react-app cd \"new_project_folder\" create-react-app \"project_name\" Next steps: Replace ReactDOM.render at /src/index.js with the one we have at index.html Copy const lakeList at /src/index.js from index.html Replace function App at /src/App.js with the one we have at index.html Install dependencies and start: cd lake-app npm install npm start Browse to http://localhost:3000/ to check the result. To create a production build: npm run build The build folder is ready to be deployed. You may serve it with a static server: npm install -g serve serve -s build","title":"Learning React.js"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#learning-reactjs","text":"","title":"Learning React.js"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#getting-started","text":"React is a JS library built at Facebook. Setup React Developer Tools using chrome: React Developer Tools extension . Edit extension setting with \"Allow access to file URLs\" enabled.","title":"Getting Started"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#react-element-and-jsx","text":"Create a react element using React.createElement : <!DOCTYPE html> < html > < head > < title > React Project </ title > < script src = \"https://unpkg.com/react@16.7.0/umd/react.development.js\" ></ script > < script src = \"https://unpkg.com/react-dom@16.7.0/umd/react-dom.development.js\" ></ script > </ head > < body > < div id = \"root\" ></ div > < script type = \"text/javascript\" > ReactDOM . render ( React . createElement ( \"div\" , //type null , //properties React . createElement ( //children \"h1\" , //type null , //properties \"Oh hello!\" //children ) ), document . getElementById ( \"root\" ) ); </ script > </ body > </ html > Using JSX (JS as XML). Require script type=\"text/babel\" and className instead of class: < script src = \"https://unpkg.com/@babel/standalone/babel.min.js\" ></ script > < script type = \"text/babel\" > let city = \"Madrid\" ; ReactDOM . render ( < h1 className = \"heading\" > Hello { city } < /h1>, document . getElementById ( \"root\" ) ); </ script >","title":"React Element and JSX"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#react-components","text":"All react components should be capitalized using Pascal Case Functions as const : const Hello = ( props ) => { return ( < div className = \"heading\" > < h1 > Welcome to { props . library } < /h1> < p > { props . message } < /p> < /div> ) }; ReactDOM . render ( < Hello library = \"React\" message = \"Enjoy!\" /> , document . getElementById ( \"root\" ) ); Properties: see below example using a functon with properties const Lake = ({ name , country }) => { return ( < div className = \"heading\" > < h1 > { name } < /h1> < p > { country } < /p> < /div> ) }; const App = () => ( < div > < Lake name = \"Lake Tahoe\" country = \"USA\" /> < Lake name = \"Angora Lake\" country = \"Angola\" /> < Lake name = \"Shirley Lake\" country = \"Australia\" /> < /div> ); ReactDOM . render ( < App /> , document . getElementById ( \"root\" ) ); Classes: always include the render method const Lake = ({ name }) => < h1 > { name } < /h1>; class App extends React . Component { render () { return ( < div > < Lake name = \"Lake Tahoe\" /> < Lake name = \"Angora Lake\" /> < Lake name = \"Shirley Lake\" /> < /div> ) } } See below example using a class with properties class Message extends React . Component { render () { return ( < div > < h1 style = {{ color : this . props . color }} > { this . props . msg } < /h1> < p > I will check back in { this . props . minutes } minutes < /p> < /div> ) } } ReactDOM . render ( < Message color = \"blue\" msg = \"how are you?\" minutes = { 5 } /> , document . getElementById ( 'root' ) ) State: When a component's State data changes, the render function will be called again to re-render the state change. class App extends React . Component { state = { logged : false } render () { return ( < div > The user is { this . state . logged ? \"logged in\" : \"logged out\" }. < /div> ) } }","title":"React Components"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#render-components","text":"Render from a list: const lakeList = [ \"Echo Lake\" , \"Maud Lake\" , \"Cascade Lake\" ]; const App = ({ lakes }) => ( < ul > { lakes . map (( lake , i ) => < li key = { i } > { lake } < /li> )} < /ul> ) ReactDOM . render ( < App lakes = { lakeList } /> , document . getElementById ( \"root\" ) ); Render from a list of objects: const lakeList = [ { id : 1 , name : \"Echo Lake\" , trailhead : \"Echo Lake\" }, { id : 2 , name : \"Maud Lake\" , trailhead : \"Wright's Lake\" }, { id : 3 , name : \"Cascade Lake\" , trailhead : \"Bayview\" } ]; const App = ({ lakes }) => ( < ul > { lakes . map ( lake => < li key = { lake . id } > { lake . name } | Trailhead : { lake . trailhead } < /li> )} < /ul> ) ReactDOM . render ( < App lakes = { lakeList } /> , document . getElementById ( \"root\" ) );","title":"Render Components"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#react-events","text":"state = { loggedIn : false } logIn = () => this . setState ({ loggedIn : true }) logOut = () => this . setState ({ loggedIn : false }) render () { return ( < div > < button onClick = { this . logIn } > Log In < /button> < button onClick = { this . logOut } > Log Out < /button> < div > The user is { this . state . logged ? \"logged in\" : \"logged out\" }. < /div> < /div> ) }","title":"React Events"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#if-else","text":"const Lake = ({ name }) => < h1 > { name } < /h1>; const Resort = ({ name }) => < h1 > { name } < /h1>; const App = ({ summer }) => ( < div > // If summer then Lake Else Resort { summer ? < Lake name = \"Lake1\" /> : < Resort name = \"Resort2\" /> } < /div> ) ReactDOM . render ( < App summer = { false } /> , document . getElementById ( \"root\" ) );","title":"If Else"},{"location":"React/Become-a-React-Developer/1-Learning-React-js/#create-react-app","text":"Create React App requires nodejs npm -v node -v npm install -g create-react-app cd \"new_project_folder\" create-react-app \"project_name\" Next steps: Replace ReactDOM.render at /src/index.js with the one we have at index.html Copy const lakeList at /src/index.js from index.html Replace function App at /src/App.js with the one we have at index.html Install dependencies and start: cd lake-app npm install npm start Browse to http://localhost:3000/ to check the result. To create a production build: npm run build The build folder is ready to be deployed. You may serve it with a static server: npm install -g serve serve -s build","title":"Create React App"},{"location":"React/Become-a-React-Developer/2-React.js-Essential-Training/","text":"React.js Essential Training \u00b6 Create react elements using JSX at /src/index.js import React from 'react' //import the react library import ReactDOM from 'react-dom' //needed to render React components var style = { backgroundColor : 'orange' , color : 'white' , fontFamily : 'Arial' } ReactDOM . render ( < div style = { style } > < h1 id = \"heading-element\" > Hello World !< /h1> < p > We 're glad you' re here !< /p> < /div>, document . getElementById ( 'root' ) //root div at /public/index.html ) Refactor React Components \u00b6 Create a class component with a custom method. See below example like SkiDayCounter: import React , { Component } from 'react' //replace React.Component by Component import { render } from 'react-dom' //replace ReactDOM.render by render let skiData = { total : 50 , powder : 20 , backcountry : 10 , goal : 100 } class SkiDayCounter extends Component { //using Component instead of React.Component calcGoalProgress = ( total , goal ) => { //custom method for this component return total / goal * 100 + '%' } render () { const { total , powder , backcountry , goal } = this . props return ( < section > < div >< p > Total Days : { total } < /p></div> < div >< p > Powder Days : { powder } < /p></div> < div >< p > Backcountry Days : { backcountry } < /p></div> < div >< p > Goal Progress : { this . calcGoalProgress ( total , goal )} < /p></div> < /section> ) } } render ( //using render instead of ReactDOM.render < SkiDayCounter total = { skiData . total } powder = { skiData . powder } backcountry = { skiData . backcountry } goal = { skiData . goal } /> , document . getElementById ( 'root' ) ) Create a function component with custom functions const calcGoalProgress = ( total , goal ) => { return total / goal * 100 + '%' } const SkiDayCounter = ({ total , powder , backcountry , goal }) => { return ( < section > < div > < p > Total Days : { total } < /p> < /div> < div > < p > Powder Days : { powder } < /p> < /div> < div > < p > Backcountry Days : { backcountry } < /p> < /div> < div > < p > Goal Progress : { calcGoalProgress ( total , goal )} < /p> < /div> < /section> ) } Props and State \u00b6 Compose components sample: import React from 'react' import { render } from 'react-dom' const Book = ({ title , author , pages }) => { return ( < section > < h2 > { title } < /h2> < p > by : { author } < /p> < p > Pages : { pages } pages < /p> < /section> ) } const Library = () => { return ( < div > < Book title = \"The Sun Also Rises\" author = \"Ernest Hemingway\" pages = { 260 } /> < Book title = \"White Teeth\" author = \"Zadie Smith\" pages = { 480 } /> < Book title = \"Cat's Cradle\" author = \"Kurt Vonnegut\" pages = { 304 } /> < /div> ) } render ( < Library /> , document . getElementById ( 'root' ) ) Refactor components sample let bookList = [ { \"title\" : \"Hunger\" , \"author\" : \"Roxane Gay\" , \"pages\" : 320 }, { \"title\" : \"The Sun Also Rises\" , \"author\" : \"Ernest Hemingway\" , \"pages\" : 260 }, { \"title\" : \"White Teeth\" , \"author\" : \"Zadie Smith\" , \"pages\" : 480 }, { \"title\" : \"Cat's Cradle\" , \"author\" : \"Kurt Vonnegut\" , \"pages\" : 304 } ] const Book = ({ title , author , pages }) => { return ( < section > < h2 > { title } < /h2> < p > by : { author } < /p> < p > Pages : { pages } pages < /p> < /section> ) } const Library = ({ books }) => { return ( < div > { books . map ( ( book , i ) => < Book key = { i } title = { book . title } author = { book . author } pages = { book . pages } /> )} < /div> ) } render ( < Library books = { bookList } /> , document . getElementById ( 'root' ) )","title":"React.js Essential Training"},{"location":"React/Become-a-React-Developer/2-React.js-Essential-Training/#reactjs-essential-training","text":"Create react elements using JSX at /src/index.js import React from 'react' //import the react library import ReactDOM from 'react-dom' //needed to render React components var style = { backgroundColor : 'orange' , color : 'white' , fontFamily : 'Arial' } ReactDOM . render ( < div style = { style } > < h1 id = \"heading-element\" > Hello World !< /h1> < p > We 're glad you' re here !< /p> < /div>, document . getElementById ( 'root' ) //root div at /public/index.html )","title":"React.js Essential Training"},{"location":"React/Become-a-React-Developer/2-React.js-Essential-Training/#refactor-react-components","text":"Create a class component with a custom method. See below example like SkiDayCounter: import React , { Component } from 'react' //replace React.Component by Component import { render } from 'react-dom' //replace ReactDOM.render by render let skiData = { total : 50 , powder : 20 , backcountry : 10 , goal : 100 } class SkiDayCounter extends Component { //using Component instead of React.Component calcGoalProgress = ( total , goal ) => { //custom method for this component return total / goal * 100 + '%' } render () { const { total , powder , backcountry , goal } = this . props return ( < section > < div >< p > Total Days : { total } < /p></div> < div >< p > Powder Days : { powder } < /p></div> < div >< p > Backcountry Days : { backcountry } < /p></div> < div >< p > Goal Progress : { this . calcGoalProgress ( total , goal )} < /p></div> < /section> ) } } render ( //using render instead of ReactDOM.render < SkiDayCounter total = { skiData . total } powder = { skiData . powder } backcountry = { skiData . backcountry } goal = { skiData . goal } /> , document . getElementById ( 'root' ) ) Create a function component with custom functions const calcGoalProgress = ( total , goal ) => { return total / goal * 100 + '%' } const SkiDayCounter = ({ total , powder , backcountry , goal }) => { return ( < section > < div > < p > Total Days : { total } < /p> < /div> < div > < p > Powder Days : { powder } < /p> < /div> < div > < p > Backcountry Days : { backcountry } < /p> < /div> < div > < p > Goal Progress : { calcGoalProgress ( total , goal )} < /p> < /div> < /section> ) }","title":"Refactor React Components"},{"location":"React/Become-a-React-Developer/2-React.js-Essential-Training/#props-and-state","text":"Compose components sample: import React from 'react' import { render } from 'react-dom' const Book = ({ title , author , pages }) => { return ( < section > < h2 > { title } < /h2> < p > by : { author } < /p> < p > Pages : { pages } pages < /p> < /section> ) } const Library = () => { return ( < div > < Book title = \"The Sun Also Rises\" author = \"Ernest Hemingway\" pages = { 260 } /> < Book title = \"White Teeth\" author = \"Zadie Smith\" pages = { 480 } /> < Book title = \"Cat's Cradle\" author = \"Kurt Vonnegut\" pages = { 304 } /> < /div> ) } render ( < Library /> , document . getElementById ( 'root' ) ) Refactor components sample let bookList = [ { \"title\" : \"Hunger\" , \"author\" : \"Roxane Gay\" , \"pages\" : 320 }, { \"title\" : \"The Sun Also Rises\" , \"author\" : \"Ernest Hemingway\" , \"pages\" : 260 }, { \"title\" : \"White Teeth\" , \"author\" : \"Zadie Smith\" , \"pages\" : 480 }, { \"title\" : \"Cat's Cradle\" , \"author\" : \"Kurt Vonnegut\" , \"pages\" : 304 } ] const Book = ({ title , author , pages }) => { return ( < section > < h2 > { title } < /h2> < p > by : { author } < /p> < p > Pages : { pages } pages < /p> < /section> ) } const Library = ({ books }) => { return ( < div > { books . map ( ( book , i ) => < Book key = { i } title = { book . title } author = { book . author } pages = { book . pages } /> )} < /div> ) } render ( < Library books = { bookList } /> , document . getElementById ( 'root' ) )","title":"Props and State"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/","text":"Web Security: OAuth and OpenID Connect \u00b6 https://www.linkedin.com/learning/web-security-oauth-and-openid-connect?u=1009514 OAuth 2.0 Servers Map of OAuth 2.0 Specs Aaron Parecki's book OAuth 2.0 Simplified What is OAuth? \u00b6 Some OAuth Servers: https://oauth2.thephpleague.com https://developers.google.com/oauthplayground https://openidconnect.net https://developer.okta.com A token introspection tool: https://www.jsonwebtoken.io flow = grant type scopes = permissions AuthZ with: API keys Id & Secret OAuth OpenID Connect (OIDC) is an OAuth 2.0 identity extension \u2013 only for users. Core Terminology \u00b6 Main OAuth extensions /authorize : grant permission to the resource. Could return an authZ code or an access token /token : trade an authZ code or refresh token for an access token /revoke : deactivate (invalidate) a token. Valid for access or refresh tokens /instrospect : learn more about the token /register : create new OAuth clients /userinfo : retrieve profile information about the authenticated user /.well-known/openid-configuration : which endpoints are support by our OAuth server Main OAuth tokens Access token: gives the client application access to the protected resource, usually the API Refresh token: used to request a new access token when this expires ID Token (included with OIDC): user profile information JWT (Json Web Token) iss : the issuer of the token, an entity we trust sub : the subject (user) of the token aud : the audience which should consume the token exp : expiration time of the token Claims: a key/value pair within the token that gives the client application information Client Credentials: AuthZ for microservices \u00b6 Private clients only where secrets are in backend code; not appropiate for web pages or mobiles apps Must use secure communications TLS no user relationship validate access token against the /instrospect endpoint expired token has to be rejected Implicit or Hybrid: AuthZ for mobile devices \u00b6 Attach the user to the process cookie/session storage AppAuth and Passport for Android or iOS Must use secure communications TLS use of redirect_uri Authorization Code \u00b6 get an Auth Code instead of an access token. The third-party application never sees our credentials. The end user never sees the access token. Resource Owner Password Flow \u00b6 recommended only for updating legacy systems danger! credentials are managed by the application itself Server side implementations \u00b6","title":"Web Security: OAuth and OpenID Connect"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#web-security-oauth-and-openid-connect","text":"https://www.linkedin.com/learning/web-security-oauth-and-openid-connect?u=1009514 OAuth 2.0 Servers Map of OAuth 2.0 Specs Aaron Parecki's book OAuth 2.0 Simplified","title":"Web Security: OAuth and OpenID Connect"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#what-is-oauth","text":"Some OAuth Servers: https://oauth2.thephpleague.com https://developers.google.com/oauthplayground https://openidconnect.net https://developer.okta.com A token introspection tool: https://www.jsonwebtoken.io flow = grant type scopes = permissions AuthZ with: API keys Id & Secret OAuth OpenID Connect (OIDC) is an OAuth 2.0 identity extension \u2013 only for users.","title":"What is OAuth?"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#core-terminology","text":"Main OAuth extensions /authorize : grant permission to the resource. Could return an authZ code or an access token /token : trade an authZ code or refresh token for an access token /revoke : deactivate (invalidate) a token. Valid for access or refresh tokens /instrospect : learn more about the token /register : create new OAuth clients /userinfo : retrieve profile information about the authenticated user /.well-known/openid-configuration : which endpoints are support by our OAuth server Main OAuth tokens Access token: gives the client application access to the protected resource, usually the API Refresh token: used to request a new access token when this expires ID Token (included with OIDC): user profile information JWT (Json Web Token) iss : the issuer of the token, an entity we trust sub : the subject (user) of the token aud : the audience which should consume the token exp : expiration time of the token Claims: a key/value pair within the token that gives the client application information","title":"Core Terminology"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#client-credentials-authz-for-microservices","text":"Private clients only where secrets are in backend code; not appropiate for web pages or mobiles apps Must use secure communications TLS no user relationship validate access token against the /instrospect endpoint expired token has to be rejected","title":"Client Credentials: AuthZ for microservices"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#implicit-or-hybrid-authz-for-mobile-devices","text":"Attach the user to the process cookie/session storage AppAuth and Passport for Android or iOS Must use secure communications TLS use of redirect_uri","title":"Implicit or Hybrid: AuthZ for mobile devices"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#authorization-code","text":"get an Auth Code instead of an access token. The third-party application never sees our credentials. The end user never sees the access token.","title":"Authorization Code"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#resource-owner-password-flow","text":"recommended only for updating legacy systems danger! credentials are managed by the application itself","title":"Resource Owner Password Flow"},{"location":"Security/Web-Security-OAuth-And-OpenID-Connect/#server-side-implementations","text":"","title":"Server side implementations"}]}